{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "repo_path = \"/home/kjakkala/mmwave\"\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='1'\n",
    "\n",
    "import sys\n",
    "sys.path.append(os.path.join(repo_path, 'models'))\n",
    "\n",
    "from utils import *\n",
    "from resnet import ResNet50\n",
    "from pix2pix import UNetGenerator, PatchGanDiscriminator\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path    = os.path.join(repo_path, 'data')\n",
    "num_classes     = 9\n",
    "batch_size      = 16\n",
    "train_src_days  = 6\n",
    "train_trg_days  = 0\n",
    "train_trg_env_days = 2\n",
    "epochs          = 500\n",
    "init_lr         = 0.0001\n",
    "num_features    = 256\n",
    "activation_fn   = 'selu'\n",
    "notes           = \"cyclegan_server_clas_lr_0.000001\"\n",
    "log_data = \"classes-{}_bs-{}_train_src_days-{}_train_trg_days-{}_train_trgenv_days-{}_initlr-{}_num_feat-{}_act_fn-{}_{}\".format(num_classes,\n",
    "                                                                                                                                 batch_size,\n",
    "                                                                                                                                 train_src_days,\n",
    "                                                                                                                                 train_trg_days,\n",
    "                                                                                                                                 train_trg_env_days,\n",
    "                                                                                                                                 init_lr,\n",
    "                                                                                                                                 num_features,\n",
    "                                                                                                                                 activation_fn,\n",
    "                                                                                                                                 notes)\n",
    "log_dir         = os.path.join(repo_path, 'logs/new_logs/CycleGAN/{}'.format(log_data))\n",
    "checkpoint_path = os.path.join(repo_path, 'checkpoints/{}'.format(log_data))\n",
    "classifier_checkpoint_path = \"/home/kjakkala/mmwave/checkpoints/classes-9_bs-64_train_src_days-6_train_trg_days-0_train_trgenv_days-0_initlr-0.0001_num_feat-256_act_fn-selu_vanilla_baseline\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9127, 256, 256, 1) (9127, 2) \n",
      " ['arahman3', 'harika', 'hchen32', 'jlaivins', 'kjakkala', 'pjanakar', 'ppinyoan', 'pwang13', 'upattnai', 'wrang']\n",
      "(8737, 256, 256, 1) (8737, 2)\n",
      "(8547, 256, 256, 1) (8547, 2) \n",
      " ['arahman3', 'hchen32', 'jlaivins', 'kjakkala', 'pjanakar', 'ppinyoan', 'pwang13', 'upattnai', 'wrang']\n",
      "Final shapes: \n",
      "Source: (4615, 256, 256, 1) (4615, 9) (513, 256, 256, 1) (513, 9)\n",
      "Time: (0, 256, 256, 1) (0, 9) (3419, 256, 256, 1) (3419, 9)\n",
      "Conf: (0, 256, 256, 1) (0,) (1350, 256, 256, 1) (1350, 9)\n",
      "Server (898, 256, 256, 1) (898, 9) (448, 256, 256, 1) (448, 9)\n",
      "Office: (899, 256, 256, 1) (899, 9)\n"
     ]
    }
   ],
   "source": [
    "X_data, y_data, classes = get_h5dataset(os.path.join(dataset_path, 'source_data.h5'))\n",
    "X_data = resize_data(X_data)\n",
    "print(X_data.shape, y_data.shape, \"\\n\", classes)\n",
    "\n",
    "X_data, y_data = balance_dataset(X_data, y_data, \n",
    "                                 num_days=10, \n",
    "                                 num_classes=len(classes), \n",
    "                                 max_samples_per_class=95)\n",
    "print(X_data.shape, y_data.shape)\n",
    "\n",
    "#remove harika's data (incomplete data)\n",
    "X_data = np.delete(X_data, np.where(y_data[:, 0] == 1)[0], 0)\n",
    "y_data = np.delete(y_data, np.where(y_data[:, 0] == 1)[0], 0)\n",
    "\n",
    "#update labes to handle 9 classes instead of 10\n",
    "y_data[y_data[:, 0] >= 2, 0] -= 1\n",
    "del classes[1]\n",
    "print(X_data.shape, y_data.shape, \"\\n\", classes)\n",
    "\n",
    "#split days of data to train and test\n",
    "X_src = X_data[y_data[:, 1] < train_src_days]\n",
    "y_src = y_data[y_data[:, 1] < train_src_days, 0]\n",
    "y_src = np.eye(len(classes))[y_src]\n",
    "X_train_src, X_test_src, y_train_src, y_test_src = train_test_split(X_src,\n",
    "                                                                    y_src,\n",
    "                                                                    stratify=y_src,\n",
    "                                                                    test_size=0.10,\n",
    "                                                                    random_state=42)\n",
    "\n",
    "X_trg = X_data[y_data[:, 1] >= train_src_days]\n",
    "y_trg = y_data[y_data[:, 1] >= train_src_days]\n",
    "X_train_trg = X_trg[y_trg[:, 1] < train_src_days+train_trg_days]\n",
    "y_train_trg = y_trg[y_trg[:, 1] < train_src_days+train_trg_days, 0]\n",
    "y_train_trg = np.eye(len(classes))[y_train_trg]\n",
    "\n",
    "X_test_trg = X_data[y_data[:, 1] >= train_src_days+train_trg_days]\n",
    "y_test_trg = y_data[y_data[:, 1] >= train_src_days+train_trg_days, 0]\n",
    "y_test_trg = np.eye(len(classes))[y_test_trg]\n",
    "\n",
    "del X_src, y_src, X_trg, y_trg, X_data, y_data\n",
    "\n",
    "#mean center and normalize dataset\n",
    "X_train_src, src_mean = mean_center(X_train_src)\n",
    "X_train_src, src_min, src_ptp = normalize(X_train_src)\n",
    "\n",
    "X_test_src, _    = mean_center(X_test_src, src_mean)\n",
    "X_test_src, _, _ = normalize(X_test_src, src_min, src_ptp)\n",
    "\n",
    "if(X_train_trg.shape[0] != 0):\n",
    "  X_train_trg, trg_mean = mean_center(X_train_trg)\n",
    "  X_train_trg, trg_min, trg_ptp = normalize(X_train_trg)\n",
    "\n",
    "  X_test_trg, _    = mean_center(X_test_trg, trg_mean)\n",
    "  X_test_trg, _, _ = normalize(X_test_trg, trg_min, trg_ptp)  \n",
    "else:\n",
    "  X_test_trg, _    = mean_center(X_test_trg, src_mean)\n",
    "  X_test_trg, _, _ = normalize(X_test_trg, src_min, src_ptp)\n",
    "  \n",
    "X_train_src = X_train_src.astype(np.float32)\n",
    "y_train_src = y_train_src.astype(np.uint8)\n",
    "X_test_src  = X_test_src.astype(np.float32)\n",
    "y_test_src  = y_test_src.astype(np.uint8)\n",
    "X_train_trg = X_train_trg.astype(np.float32)\n",
    "y_train_trg = y_train_trg.astype(np.uint8)\n",
    "X_test_trg  = X_test_trg.astype(np.float32)\n",
    "y_test_trg  = y_test_trg.astype(np.uint8)\n",
    "print(\"Final shapes: \")\n",
    "print(\"Source:\", X_train_src.shape, y_train_src.shape,  X_test_src.shape, y_test_src.shape)\n",
    "print(\"Time:\", X_train_trg.shape, y_train_trg.shape, X_test_trg.shape, y_test_trg.shape)\n",
    "\n",
    "X_train_conf,   y_train_conf,   X_test_conf,   y_test_conf   = get_trg_data(os.path.join(dataset_path, 'target_conf_data.h5'),   classes, 0)\n",
    "X_train_server, y_train_server, X_test_server, y_test_server = get_trg_data(os.path.join(dataset_path, 'target_server_data.h5'), classes, train_trg_env_days)\n",
    "_             , _             , X_data_office, y_data_office = get_trg_data(os.path.join(dataset_path, 'target_office_data.h5'), classes, 0)\n",
    "\n",
    "print(\"Conf:\",   X_train_conf.shape,   y_train_conf.shape,    X_test_conf.shape,   y_test_conf.shape)\n",
    "print(\"Server\",  X_train_server.shape, y_train_server.shape,  X_test_server.shape, y_test_server.shape)\n",
    "print(\"Office:\", X_data_office.shape,  y_data_office.shape)\n",
    "\n",
    "#get tf.data objects for each set\n",
    "\n",
    "#Test\n",
    "conf_test_set = tf.data.Dataset.from_tensor_slices((X_test_conf, y_test_conf))\n",
    "conf_test_set = conf_test_set.batch(batch_size, drop_remainder=False)\n",
    "conf_test_set = conf_test_set.prefetch(batch_size)\n",
    "\n",
    "server_test_set = tf.data.Dataset.from_tensor_slices((X_test_server, y_test_server))\n",
    "server_test_set = server_test_set.batch(batch_size, drop_remainder=False)\n",
    "server_test_set = server_test_set.prefetch(batch_size)\n",
    "\n",
    "office_test_set = tf.data.Dataset.from_tensor_slices((X_data_office, y_data_office))\n",
    "office_test_set = office_test_set.batch(batch_size, drop_remainder=False)\n",
    "office_test_set = office_test_set.prefetch(batch_size)\n",
    "\n",
    "src_test_set = tf.data.Dataset.from_tensor_slices((X_test_src, y_test_src))\n",
    "src_test_set = src_test_set.batch(batch_size, drop_remainder=False)\n",
    "src_test_set = src_test_set.prefetch(batch_size)\n",
    "\n",
    "time_test_set = tf.data.Dataset.from_tensor_slices((X_test_trg, y_test_trg))\n",
    "time_test_set = time_test_set.batch(batch_size, drop_remainder=False)\n",
    "time_test_set = time_test_set.prefetch(batch_size)\n",
    "\n",
    "#Train\n",
    "src_train_set = tf.data.Dataset.from_tensor_slices((X_train_src, y_train_src))\n",
    "src_train_set = src_train_set.shuffle(X_train_src.shape[0])\n",
    "src_train_set = src_train_set.batch(batch_size, drop_remainder=True)\n",
    "src_train_set = src_train_set.prefetch(batch_size)\n",
    "\n",
    "server_train_set = tf.data.Dataset.from_tensor_slices((X_train_server, y_train_server))\n",
    "server_train_set = server_train_set.shuffle(X_train_server.shape[0])\n",
    "server_train_set = server_train_set.batch(batch_size, drop_remainder=True)\n",
    "server_train_set = server_train_set.prefetch(batch_size)\n",
    "server_train_set = server_train_set.repeat(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAMBDA = 10\n",
    "\n",
    "loss_obj = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "def discriminator_loss(real, generated):\n",
    "  real_loss = loss_obj(tf.ones_like(real), real)\n",
    "  generated_loss = loss_obj(tf.zeros_like(generated), generated)\n",
    "  total_disc_loss = real_loss + generated_loss\n",
    "  return total_disc_loss * 0.5\n",
    "\n",
    "def generator_loss(generated):\n",
    "  return loss_obj(tf.ones_like(generated), generated)\n",
    "\n",
    "def identity_loss(real_image, same_image):\n",
    "  loss = tf.reduce_mean(tf.abs(real_image - same_image))\n",
    "  return LAMBDA * 0.5 * loss\n",
    "\n",
    "def get_cross_entropy_loss(labels, logits):\n",
    "  loss = tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n",
    "  return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_gen_src_entropy_loss           = tf.keras.metrics.Mean(name='gen_src_entropy_loss')\n",
    "tb_gen_trg_entropy_loss_src_guide = tf.keras.metrics.Mean(name='gen_trg_entropy_loss_src_guide')\n",
    "tb_gen_trg_entropy_loss_src_data  = tf.keras.metrics.Mean(name='gen_trg_entropy_loss_src_data')\n",
    "tb_disc_src_loss                  = tf.keras.metrics.Mean(name='disc_src_loss')\n",
    "tb_disc_trg_loss                  = tf.keras.metrics.Mean(name='disc_trg_loss')\n",
    "tb_gen_src_loss                   = tf.keras.metrics.Mean(name='gen_src_loss')\n",
    "tb_gen_trg_loss                   = tf.keras.metrics.Mean(name='gen_trg_loss')\n",
    "tb_src_identity_loss              = tf.keras.metrics.Mean(name='src_identity_loss')\n",
    "tb_trg_identity_loss              = tf.keras.metrics.Mean(name='trg_identity_loss')\n",
    "tb_total_gen_s_loss               = tf.keras.metrics.Mean(name='total_gen_s_loss')   \n",
    "tb_total_gen_t_loss               = tf.keras.metrics.Mean(name='total_gen_t_loss')\n",
    "tb_total_clas_s_loss              = tf.keras.metrics.Mean(name='total_clas_s_loss')   \n",
    "tb_total_clas_t_loss              = tf.keras.metrics.Mean(name='total_clas_t_loss')\n",
    "temporal_test_acc    = tf.keras.metrics.CategoricalAccuracy(name='temporal_test_acc')\n",
    "source_train_acc     = tf.keras.metrics.CategoricalAccuracy(name='source_train_acc')\n",
    "source_test_acc      = tf.keras.metrics.CategoricalAccuracy(name='source_test_acc')\n",
    "office_test_acc      = tf.keras.metrics.CategoricalAccuracy(name='office_test_acc')\n",
    "server_train_acc     = tf.keras.metrics.CategoricalAccuracy(name='server_train_acc')\n",
    "server_test_acc      = tf.keras.metrics.CategoricalAccuracy(name='server_test_acc')\n",
    "conference_test_acc  = tf.keras.metrics.CategoricalAccuracy(name='conference_test_acc')\n",
    "\n",
    "@tf.function\n",
    "def train_step(src_x, src_y, trg_x, trg_y):\n",
    "  # persistent is set to True because the tape is used more than\n",
    "  # once to calculate the gradients.\n",
    "  with tf.GradientTape(persistent=True) as tape:\n",
    "    src_x_fake   = generator_s(trg_x,      training=True)\n",
    "    trg_x_cycled = generator_t(src_x_fake, training=True)\n",
    "\n",
    "    trg_x_fake   = generator_t(src_x,      training=True)\n",
    "    src_x_cycled = generator_s(trg_x_fake, training=True)\n",
    "    \n",
    "    src_x_same = generator_s(src_x, training=True)\n",
    "    trg_x_same = generator_t(trg_x, training=True)\n",
    "    \n",
    "    src_logits, _ = classifier_s(src_x, training=True)\n",
    "    trg_logits, _ = classifier_t(trg_x, training=True)\n",
    "    \n",
    "    src_logits_fake, _ = classifier_s(src_x_fake, training=True)\n",
    "    trg_logits_fake, _ = classifier_t(trg_x_fake, training=True)\n",
    "    \n",
    "    #Classifier##########################################\n",
    "    gen_src_entropy_loss = get_cross_entropy_loss(labels=src_y, \n",
    "                                                  logits=src_logits)\n",
    "    \n",
    "    gen_trg_entropy_loss_src_guide = get_cross_entropy_loss(labels=tf.nn.softmax(src_logits_fake), \n",
    "                                                            logits=trg_logits)   \n",
    "    gen_trg_entropy_loss_src_data  = get_cross_entropy_loss(labels=src_y, \n",
    "                                                            logits=trg_logits_fake)\n",
    "    \n",
    "    #Discriminator##########################################\n",
    "    disc_src      = discriminator_s(src_x, training=True)\n",
    "    disc_src_fake = discriminator_s(src_x_fake, training=True)\n",
    "    disc_src_loss = discriminator_loss(disc_src, disc_src_fake)\n",
    "\n",
    "    disc_trg      = discriminator_t(trg_x, training=True)\n",
    "    disc_trg_fake = discriminator_t(trg_x_fake, training=True)\n",
    "    disc_trg_loss = discriminator_loss(disc_trg, disc_trg_fake)\n",
    "\n",
    "    #Generator##########################################\n",
    "    gen_src_loss = generator_loss(disc_src_fake)\n",
    "    gen_trg_loss = generator_loss(disc_trg_fake)\n",
    "    \n",
    "    src_identity_loss = identity_loss(src_x, src_x_same)\n",
    "    trg_identity_loss = identity_loss(trg_x, trg_x_same)\n",
    "    \n",
    "    #Loss##########################################\n",
    "    total_gen_s_loss = gen_src_loss + src_identity_loss + gen_src_entropy_loss\n",
    "    total_gen_t_loss = gen_trg_loss + \\\n",
    "                       trg_identity_loss + \\\n",
    "                       gen_trg_entropy_loss_src_guide + \\\n",
    "                       gen_trg_entropy_loss_src_data \n",
    "    \n",
    "    total_clas_s_loss = gen_src_entropy_loss\n",
    "    total_clas_t_loss = gen_trg_entropy_loss_src_guide + gen_trg_entropy_loss_src_data \n",
    "    \n",
    "  # Calculate the gradients for generator and discriminator\n",
    "  classifier_s_gradients = tape.gradient(total_clas_s_loss, \n",
    "                                         classifier_s.trainable_variables)\n",
    "  classifier_t_gradients = tape.gradient(total_clas_t_loss, \n",
    "                                         classifier_t.trainable_variables)\n",
    "  \n",
    "  generator_s_gradients = tape.gradient(total_gen_s_loss, \n",
    "                                        generator_s.trainable_variables)\n",
    "  generator_t_gradients = tape.gradient(total_gen_t_loss, \n",
    "                                        generator_t.trainable_variables)\n",
    "  \n",
    "  discriminator_s_gradients = tape.gradient(disc_src_loss, \n",
    "                                            discriminator_s.trainable_variables)\n",
    "  discriminator_t_gradients = tape.gradient(disc_trg_loss, \n",
    "                                            discriminator_t.trainable_variables)\n",
    "    \n",
    "  # Apply the gradients to the optimizer\n",
    "  classifier_s_optimizer.apply_gradients(zip(classifier_s_gradients, \n",
    "                                             classifier_s.trainable_variables))\n",
    "\n",
    "  classifier_t_optimizer.apply_gradients(zip(classifier_t_gradients, \n",
    "                                             classifier_t.trainable_variables))\n",
    "  \n",
    "  generator_s_optimizer.apply_gradients(zip(generator_s_gradients, \n",
    "                                            generator_s.trainable_variables))\n",
    "\n",
    "  generator_t_optimizer.apply_gradients(zip(generator_t_gradients, \n",
    "                                            generator_t.trainable_variables))\n",
    "  \n",
    "  discriminator_s_optimizer.apply_gradients(zip(discriminator_s_gradients,\n",
    "                                                discriminator_s.trainable_variables))\n",
    "  \n",
    "  discriminator_t_optimizer.apply_gradients(zip(discriminator_t_gradients,\n",
    "                                                discriminator_t.trainable_variables))\n",
    "  \n",
    "  tb_gen_src_entropy_loss(gen_src_entropy_loss)\n",
    "  tb_gen_trg_entropy_loss_src_guide(gen_trg_entropy_loss_src_guide)\n",
    "  tb_gen_trg_entropy_loss_src_data(gen_trg_entropy_loss_src_data)\n",
    "  tb_disc_src_loss(disc_src_loss)\n",
    "  tb_disc_trg_loss(disc_trg_loss)\n",
    "  tb_gen_src_loss(gen_src_loss)\n",
    "  tb_gen_trg_loss(gen_trg_loss)\n",
    "  tb_src_identity_loss(src_identity_loss)\n",
    "  tb_trg_identity_loss(trg_identity_loss)\n",
    "  tb_total_gen_s_loss(total_gen_s_loss)      \n",
    "  tb_total_gen_t_loss(total_gen_t_loss)    \n",
    "  tb_total_clas_s_loss(total_clas_s_loss)    \n",
    "  tb_total_clas_t_loss(total_clas_t_loss)    \n",
    "  source_train_acc(src_y, tf.nn.softmax(src_logits))\n",
    "  server_train_acc(trg_y, tf.nn.softmax(trg_logits))\n",
    "\n",
    "@tf.function\n",
    "def test_step(images):\n",
    "  logits, _ =  classifier_s(images, training=False)\n",
    "  return tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest checkpoint restored to source classifier!!\n",
      "Latest checkpoint restored to target classifier!!\n"
     ]
    }
   ],
   "source": [
    "generator_t     = UNetGenerator(input_channels=1, output_channels=1, norm_type='instancenorm')\n",
    "generator_s     = UNetGenerator(input_channels=1, output_channels=1, norm_type='instancenorm')\n",
    "discriminator_s = PatchGanDiscriminator(input_channels=1, norm_type='instancenorm', target=False)\n",
    "discriminator_t = PatchGanDiscriminator(input_channels=1, norm_type='instancenorm', target=False)\n",
    "classifier_s    = ResNet50(num_classes, num_features, \"selu\")\n",
    "classifier_t    = ResNet50(num_classes, num_features, \"selu\")\n",
    "\n",
    "learning_rate  = tf.keras.optimizers.schedules.PolynomialDecay(init_lr,\n",
    "                                                               decay_steps=5000,\n",
    "                                                               end_learning_rate=init_lr*1e-2)\n",
    "classifier_learning_rate  = tf.keras.optimizers.schedules.PolynomialDecay(init_lr*1e-2,\n",
    "                                                                          decay_steps=5000,\n",
    "                                                                          end_learning_rate=init_lr*1e-4)\n",
    "classifier_s_optimizer    = tf.keras.optimizers.Adam(classifier_learning_rate, beta_1=0.5)\n",
    "classifier_t_optimizer    = tf.keras.optimizers.Adam(classifier_learning_rate, beta_1=0.5)\n",
    "generator_s_optimizer     = tf.keras.optimizers.Adam(learning_rate, beta_1=0.5)\n",
    "generator_t_optimizer     = tf.keras.optimizers.Adam(learning_rate, beta_1=0.5)\n",
    "discriminator_s_optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.5)\n",
    "discriminator_t_optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.5)\n",
    "\n",
    "summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "classifier_ckpt = tf.train.Checkpoint(model=classifier_s)\n",
    "ckpt_manager = tf.train.CheckpointManager(classifier_ckpt, classifier_checkpoint_path, max_to_keep=5)\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "  classifier_ckpt.restore(ckpt_manager.latest_checkpoint).expect_partial()\n",
    "  print ('Latest checkpoint restored to source classifier!!')\n",
    "    \n",
    "classifier_ckpt = tf.train.Checkpoint(model=classifier_t)\n",
    "ckpt_manager = tf.train.CheckpointManager(classifier_ckpt, classifier_checkpoint_path, max_to_keep=5)\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "  classifier_ckpt.restore(ckpt_manager.latest_checkpoint).expect_partial()\n",
    "  print ('Latest checkpoint restored to target classifier!!')\n",
    "    \n",
    "\n",
    "ckpt = tf.train.Checkpoint(generator_t=generator_t,\n",
    "                           generator_s=generator_s,\n",
    "                           discriminator_t=discriminator_t,\n",
    "                           discriminator_s=discriminator_s,\n",
    "                           classifier_t=classifier_t,\n",
    "                           classifier_s=classifier_s)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint for epoch 25 at /home/kjakkala/mmwave/checkpoints/classes-9_bs-16_train_src_days-6_train_trg_days-0_train_trgenv_days-2_initlr-0.0001_num_feat-256_act_fn-selu_cyclegan_server_clas_lr_0.000001/ckpt-1\n",
      "Saving checkpoint for epoch 50 at /home/kjakkala/mmwave/checkpoints/classes-9_bs-16_train_src_days-6_train_trg_days-0_train_trgenv_days-2_initlr-0.0001_num_feat-256_act_fn-selu_cyclegan_server_clas_lr_0.000001/ckpt-2\n",
      "Saving checkpoint for epoch 75 at /home/kjakkala/mmwave/checkpoints/classes-9_bs-16_train_src_days-6_train_trg_days-0_train_trgenv_days-2_initlr-0.0001_num_feat-256_act_fn-selu_cyclegan_server_clas_lr_0.000001/ckpt-3\n",
      "Saving checkpoint for epoch 100 at /home/kjakkala/mmwave/checkpoints/classes-9_bs-16_train_src_days-6_train_trg_days-0_train_trgenv_days-2_initlr-0.0001_num_feat-256_act_fn-selu_cyclegan_server_clas_lr_0.000001/ckpt-4\n",
      "Saving checkpoint for epoch 125 at /home/kjakkala/mmwave/checkpoints/classes-9_bs-16_train_src_days-6_train_trg_days-0_train_trgenv_days-2_initlr-0.0001_num_feat-256_act_fn-selu_cyclegan_server_clas_lr_0.000001/ckpt-5\n",
      "Saving checkpoint for epoch 150 at /home/kjakkala/mmwave/checkpoints/classes-9_bs-16_train_src_days-6_train_trg_days-0_train_trgenv_days-2_initlr-0.0001_num_feat-256_act_fn-selu_cyclegan_server_clas_lr_0.000001/ckpt-6\n",
      "Saving checkpoint for epoch 175 at /home/kjakkala/mmwave/checkpoints/classes-9_bs-16_train_src_days-6_train_trg_days-0_train_trgenv_days-2_initlr-0.0001_num_feat-256_act_fn-selu_cyclegan_server_clas_lr_0.000001/ckpt-7\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-cebc07e8cdc6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0msource_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserver_data\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_train_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserver_train_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserver_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserver_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtime_test_set\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    485\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1821\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1822\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1823\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1825\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1139\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1140\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1141\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "  for source_data, server_data in zip(src_train_set, server_train_set):\n",
    "    train_step(source_data[0], source_data[1], server_data[0], server_data[1])\n",
    "    \n",
    "  for data in time_test_set:\n",
    "    temporal_test_acc(test_step(data[0]), data[1])\n",
    "\n",
    "  for data in src_test_set:\n",
    "    source_test_acc(test_step(data[0]), data[1])\n",
    "\n",
    "  for data in office_test_set:\n",
    "    office_test_acc(test_step(data[0]), data[1])\n",
    "\n",
    "  for data in server_test_set:\n",
    "    server_test_acc(test_step(data[0]), data[1])\n",
    "\n",
    "  for data in conf_test_set:\n",
    "    conference_test_acc(test_step(data[0]), data[1])\n",
    "\n",
    "  with summary_writer.as_default():\n",
    "    tf.summary.scalar(\"tb_gen_src_entropy_loss\", tb_gen_src_entropy_loss.result(), step=epoch)\n",
    "    tf.summary.scalar(\"tb_gen_trg_entropy_loss_src_guide\", tb_gen_trg_entropy_loss_src_guide.result(), step=epoch)\n",
    "    tf.summary.scalar(\"tb_gen_trg_entropy_loss_src_data\", tb_gen_trg_entropy_loss_src_data.result(), step=epoch)\n",
    "    tf.summary.scalar(\"tb_disc_src_loss\", tb_disc_src_loss.result(), step=epoch)\n",
    "    tf.summary.scalar(\"tb_disc_trg_loss\", tb_disc_trg_loss.result(), step=epoch)\n",
    "    tf.summary.scalar(\"tb_gen_src_loss\", tb_gen_src_loss.result(), step=epoch)\n",
    "    tf.summary.scalar(\"tb_gen_trg_loss\", tb_gen_trg_loss.result(), step=epoch)\n",
    "    tf.summary.scalar(\"tb_src_identity_loss\", tb_src_identity_loss.result(), step=epoch)\n",
    "    tf.summary.scalar(\"tb_trg_identity_loss\", tb_trg_identity_loss.result(), step=epoch)\n",
    "    tf.summary.scalar(\"tb_total_gen_s_loss\", tb_total_gen_s_loss.result(), step=epoch)\n",
    "    tf.summary.scalar(\"tb_total_gen_t_loss\", tb_total_gen_t_loss.result(), step=epoch)\n",
    "    tf.summary.scalar(\"tb_total_clas_s_loss\", tb_total_clas_s_loss.result(), step=epoch)\n",
    "    tf.summary.scalar(\"tb_total_clas_t_loss\", tb_total_clas_t_loss.result(), step=epoch)    \n",
    "    tf.summary.scalar(\"source_train_acc\", source_train_acc.result(), step=epoch)\n",
    "    tf.summary.scalar(\"server_train_acc\", server_train_acc.result(), step=epoch)\n",
    "    tf.summary.scalar(\"temporal_test_acc\", temporal_test_acc.result(), step=epoch)\n",
    "    tf.summary.scalar(\"source_test_acc\", source_test_acc.result(), step=epoch)\n",
    "    tf.summary.scalar(\"office_test_acc\", office_test_acc.result(), step=epoch)\n",
    "    tf.summary.scalar(\"server_test_acc\", server_test_acc.result(), step=epoch)\n",
    "    tf.summary.scalar(\"conference_test_acc\", conference_test_acc.result(), step=epoch)\n",
    "    \n",
    "  if (epoch + 1) % 25 == 0:\n",
    "    ckpt_save_path = ckpt_manager.save()\n",
    "    print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n",
    "                                                         ckpt_save_path))\n",
    "    \n",
    "  tb_gen_src_entropy_loss.reset_states()\n",
    "  tb_gen_trg_entropy_loss_src_guide.reset_states()\n",
    "  tb_gen_trg_entropy_loss_src_data.reset_states()\n",
    "  tb_disc_src_loss.reset_states()\n",
    "  tb_disc_trg_loss.reset_states()\n",
    "  tb_gen_src_loss.reset_states()\n",
    "  tb_gen_trg_loss.reset_states()\n",
    "  tb_src_identity_loss.reset_states()\n",
    "  tb_trg_identity_loss.reset_states()\n",
    "  tb_total_gen_s_loss.reset_states()    \n",
    "  tb_total_gen_t_loss.reset_states() \n",
    "  tb_total_clas_s_loss.reset_states() \n",
    "  tb_total_clas_t_loss.reset_states()     \n",
    "  source_train_acc.reset_states()\n",
    "  server_train_acc.reset_states()\n",
    "  temporal_test_acc.reset_states()\n",
    "  source_test_acc.reset_states()\n",
    "  office_test_acc.reset_states()\n",
    "  server_test_acc.reset_states()\n",
    "  conference_test_acc.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
