{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.transform import resize\n",
    "from sklearn.model_selection import train_test_split\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes     = 9\n",
    "batch_size      = 16\n",
    "train_src_days  = 6\n",
    "train_trg_days  = 0\n",
    "train_trg_env_days = 2\n",
    "epochs          = 500\n",
    "init_lr         = 0.0001\n",
    "num_features    = 256\n",
    "notes           = 'classes-{}_bs-{}_train_src_days-{}_train_trg_days-{}_train_trgenv_days-{}_initlr-{}_num_features-{}'.format(num_classes,\n",
    "                                                                                                                             batch_size,\n",
    "                                                                                                                             train_src_days,\n",
    "                                                                                                                             train_trg_days,\n",
    "                                                                                                                             train_trg_env_days,\n",
    "                                                                                                                             init_lr,\n",
    "                                                                                                                             num_features)                                                                                                                                        \n",
    "\n",
    "dataset_path    = '/users/kjakkala/mmwave/data/'\n",
    "log_dir         = '/users/kjakkala/mmwave/logs/new_logs/CycleGAN/{}'.format(notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_data(data, output_shape=(256, 256)):\n",
    "  _, height, width, channels = data.shape\n",
    "  data = data.transpose((1, 2, 3, 0))\n",
    "  data = resize(data.reshape(height, width, -1), output_shape)\n",
    "  data = data.reshape(*output_shape, channels, -1)\n",
    "  data = data.transpose((3, 0, 1, 2))\n",
    "  return data\n",
    "\n",
    "#Read data\n",
    "hf = h5py.File(os.path.join(dataset_path, 'source_data.h5'), 'r')\n",
    "X_data = resize_data(np.expand_dims(hf.get('X_data'), axis=-1))\n",
    "y_data = np.array(hf.get('y_data'))\n",
    "classes = list(hf.get('classes'))\n",
    "classes = [n.decode(\"ascii\", \"ignore\") for n in classes]\n",
    "hf.close()\n",
    "print(X_data.shape, y_data.shape, \"\\n\", classes)\n",
    "\n",
    "#balence dataset to 95 samples per day for each person\n",
    "X_data_tmp = []\n",
    "y_data_tmp = []\n",
    "for day in range(10):\n",
    "  for idx in range(len(classes)):\n",
    "    X_data_tmp.extend(X_data[(y_data[:, 0] == idx) & (y_data[:, 1] == day)][:95])\n",
    "    y_data_tmp.extend(y_data[(y_data[:, 0] == idx) & (y_data[:, 1] == day)][:95])\n",
    "X_data = np.array(X_data_tmp)\n",
    "y_data = np.array(y_data_tmp)\n",
    "del X_data_tmp, y_data_tmp\n",
    "print(X_data.shape, y_data.shape)\n",
    "\n",
    "#remove harika's data\n",
    "X_data = np.delete(X_data, np.where(y_data[:, 0] == 1)[0], 0)\n",
    "y_data = np.delete(y_data, np.where(y_data[:, 0] == 1)[0], 0)\n",
    "\n",
    "#update labes to handle 9 classes instead of 10\n",
    "y_data[y_data[:, 0] >= 2, 0] -= 1\n",
    "del classes[1]\n",
    "print(X_data.shape, y_data.shape, \"\\n\", classes)\n",
    "\n",
    "#split days of data to train and test\n",
    "X_src = X_data[y_data[:, 1] < train_src_days]\n",
    "y_src = y_data[y_data[:, 1] < train_src_days, 0]\n",
    "y_src = np.eye(len(classes))[y_src]\n",
    "X_train_src, X_test_src, y_train_src, y_test_src = train_test_split(X_src,\n",
    "                                                                    y_src,\n",
    "                                                                    stratify=y_src,\n",
    "                                                                    test_size=0.10,\n",
    "                                                                    random_state=42)\n",
    "\n",
    "X_trg = X_data[y_data[:, 1] >= train_src_days]\n",
    "y_trg = y_data[y_data[:, 1] >= train_src_days]\n",
    "X_train_trg = X_trg[y_trg[:, 1] < train_src_days+train_trg_days]\n",
    "y_train_trg = y_trg[y_trg[:, 1] < train_src_days+train_trg_days, 0]\n",
    "y_train_trg = np.eye(len(classes))[y_train_trg]\n",
    "\n",
    "X_test_trg = X_data[y_data[:, 1] >= train_src_days+train_trg_days]\n",
    "y_test_trg = y_data[y_data[:, 1] >= train_src_days+train_trg_days, 0]\n",
    "y_test_trg = np.eye(len(classes))[y_test_trg]\n",
    "\n",
    "del X_src, y_src, X_trg, y_trg, X_data, y_data\n",
    "\n",
    "#standardise dataset\n",
    "src_mean = np.mean(X_train_src)\n",
    "X_train_src -= src_mean\n",
    "src_min = np.min(X_train_src)\n",
    "src_ptp = np.ptp(X_train_src)\n",
    "X_train_src = 2.*(X_train_src - src_min)/src_ptp-1\n",
    "\n",
    "X_test_src -= src_mean\n",
    "X_test_src = 2.*(X_test_src - src_min)/src_ptp-1\n",
    "\n",
    "if(X_train_trg.shape[0] != 0):\n",
    "  trg_mean = np.mean(X_train_trg)\n",
    "  X_train_trg -= trg_mean\n",
    "  trg_min = np.min(X_train_trg)\n",
    "  trg_ptp = np.ptp(X_train_trg)\n",
    "  X_train_trg = 2.*(X_train_trg - trg_min)/trg_ptp-1\n",
    "\n",
    "  X_test_trg -= trg_mean\n",
    "  X_test_trg = 2.*(X_test_trg - trg_min)/trg_ptp-1\n",
    "else:\n",
    "  X_test_trg -= src_mean\n",
    "  X_test_trg = 2.*(X_test_trg - src_min)/src_ptp-1\n",
    "  \n",
    "X_train_src = X_train_src.astype(np.float32)\n",
    "y_train_src = y_train_src.astype(np.uint8)\n",
    "X_test_src  = X_test_src.astype(np.float32)\n",
    "y_test_src  = y_test_src.astype(np.uint8)\n",
    "X_train_trg = X_train_trg.astype(np.float32)\n",
    "y_train_trg = y_train_trg.astype(np.uint8)\n",
    "X_test_trg  = X_test_trg.astype(np.float32)\n",
    "y_test_trg  = y_test_trg.astype(np.uint8)\n",
    "\n",
    "print(X_train_src.shape, y_train_src.shape,  X_test_src.shape, y_test_src.shape, X_train_trg.shape, y_train_trg.shape, X_test_trg.shape, y_test_trg.shape)\n",
    "\n",
    "def get_trg_data(fname, src_classes, train_trg_days):\n",
    "  #Read data\n",
    "  hf = h5py.File(fname, 'r')\n",
    "  X_data_trg = resize_data(np.expand_dims(hf.get('X_data'), axis=-1))\n",
    "  y_data_trg = np.array(hf.get('y_data'))\n",
    "  trg_classes = list(hf.get('classes'))\n",
    "  trg_classes = [n.decode(\"ascii\", \"ignore\") for n in trg_classes]\n",
    "  hf.close()\n",
    "\n",
    "  #split days of data to train and test\n",
    "  X_train_trg = X_data_trg[y_data_trg[:, 1] < train_trg_days]\n",
    "  y_train_trg = y_data_trg[y_data_trg[:, 1] < train_trg_days, 0]\n",
    "  y_train_trg = np.array([src_classes.index(trg_classes[y_train_trg[i]]) for i in range(y_train_trg.shape[0])])\n",
    "  y_train_trg = np.eye(len(src_classes))[y_train_trg]\n",
    "  y_train_trg = y_train_trg.astype(np.int64)\n",
    "\n",
    "  X_test_trg = X_data_trg[y_data_trg[:, 1] >= train_trg_days]\n",
    "  y_test_trg = y_data_trg[y_data_trg[:, 1] >= train_trg_days, 0]\n",
    "  y_test_trg = np.eye(len(src_classes))[y_test_trg]\n",
    "  y_test_trg = y_test_trg.astype(np.int64)\n",
    "\n",
    "  if(X_train_trg.shape[0] != 0):\n",
    "    trg_mean = np.mean(X_train_trg)\n",
    "    X_train_trg -= trg_mean\n",
    "    trg_min = np.min(X_train_trg)\n",
    "    trg_ptp = np.ptp(X_train_trg)\n",
    "    X_train_trg = 2.*(X_train_trg - trg_min)/trg_ptp-1\n",
    "\n",
    "    X_test_trg -= trg_mean\n",
    "    X_test_trg = 2.*(X_test_trg - trg_min)/trg_ptp-1\n",
    "  else:\n",
    "    X_test_trg -= np.mean(X_test_trg)\n",
    "    trg_min = np.min(X_test_trg)\n",
    "    trg_ptp = np.ptp(X_test_trg)\n",
    "    X_test_trg = 2.*(X_test_trg - trg_min)/trg_ptp-1\n",
    "    \n",
    "  return X_train_trg.astype(np.float32), y_train_trg.astype(np.uint8), X_test_trg.astype(np.float32), y_test_trg.astype(np.uint8)\n",
    "\n",
    "X_train_conf,   y_train_conf,   X_test_conf,   y_test_conf   = get_trg_data(os.path.join(dataset_path, 'target_conf_data.h5'),   classes, 3)\n",
    "X_train_server, y_train_server, X_test_server, y_test_server = get_trg_data(os.path.join(dataset_path, 'target_server_data.h5'), classes, train_trg_env_days)\n",
    "X_data_office,  y_data_office,  _,             _             = get_trg_data(os.path.join(dataset_path, 'target_office_data.h5'), classes, 3)\n",
    "\n",
    "print(X_train_conf.shape,   y_train_conf.shape,    X_test_conf.shape,   y_test_conf.shape)\n",
    "print(X_train_server.shape, y_train_server.shape,  X_test_server.shape, y_test_server.shape)\n",
    "print(X_data_office.shape, y_data_office.shape)\n",
    "\n",
    "#get tf.data objects for each set\n",
    "\n",
    "#Test\n",
    "conf_test_set = tf.data.Dataset.from_tensor_slices((X_train_conf, y_train_conf))\n",
    "conf_test_set = conf_test_set.batch(batch_size, drop_remainder=False)\n",
    "conf_test_set = conf_test_set.prefetch(batch_size)\n",
    "\n",
    "server_test_set = tf.data.Dataset.from_tensor_slices((X_test_server, y_test_server))\n",
    "server_test_set = server_test_set.batch(batch_size, drop_remainder=False)\n",
    "server_test_set = server_test_set.prefetch(batch_size)\n",
    "\n",
    "office_test_set = tf.data.Dataset.from_tensor_slices((X_data_office, y_data_office))\n",
    "office_test_set = office_test_set.batch(batch_size, drop_remainder=False)\n",
    "office_test_set = office_test_set.prefetch(batch_size)\n",
    "\n",
    "src_test_set = tf.data.Dataset.from_tensor_slices((X_test_src, y_test_src))\n",
    "src_test_set = src_test_set.batch(batch_size, drop_remainder=False)\n",
    "src_test_set = src_test_set.prefetch(batch_size)\n",
    "\n",
    "time_test_set = tf.data.Dataset.from_tensor_slices((X_test_trg, y_test_trg))\n",
    "time_test_set = time_test_set.batch(batch_size, drop_remainder=False)\n",
    "time_test_set = time_test_set.prefetch(batch_size)\n",
    "\n",
    "#Train\n",
    "src_train_set = tf.data.Dataset.from_tensor_slices((X_train_src, y_train_src))\n",
    "src_train_set = src_train_set.shuffle(X_train_src.shape[0])\n",
    "src_train_set = src_train_set.batch(batch_size, drop_remainder=True)\n",
    "src_train_set = src_train_set.prefetch(batch_size)\n",
    "\n",
    "server_train_set = tf.data.Dataset.from_tensor_slices((X_train_server, y_train_server))\n",
    "server_train_set = server_train_set.shuffle(X_train_server.shape[0])\n",
    "server_train_set = server_train_set.batch(batch_size, drop_remainder=True)\n",
    "server_train_set = server_train_set.prefetch(batch_size)\n",
    "server_train_set = server_train_set.repeat(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InstanceNormalization(tf.keras.layers.Layer):\n",
    "  \"\"\"Instance Normalization Layer (https://arxiv.org/abs/1607.08022).\"\"\"\n",
    "\n",
    "  def __init__(self, epsilon=1e-5):\n",
    "    super(InstanceNormalization, self).__init__()\n",
    "    self.epsilon = epsilon\n",
    "\n",
    "  def build(self, input_shape):\n",
    "    self.scale = self.add_weight(\n",
    "        name='scale',\n",
    "        shape=input_shape[-1:],\n",
    "        initializer=tf.random_normal_initializer(1., 0.02),\n",
    "        trainable=True)\n",
    "\n",
    "    self.offset = self.add_weight(\n",
    "        name='offset',\n",
    "        shape=input_shape[-1:],\n",
    "        initializer='zeros',\n",
    "        trainable=True)\n",
    "\n",
    "  def call(self, x):\n",
    "    mean, variance = tf.nn.moments(x, axes=[1, 2], keepdims=True)\n",
    "    inv = tf.math.rsqrt(variance + self.epsilon)\n",
    "    normalized = (x - mean) * inv\n",
    "    return self.scale * normalized + self.offset\n",
    "  \n",
    "def downsample(filters, size, norm_type='batchnorm', apply_norm=True):\n",
    "  \"\"\"Downsamples an input.\n",
    "  Conv2D => Batchnorm => LeakyRelu\n",
    "  Args:\n",
    "    filters: number of filters\n",
    "    size: filter size\n",
    "    norm_type: Normalization type; either 'batchnorm' or 'instancenorm'.\n",
    "    apply_norm: If True, adds the batchnorm layer\n",
    "  Returns:\n",
    "    Downsample Sequential Model\n",
    "  \"\"\"\n",
    "  initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "  result = tf.keras.Sequential()\n",
    "  result.add(\n",
    "      tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',\n",
    "                             kernel_initializer=initializer, use_bias=False))\n",
    "\n",
    "  if apply_norm:\n",
    "    if norm_type.lower() == 'batchnorm':\n",
    "      result.add(tf.keras.layers.BatchNormalization())\n",
    "    elif norm_type.lower() == 'instancenorm':\n",
    "      result.add(InstanceNormalization())\n",
    "\n",
    "  result.add(tf.keras.layers.LeakyReLU())\n",
    "\n",
    "  return result\n",
    "\n",
    "\n",
    "def upsample(filters, size, norm_type='batchnorm', apply_dropout=False):\n",
    "  \"\"\"Upsamples an input.\n",
    "  Conv2DTranspose => Batchnorm => Dropout => Relu\n",
    "  Args:\n",
    "    filters: number of filters\n",
    "    size: filter size\n",
    "    norm_type: Normalization type; either 'batchnorm' or 'instancenorm'.\n",
    "    apply_dropout: If True, adds the dropout layer\n",
    "  Returns:\n",
    "    Upsample Sequential Model\n",
    "  \"\"\"\n",
    "\n",
    "  initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "  result = tf.keras.Sequential()\n",
    "  result.add(\n",
    "      tf.keras.layers.Conv2DTranspose(filters, size, strides=2,\n",
    "                                      padding='same',\n",
    "                                      kernel_initializer=initializer,\n",
    "                                      use_bias=False))\n",
    "\n",
    "  if norm_type.lower() == 'batchnorm':\n",
    "    result.add(tf.keras.layers.BatchNormalization())\n",
    "  elif norm_type.lower() == 'instancenorm':\n",
    "    result.add(InstanceNormalization())\n",
    "\n",
    "  if apply_dropout:\n",
    "    result.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "  result.add(tf.keras.layers.ReLU())\n",
    "\n",
    "  return result\n",
    "\n",
    "\n",
    "def unet_generator(output_channels, norm_type='batchnorm'):\n",
    "  \"\"\"Modified u-net generator model (https://arxiv.org/abs/1611.07004).\n",
    "  Args:\n",
    "    output_channels: Output channels\n",
    "    norm_type: Type of normalization. Either 'batchnorm' or 'instancenorm'.\n",
    "  Returns:\n",
    "    Generator model\n",
    "  \"\"\"\n",
    "\n",
    "  down_stack = [\n",
    "      downsample(64, 4, norm_type, apply_norm=False),  # (bs, 128, 128, 64)\n",
    "      downsample(128, 4, norm_type),  # (bs, 64, 64, 128)\n",
    "      downsample(256, 4, norm_type),  # (bs, 32, 32, 256)\n",
    "      downsample(512, 4, norm_type),  # (bs, 16, 16, 512)\n",
    "      downsample(512, 4, norm_type),  # (bs, 8, 8, 512)\n",
    "      downsample(512, 4, norm_type),  # (bs, 4, 4, 512)\n",
    "      downsample(512, 4, norm_type),  # (bs, 2, 2, 512)\n",
    "      downsample(512, 4, norm_type),  # (bs, 1, 1, 512)\n",
    "  ]\n",
    "\n",
    "  up_stack = [\n",
    "      upsample(512, 4, norm_type, apply_dropout=True),  # (bs, 2, 2, 1024)\n",
    "      upsample(512, 4, norm_type, apply_dropout=True),  # (bs, 4, 4, 1024)\n",
    "      upsample(512, 4, norm_type, apply_dropout=True),  # (bs, 8, 8, 1024)\n",
    "      upsample(512, 4, norm_type),  # (bs, 16, 16, 1024)\n",
    "      upsample(256, 4, norm_type),  # (bs, 32, 32, 512)\n",
    "      upsample(128, 4, norm_type),  # (bs, 64, 64, 256)\n",
    "      upsample(64, 4, norm_type),  # (bs, 128, 128, 128)\n",
    "  ]\n",
    "\n",
    "  initializer = tf.random_normal_initializer(0., 0.02)\n",
    "  last = tf.keras.layers.Conv2DTranspose(\n",
    "      output_channels, 4, strides=2,\n",
    "      padding='same', kernel_initializer=initializer,\n",
    "      activation='tanh')  # (bs, 256, 256, 3)\n",
    "\n",
    "  concat = tf.keras.layers.Concatenate()\n",
    "\n",
    "  inputs = tf.keras.layers.Input(shape=[None, None, 1])\n",
    "  x = inputs\n",
    "\n",
    "  # Downsampling through the model\n",
    "  skips = []\n",
    "  for down in down_stack:\n",
    "    x = down(x)\n",
    "    skips.append(x)\n",
    "\n",
    "  skips = reversed(skips[:-1])\n",
    "\n",
    "  # Upsampling and establishing the skip connections\n",
    "  for up, skip in zip(up_stack, skips):\n",
    "    x = up(x)\n",
    "    x = concat([x, skip])\n",
    "\n",
    "  x = last(x)\n",
    "\n",
    "  return tf.keras.Model(inputs=inputs, outputs=x)\n",
    "\n",
    "\n",
    "def discriminator(norm_type='batchnorm', target=True):\n",
    "  \"\"\"PatchGan discriminator model (https://arxiv.org/abs/1611.07004).\n",
    "  Args:\n",
    "    norm_type: Type of normalization. Either 'batchnorm' or 'instancenorm'.\n",
    "    target: Bool, indicating whether target image is an input or not.\n",
    "  Returns:\n",
    "    Discriminator model\n",
    "  \"\"\"\n",
    "\n",
    "  initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "  inp = tf.keras.layers.Input(shape=[None, None, 1], name='input_image')\n",
    "  x = inp\n",
    "\n",
    "  if target:\n",
    "    tar = tf.keras.layers.Input(shape=[None, None, 1], name='target_image')\n",
    "    x = tf.keras.layers.concatenate([inp, tar])  # (bs, 256, 256, channels*2)\n",
    "\n",
    "  down1 = downsample(64, 4, norm_type, False)(x)  # (bs, 128, 128, 64)\n",
    "  down2 = downsample(128, 4, norm_type)(down1)  # (bs, 64, 64, 128)\n",
    "  down3 = downsample(256, 4, norm_type)(down2)  # (bs, 32, 32, 256)\n",
    "\n",
    "  zero_pad1 = tf.keras.layers.ZeroPadding2D()(down3)  # (bs, 34, 34, 256)\n",
    "  conv = tf.keras.layers.Conv2D(\n",
    "      512, 4, strides=1, kernel_initializer=initializer,\n",
    "      use_bias=False)(zero_pad1)  # (bs, 31, 31, 512)\n",
    "\n",
    "  if norm_type.lower() == 'batchnorm':\n",
    "    norm1 = tf.keras.layers.BatchNormalization()(conv)\n",
    "  elif norm_type.lower() == 'instancenorm':\n",
    "    norm1 = InstanceNormalization()(conv)\n",
    "\n",
    "  leaky_relu = tf.keras.layers.LeakyReLU()(norm1)\n",
    "\n",
    "  zero_pad2 = tf.keras.layers.ZeroPadding2D()(leaky_relu)  # (bs, 33, 33, 512)\n",
    "\n",
    "  last = tf.keras.layers.Conv2D(\n",
    "      1, 4, strides=1,\n",
    "      kernel_initializer=initializer)(zero_pad2)  # (bs, 30, 30, 1)\n",
    "\n",
    "  if target:\n",
    "    return tf.keras.Model(inputs=[inp, tar], outputs=last)\n",
    "  else:\n",
    "    return tf.keras.Model(inputs=inp, outputs=last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L2_WEIGHT_DECAY = 1e-4\n",
    "BATCH_NORM_DECAY = 0.9\n",
    "BATCH_NORM_EPSILON = 1e-5\n",
    "\n",
    "class IdentityBlock(tf.keras.Model):\n",
    "  def __init__(self, kernel_size, filters, stage, block, activation='relu'):\n",
    "    self.activation = activation\n",
    "\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "\n",
    "    super().__init__(name='stage-' + str(stage) + '_block-' + block)\n",
    "\n",
    "    filters1, filters2, filters3 = filters\n",
    "    bn_axis = -1\n",
    "\n",
    "    self.conv2a = tf.keras.layers.Conv2D(filters1, (1, 1),\n",
    "                                         use_bias=False,\n",
    "                                         kernel_initializer='he_normal',\n",
    "                                         kernel_regularizer=tf.keras.regularizers.l2(L2_WEIGHT_DECAY),\n",
    "                                         name=conv_name_base + '2a')\n",
    "    self.bn2a = tf.keras.layers.BatchNormalization(axis=bn_axis,\n",
    "                                                   momentum=BATCH_NORM_DECAY,\n",
    "                                                   epsilon=BATCH_NORM_EPSILON,\n",
    "                                                   name=bn_name_base + '2a')\n",
    "    self.act1  = tf.keras.layers.Activation(self.activation)\n",
    "\n",
    "    self.conv2b = tf.keras.layers.Conv2D(filters2, kernel_size,\n",
    "                                         padding='same',\n",
    "                                         use_bias=False,\n",
    "                                         kernel_initializer='he_normal',\n",
    "                                         kernel_regularizer=tf.keras.regularizers.l2(L2_WEIGHT_DECAY),\n",
    "                                         name=conv_name_base + '2b')\n",
    "    self.bn2b = tf.keras.layers.BatchNormalization(axis=bn_axis,\n",
    "                                                   momentum=BATCH_NORM_DECAY,\n",
    "                                                   epsilon=BATCH_NORM_EPSILON,\n",
    "                                                   name=bn_name_base + '2b')\n",
    "    self.act2  = tf.keras.layers.Activation(self.activation)\n",
    "\n",
    "    self.conv2c = tf.keras.layers.Conv2D(filters3, (1, 1),\n",
    "                                         use_bias=False,\n",
    "                                         kernel_initializer='he_normal',\n",
    "                                         kernel_regularizer=tf.keras.regularizers.l2(L2_WEIGHT_DECAY),\n",
    "                                         name=conv_name_base + '2c')\n",
    "    self.bn2c = tf.keras.layers.BatchNormalization(axis=bn_axis,\n",
    "                                                   momentum=BATCH_NORM_DECAY,\n",
    "                                                   epsilon=BATCH_NORM_EPSILON,\n",
    "                                                   name=bn_name_base + '2c')\n",
    "    self.act3  = tf.keras.layers.Activation(self.activation)\n",
    "\n",
    "  def call(self, input_tensor, training=False):\n",
    "    x = self.conv2a(input_tensor)\n",
    "    x = self.bn2a(x, training=training)\n",
    "    x = self.act1(x)\n",
    "\n",
    "    x = self.conv2b(x)\n",
    "    x = self.bn2b(x, training=training)\n",
    "    x = self.act2(x)\n",
    "\n",
    "    x = self.conv2c(x)\n",
    "    x = self.bn2c(x, training=training)\n",
    "\n",
    "    x = tf.keras.layers.add([x, input_tensor])\n",
    "    x = self.act3(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "\"\"\"A block that has a conv layer at shortcut.\n",
    "\n",
    "Note that from stage 3,\n",
    "the second conv layer at main path is with strides=(2, 2)\n",
    "And the shortcut should have strides=(2, 2) as well\n",
    "\n",
    "Args:\n",
    "  kernel_size: the kernel size of middle conv layer at main path\n",
    "  filters: list of integers, the filters of 3 conv layer at main path\n",
    "  stage: integer, current stage label, used for generating layer names\n",
    "  block: 'a','b'..., current block label, used for generating layer names\n",
    "  strides: Strides for the second conv layer in the block.\n",
    "\n",
    "Returns:\n",
    "  A Keras model instance for the block.\n",
    "\"\"\"\n",
    "class ConvBlock(tf.keras.Model):\n",
    "  def __init__(self, kernel_size, filters, stage, block, strides=(2, 2), activation='relu'):\n",
    "    self.activation = activation\n",
    "\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "\n",
    "    super().__init__(name='stage-' + str(stage) + '_block-' + block)\n",
    "\n",
    "    filters1, filters2, filters3 = filters\n",
    "    bn_axis = -1\n",
    "\n",
    "    self.conv2a = tf.keras.layers.Conv2D(filters1, (1, 1),\n",
    "                                         use_bias=False,\n",
    "                                         kernel_initializer='he_normal',\n",
    "                                         kernel_regularizer=tf.keras.regularizers.l2(L2_WEIGHT_DECAY),\n",
    "                                         name=conv_name_base + '2a')\n",
    "    self.bn2a = tf.keras.layers.BatchNormalization(axis=bn_axis,\n",
    "                                                   momentum=BATCH_NORM_DECAY,\n",
    "                                                   epsilon=BATCH_NORM_EPSILON,\n",
    "                                                   name=bn_name_base + '2a')\n",
    "    self.act1  = tf.keras.layers.Activation(self.activation)\n",
    "\n",
    "    self.conv2b = tf.keras.layers.Conv2D(filters2, kernel_size,\n",
    "                                         strides=strides,\n",
    "                                         padding='same',\n",
    "                                         use_bias=False,\n",
    "                                         kernel_initializer='he_normal',\n",
    "                                         kernel_regularizer=tf.keras.regularizers.l2(L2_WEIGHT_DECAY),\n",
    "                                         name=conv_name_base + '2b')\n",
    "    self.bn2b = tf.keras.layers.BatchNormalization(axis=bn_axis,\n",
    "                                                   momentum=BATCH_NORM_DECAY,\n",
    "                                                   epsilon=BATCH_NORM_EPSILON,\n",
    "                                                   name=bn_name_base + '2b')\n",
    "    self.act2  = tf.keras.layers.Activation(self.activation)\n",
    "\n",
    "    self.conv2c = tf.keras.layers.Conv2D(filters3, (1, 1),\n",
    "                                         use_bias=False,\n",
    "                                         kernel_initializer='he_normal',\n",
    "                                         kernel_regularizer=tf.keras.regularizers.l2(L2_WEIGHT_DECAY),\n",
    "                                         name=conv_name_base + '2c')\n",
    "    self.bn2c = tf.keras.layers.BatchNormalization(axis=bn_axis,\n",
    "                                                   momentum=BATCH_NORM_DECAY,\n",
    "                                                   epsilon=BATCH_NORM_EPSILON,\n",
    "                                                   name=bn_name_base + '2c')\n",
    "\n",
    "    self.conv2s = tf.keras.layers.Conv2D(filters3, (1, 1),\n",
    "                                         strides=strides,\n",
    "                                         use_bias=False,\n",
    "                                         kernel_initializer='he_normal',\n",
    "                                         kernel_regularizer=tf.keras.regularizers.l2(L2_WEIGHT_DECAY),\n",
    "                                         name=conv_name_base + '1')\n",
    "    self.bn2s = tf.keras.layers.BatchNormalization(axis=bn_axis,\n",
    "                                                   momentum=BATCH_NORM_DECAY,\n",
    "                                                   epsilon=BATCH_NORM_EPSILON,\n",
    "                                                   name=bn_name_base + '1')\n",
    "    self.act3  = tf.keras.layers.Activation(self.activation)\n",
    "\n",
    "  def call(self, input_tensor, training=False):\n",
    "    x = self.conv2a(input_tensor)\n",
    "    x = self.bn2a(x, training=training)\n",
    "    x = self.act1(x)\n",
    "\n",
    "    x = self.conv2b(x)\n",
    "    x = self.bn2b(x, training=training)\n",
    "    x = self.act2(x)\n",
    "\n",
    "    x = self.conv2c(x)\n",
    "    x = self.bn2c(x, training=training)\n",
    "\n",
    "    shortcut = self.conv2s(input_tensor)\n",
    "    shortcut = self.bn2s(shortcut, training=training)\n",
    "\n",
    "    x = tf.keras.layers.add([x, shortcut])\n",
    "    x = self.act3(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "\"\"\"Instantiates the ResNet50 architecture.\n",
    "\n",
    "Args:\n",
    "  num_classes: `int` number of classes for image classification.\n",
    "\n",
    "Returns:\n",
    "    A Keras model instance.\n",
    "\"\"\"\n",
    "class ResNet50(tf.keras.Model):\n",
    "  def __init__(self, num_classes, num_features, activation='relu'):\n",
    "    super().__init__(name='generator')\n",
    "    bn_axis = -1\n",
    "    self.activation = activation\n",
    "    self.num_classes = num_classes\n",
    "\n",
    "    self.conv1 = tf.keras.layers.Conv2D(32, (7, 7),\n",
    "                                        strides=(2, 2),\n",
    "                                        padding='valid',\n",
    "                                        use_bias=False,\n",
    "                                        kernel_initializer='he_normal',\n",
    "                                        kernel_regularizer=tf.keras.regularizers.l2(L2_WEIGHT_DECAY),\n",
    "                                        name='conv1')\n",
    "    self.bn1 = tf.keras.layers.BatchNormalization(axis=bn_axis,\n",
    "                                                  momentum=BATCH_NORM_DECAY,\n",
    "                                                  epsilon=BATCH_NORM_EPSILON,\n",
    "                                                  name='bn_conv1')\n",
    "    self.act1 = tf.keras.layers.Activation(self.activation, name=self.activation+'1')\n",
    "    self.max_pool1 = tf.keras.layers.MaxPooling2D((3, 3),\n",
    "                                                  strides=(2, 2),\n",
    "                                                  padding='same',\n",
    "                                                  name='max_pool1')\n",
    "\n",
    "    self.blocks = []\n",
    "    self.blocks.append(ConvBlock(3, [32, 32, 128], strides=(1, 1), stage=2, block='a', activation=self.activation))\n",
    "    self.blocks.append(IdentityBlock(3, [32, 32, 128], stage=2, block='b', activation=self.activation))\n",
    "\n",
    "    self.blocks.append(ConvBlock(3, [64, 64, 256], stage=3, block='a', activation=self.activation))\n",
    "    self.blocks.append(IdentityBlock(3, [64, 64, 256], stage=3, block='b', activation=self.activation))\n",
    "\n",
    "    self.blocks.append(ConvBlock(3, [64, 64, 256], stage=4, block='a', activation=self.activation))\n",
    "    self.blocks.append(IdentityBlock(3, [64, 64, 256], stage=4, block='b', activation=self.activation))\n",
    "\n",
    "    self.avg_pool = tf.keras.layers.GlobalAveragePooling2D(name='avg_pool')\n",
    "    self.fc1 = tf.keras.layers.Dense(num_features,\n",
    "                                     activation=self.activation,\n",
    "                                     kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.01),\n",
    "                                     kernel_regularizer=tf.keras.regularizers.l2(L2_WEIGHT_DECAY),\n",
    "                                     bias_regularizer=tf.keras.regularizers.l2(L2_WEIGHT_DECAY),\n",
    "                                     name='fc1')\n",
    "\n",
    "    self.logits = tf.keras.layers.Dense(num_classes,\n",
    "                                        activation=None,\n",
    "                                        kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.01),\n",
    "                                        kernel_regularizer=tf.keras.regularizers.l2(L2_WEIGHT_DECAY),\n",
    "                                        bias_regularizer=tf.keras.regularizers.l2(L2_WEIGHT_DECAY),\n",
    "                                        name='logits')\n",
    "    \n",
    "  def call(self, img_input, training=False):\n",
    "    x = self.conv1(img_input)\n",
    "    x = self.bn1(x, training=training)\n",
    "    x = self.act1(x)\n",
    "    x = self.max_pool1(x)\n",
    "\n",
    "    for block in self.blocks:\n",
    "      x = block(x, training=training)\n",
    "\n",
    "    x = self.avg_pool(x)\n",
    "    fc1 = self.fc1(x)\n",
    "    logits = self.logits(fc1)\n",
    "\n",
    "    return logits, fc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAMBDA = 10\n",
    "\n",
    "loss_obj = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "def discriminator_loss(real, generated):\n",
    "  real_loss = loss_obj(tf.ones_like(real), real)\n",
    "  generated_loss = loss_obj(tf.zeros_like(generated), generated)\n",
    "  total_disc_loss = real_loss + generated_loss\n",
    "  return total_disc_loss * 0.5\n",
    "\n",
    "def generator_loss(generated):\n",
    "  return loss_obj(tf.ones_like(generated), generated)\n",
    "\n",
    "def identity_loss(real_image, same_image):\n",
    "  loss = tf.reduce_mean(tf.abs(real_image - same_image))\n",
    "  return LAMBDA * 0.5 * loss\n",
    "\n",
    "def get_cross_entropy_loss(labels, logits):\n",
    "  loss = tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n",
    "  return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_gen_src_entropy_loss           = tf.keras.metrics.Mean(name='gen_src_entropy_loss')\n",
    "tb_gen_trg_entropy_loss_src_guide = tf.keras.metrics.Mean(name='gen_trg_entropy_loss_src_guide')\n",
    "tb_gen_trg_entropy_loss_src_data  = tf.keras.metrics.Mean(name='gen_trg_entropy_loss_src_data')\n",
    "tb_disc_src_loss                  = tf.keras.metrics.Mean(name='disc_src_loss')\n",
    "tb_disc_trg_loss                  = tf.keras.metrics.Mean(name='disc_trg_loss')\n",
    "tb_gen_src_loss                   = tf.keras.metrics.Mean(name='gen_src_loss')\n",
    "tb_gen_trg_loss                   = tf.keras.metrics.Mean(name='gen_trg_loss')\n",
    "tb_src_identity_loss              = tf.keras.metrics.Mean(name='src_identity_loss')\n",
    "tb_trg_identity_loss              = tf.keras.metrics.Mean(name='trg_identity_loss')\n",
    "tb_total_gen_s_loss               = tf.keras.metrics.Mean(name='total_gen_s_loss')   \n",
    "tb_total_gen_t_loss               = tf.keras.metrics.Mean(name='total_gen_t_loss')\n",
    "tb_total_clas_s_loss              = tf.keras.metrics.Mean(name='total_clas_s_loss')   \n",
    "tb_total_clas_t_loss              = tf.keras.metrics.Mean(name='total_clas_t_loss')\n",
    "temporal_test_acc    = tf.keras.metrics.CategoricalAccuracy(name='temporal_test_acc')\n",
    "source_train_acc     = tf.keras.metrics.CategoricalAccuracy(name='source_train_acc')\n",
    "source_test_acc      = tf.keras.metrics.CategoricalAccuracy(name='source_test_acc')\n",
    "office_test_acc      = tf.keras.metrics.CategoricalAccuracy(name='office_test_acc')\n",
    "server_train_acc     = tf.keras.metrics.CategoricalAccuracy(name='server_train_acc')\n",
    "server_test_acc      = tf.keras.metrics.CategoricalAccuracy(name='server_test_acc')\n",
    "conference_test_acc  = tf.keras.metrics.CategoricalAccuracy(name='conference_test_acc')\n",
    "\n",
    "@tf.function\n",
    "def train_step(src_x, src_y, trg_x, trg_y):\n",
    "  # persistent is set to True because the tape is used more than\n",
    "  # once to calculate the gradients.\n",
    "  with tf.GradientTape(persistent=True) as tape:\n",
    "    src_x_fake   = generator_s(trg_x,      training=True)\n",
    "    trg_x_cycled = generator_t(src_x_fake, training=True)\n",
    "\n",
    "    trg_x_fake   = generator_t(src_x,      training=True)\n",
    "    src_x_cycled = generator_s(trg_x_fake, training=True)\n",
    "    \n",
    "    src_x_same = generator_s(src_x, training=True)\n",
    "    trg_x_same = generator_t(trg_x, training=True)\n",
    "    \n",
    "    src_logits, _ = classifier_s(src_x, training=True)\n",
    "    trg_logits, _ = classifier_t(trg_x, training=True)\n",
    "    \n",
    "    src_logits_fake, _ = classifier_s(src_x_fake, training=True)\n",
    "    trg_logits_fake, _ = classifier_t(trg_x_fake, training=True)\n",
    "    \n",
    "    #Classifier##########################################\n",
    "    gen_src_entropy_loss = get_cross_entropy_loss(labels=src_y, \n",
    "                                                  logits=src_logits)\n",
    "    \n",
    "    gen_trg_entropy_loss_src_guide = get_cross_entropy_loss(labels=tf.nn.softmax(src_logits_fake), \n",
    "                                                            logits=trg_logits)   \n",
    "    gen_trg_entropy_loss_src_data  = get_cross_entropy_loss(labels=src_y, \n",
    "                                                            logits=trg_logits_fake)\n",
    "    \n",
    "    #Discriminator##########################################\n",
    "    disc_src      = discriminator_s(src_x, training=True)\n",
    "    disc_src_fake = discriminator_s(src_x_fake, training=True)\n",
    "    disc_src_loss = discriminator_loss(disc_src, disc_src_fake)\n",
    "\n",
    "    disc_trg      = discriminator_t(trg_x, training=True)\n",
    "    disc_trg_fake = discriminator_t(trg_x_fake, training=True)\n",
    "    disc_trg_loss = discriminator_loss(disc_trg, disc_trg_fake)\n",
    "\n",
    "    #Generator##########################################\n",
    "    gen_src_loss = generator_loss(disc_src_fake)\n",
    "    gen_trg_loss = generator_loss(disc_trg_fake)\n",
    "    \n",
    "    src_identity_loss = identity_loss(src_x, src_x_same)\n",
    "    trg_identity_loss = identity_loss(trg_x, trg_x_same)\n",
    "    \n",
    "    #Loss##########################################\n",
    "    total_gen_s_loss = gen_src_loss + src_identity_loss + gen_src_entropy_loss\n",
    "    total_gen_t_loss = gen_trg_loss + \\\n",
    "                       trg_identity_loss + \\\n",
    "                       gen_trg_entropy_loss_src_guide + \\\n",
    "                       gen_trg_entropy_loss_src_data \n",
    "    \n",
    "    total_clas_s_loss = gen_src_entropy_loss\n",
    "    total_clas_t_loss = gen_trg_entropy_loss_src_guide + gen_trg_entropy_loss_src_data \n",
    "    \n",
    "  # Calculate the gradients for generator and discriminator\n",
    "  classifier_s_gradients = tape.gradient(total_clas_s_loss, \n",
    "                                         classifier_s.trainable_variables)\n",
    "  classifier_t_gradients = tape.gradient(total_clas_t_loss, \n",
    "                                         classifier_t.trainable_variables)\n",
    "  \n",
    "  generator_s_gradients = tape.gradient(total_gen_s_loss, \n",
    "                                        generator_s.trainable_variables)\n",
    "  generator_t_gradients = tape.gradient(total_gen_t_loss, \n",
    "                                        generator_t.trainable_variables)\n",
    "  \n",
    "  discriminator_s_gradients = tape.gradient(disc_src_loss, \n",
    "                                            discriminator_s.trainable_variables)\n",
    "  discriminator_t_gradients = tape.gradient(disc_trg_loss, \n",
    "                                            discriminator_t.trainable_variables)\n",
    "    \n",
    "  # Apply the gradients to the optimizer\n",
    "  classifier_s_optimizer.apply_gradients(zip(classifier_s_gradients, \n",
    "                                             classifier_s.trainable_variables))\n",
    "\n",
    "  classifier_t_optimizer.apply_gradients(zip(classifier_t_gradients, \n",
    "                                             classifier_t.trainable_variables))\n",
    "  \n",
    "  generator_s_optimizer.apply_gradients(zip(generator_s_gradients, \n",
    "                                            generator_s.trainable_variables))\n",
    "\n",
    "  generator_t_optimizer.apply_gradients(zip(generator_t_gradients, \n",
    "                                            generator_t.trainable_variables))\n",
    "  \n",
    "  discriminator_s_optimizer.apply_gradients(zip(discriminator_s_gradients,\n",
    "                                                discriminator_s.trainable_variables))\n",
    "  \n",
    "  discriminator_t_optimizer.apply_gradients(zip(discriminator_t_gradients,\n",
    "                                                discriminator_t.trainable_variables))\n",
    "  \n",
    "  tb_gen_src_entropy_loss(gen_src_entropy_loss)\n",
    "  tb_gen_trg_entropy_loss_src_guide(gen_trg_entropy_loss_src_guide)\n",
    "  tb_gen_trg_entropy_loss_src_data(gen_trg_entropy_loss_src_data)\n",
    "  tb_disc_src_loss(disc_src_loss)\n",
    "  tb_disc_trg_loss(disc_trg_loss)\n",
    "  tb_gen_src_loss(gen_src_loss)\n",
    "  tb_gen_trg_loss(gen_trg_loss)\n",
    "  tb_src_identity_loss(src_identity_loss)\n",
    "  tb_trg_identity_loss(trg_identity_loss)\n",
    "  tb_total_gen_s_loss(total_gen_s_loss)      \n",
    "  tb_total_gen_t_loss(total_gen_t_loss)    \n",
    "  tb_total_clas_s_loss(total_clas_s_loss)    \n",
    "  tb_total_clas_t_loss(total_clas_t_loss)    \n",
    "  source_train_acc(src_y, tf.nn.softmax(src_logits))\n",
    "  server_train_acc(trg_y, tf.nn.softmax(trg_logits))\n",
    "  \n",
    "@tf.function\n",
    "def test_src(images):\n",
    "  _, logits =  generator_s(images, training=False)\n",
    "  return tf.nn.softmax(logits)\n",
    "\n",
    "@tf.function\n",
    "def test_trg(images):\n",
    "  _, logits =  generator_t(images, training=False)\n",
    "  return tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lr_schedule():\n",
    "  def __init__(self, init_lr=0.01, alpha=10, beta=0.75):\n",
    "    self.init_lr = init_lr\n",
    "    self.alpha = alpha\n",
    "    self.beta = beta\n",
    "    self.p = 0\n",
    "\n",
    "  def set_p(self, p):\n",
    "    self.p = p\n",
    "\n",
    "  def __call__(self):\n",
    "    return self.init_lr/((1+(self.alpha*self.p))**self.beta)\n",
    "\n",
    "generator_t     = unet_generator(1, norm_type='instancenorm')\n",
    "generator_s     = unet_generator(1, norm_type='instancenorm')\n",
    "discriminator_s = discriminator(norm_type='instancenorm', target=False)\n",
    "discriminator_t = discriminator(norm_type='instancenorm', target=False)\n",
    "classifier_s    = ResNet50(num_classes, num_features, \"selu\")\n",
    "classifier_t    = ResNet50(num_classes, num_features, \"selu\")\n",
    "\n",
    "learning_rate             = lr_schedule(init_lr=init_lr)\n",
    "classifier_s_optimizer    = tf.keras.optimizers.Adam(learning_rate, beta_1=0.5)\n",
    "classifier_t_optimizer    = tf.keras.optimizers.Adam(learning_rate, beta_1=0.5)\n",
    "generator_s_optimizer     = tf.keras.optimizers.Adam(learning_rate, beta_1=0.5)\n",
    "generator_t_optimizer     = tf.keras.optimizers.Adam(learning_rate, beta_1=0.5)\n",
    "discriminator_s_optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.5)\n",
    "discriminator_t_optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.5)\n",
    "\n",
    "summary_writer = tf.summary.create_file_writer(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "  print(epoch)\n",
    "  learning_rate.set_p(epoch/epochs)\n",
    "\n",
    "  for source_data, server_data in zip(src_train_set, server_train_set):\n",
    "    train_step(source_data[0], source_data[1], server_data[0], server_data[1])\n",
    "    \n",
    "  for data in time_test_set:\n",
    "    temporal_test_acc(test_trg(data[0]), data[1])\n",
    "\n",
    "  for data in src_test_set:\n",
    "    source_test_acc(test_trg(data[0]), data[1])\n",
    "\n",
    "  for data in office_test_set:\n",
    "    office_test_acc(test_trg(data[0]), data[1])\n",
    "\n",
    "  for data in server_test_set:\n",
    "    server_test_acc(test_trg(data[0]), data[1])\n",
    "\n",
    "  for data in conf_test_set:\n",
    "    conference_test_acc(test_trg(data[0]), data[1])\n",
    "\n",
    "  with summary_writer.as_default():\n",
    "    tf.summary.scalar(\"tb_gen_src_entropy_loss\", tb_gen_src_entropy_loss.result(), step=epoch)\n",
    "    tf.summary.scalar(\"tb_gen_trg_entropy_loss_src_guide\", tb_gen_trg_entropy_loss_src_guide.result(), step=epoch)\n",
    "    tf.summary.scalar(\"tb_gen_trg_entropy_loss_src_data\", tb_gen_trg_entropy_loss_src_data.result(), step=epoch)\n",
    "    tf.summary.scalar(\"tb_disc_src_loss\", tb_disc_src_loss.result(), step=epoch)\n",
    "    tf.summary.scalar(\"tb_disc_trg_loss\", tb_disc_trg_loss.result(), step=epoch)\n",
    "    tf.summary.scalar(\"tb_gen_src_loss\", tb_gen_src_loss.result(), step=epoch)\n",
    "    tf.summary.scalar(\"tb_gen_trg_loss\", tb_gen_trg_loss.result(), step=epoch)\n",
    "    tf.summary.scalar(\"tb_src_identity_loss\", tb_src_identity_loss.result(), step=epoch)\n",
    "    tf.summary.scalar(\"tb_trg_identity_loss\", tb_trg_identity_loss.result(), step=epoch)\n",
    "    tf.summary.scalar(\"tb_total_gen_s_loss\", tb_total_gen_s_loss.result(), step=epoch)\n",
    "    tf.summary.scalar(\"tb_total_gen_t_loss\", tb_total_gen_t_loss.result(), step=epoch)\n",
    "    tf.summary.scalar(\"tb_total_clas_s_loss\", tb_total_clas_s_loss.result(), step=epoch)\n",
    "    tf.summary.scalar(\"tb_total_clas_t_loss\", tb_total_clas_t_loss.result(), step=epoch)    \n",
    "    tf.summary.scalar(\"source_train_acc\", source_train_acc.result(), step=epoch)\n",
    "    tf.summary.scalar(\"server_train_acc\", server_train_acc.result(), step=epoch)\n",
    "    tf.summary.scalar(\"temporal_test_acc\", temporal_test_acc.result(), step=epoch)\n",
    "    tf.summary.scalar(\"source_test_acc\", source_test_acc.result(), step=epoch)\n",
    "    tf.summary.scalar(\"office_test_acc\", office_test_acc.result(), step=epoch)\n",
    "    tf.summary.scalar(\"server_test_acc\", server_test_acc.result(), step=epoch)\n",
    "    tf.summary.scalar(\"conference_test_acc\", conference_test_acc.result(), step=epoch)\n",
    "    \n",
    "  tb_gen_src_entropy_loss.reset_states()\n",
    "  tb_gen_trg_entropy_loss_src_guide.reset_states()\n",
    "  tb_gen_trg_entropy_loss_src_data.reset_states()\n",
    "  tb_disc_src_loss.reset_states()\n",
    "  tb_disc_trg_loss.reset_states()\n",
    "  tb_gen_src_loss.reset_states()\n",
    "  tb_gen_trg_loss.reset_states()\n",
    "  tb_src_identity_loss.reset_states()\n",
    "  tb_trg_identity_loss.reset_states()\n",
    "  tb_total_gen_s_loss.reset_states()    \n",
    "  tb_total_gen_t_loss.reset_states() \n",
    "  tb_total_clas_s_loss.reset_states() \n",
    "  tb_total_clas_t_loss.reset_states()     \n",
    "  source_train_acc.reset_states()\n",
    "  server_train_acc.reset_states()\n",
    "  temporal_test_acc.reset_states()\n",
    "  source_test_acc.reset_states()\n",
    "  office_test_acc.reset_states()\n",
    "  server_test_acc.reset_states()\n",
    "  conference_test_acc.reset_states()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
