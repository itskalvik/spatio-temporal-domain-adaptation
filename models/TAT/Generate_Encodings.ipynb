{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "repo_path = \"/home/kjakkala/mmwave\"\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "\n",
    "import sys\n",
    "sys.path.append(os.path.join(repo_path, 'models'))\n",
    "\n",
    "from utils import *\n",
    "from resnet import ResNet50\n",
    "from pix2pix import upsample\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path    = os.path.join(repo_path, 'data')\n",
    "num_classes     = 9\n",
    "batch_size      = 1\n",
    "train_src_days  = 3\n",
    "train_trg_days  = 0\n",
    "train_trg_env_days = 2\n",
    "num_features    = 256\n",
    "activation_fn   = 'selu'\n",
    "checkpoint_path = os.path.join(repo_path, 'checkpoints/Baselines/ckpt')\n",
    "encodings_file  = os.path.join(repo_path, 'data/encodings_server_conf.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9127, 256, 256, 1) (9127, 2) \n",
      " ['arahman3', 'harika', 'hchen32', 'jlaivins', 'kjakkala', 'pjanakar', 'ppinyoan', 'pwang13', 'upattnai', 'wrang']\n",
      "(8737, 256, 256, 1) (8737, 2)\n",
      "(8547, 256, 256, 1) (8547, 2) \n",
      " ['arahman3', 'hchen32', 'jlaivins', 'kjakkala', 'pjanakar', 'ppinyoan', 'pwang13', 'upattnai', 'wrang']\n",
      "Final shapes: \n",
      "(2308, 256, 256, 1) (2308, 9) (257, 256, 256, 1) (257, 9) (0, 256, 256, 1) (0, 9) (5982, 256, 256, 1) (5982, 9)\n",
      "(900, 256, 256, 1) (900, 9) (450, 256, 256, 1) (450, 9)\n",
      "(898, 256, 256, 1) (898, 9) (448, 256, 256, 1) (448, 9)\n",
      "(899, 256, 256, 1) (899, 9)\n"
     ]
    }
   ],
   "source": [
    "X_data, y_data, classes = get_h5dataset(os.path.join(dataset_path, 'source_data.h5'))\n",
    "X_data = resize_data(X_data)\n",
    "print(X_data.shape, y_data.shape, \"\\n\", classes)\n",
    "\n",
    "X_data, y_data = balance_dataset(X_data, y_data, \n",
    "                                 num_days=10, \n",
    "                                 num_classes=len(classes), \n",
    "                                 max_samples_per_class=95)\n",
    "print(X_data.shape, y_data.shape)\n",
    "\n",
    "#remove harika's data (incomplete data)\n",
    "X_data = np.delete(X_data, np.where(y_data[:, 0] == 1)[0], 0)\n",
    "y_data = np.delete(y_data, np.where(y_data[:, 0] == 1)[0], 0)\n",
    "\n",
    "#update labes to handle 9 classes instead of 10\n",
    "y_data[y_data[:, 0] >= 2, 0] -= 1\n",
    "del classes[1]\n",
    "print(X_data.shape, y_data.shape, \"\\n\", classes)\n",
    "\n",
    "#split days of data to train and test\n",
    "X_src = X_data[y_data[:, 1] < train_src_days]\n",
    "y_src = y_data[y_data[:, 1] < train_src_days, 0]\n",
    "y_src = np.eye(len(classes))[y_src]\n",
    "X_train_src, X_test_src, y_train_src, y_test_src = train_test_split(X_src,\n",
    "                                                                    y_src,\n",
    "                                                                    stratify=y_src,\n",
    "                                                                    test_size=0.10,\n",
    "                                                                    random_state=42)\n",
    "\n",
    "X_trg = X_data[y_data[:, 1] >= train_src_days]\n",
    "y_trg = y_data[y_data[:, 1] >= train_src_days]\n",
    "X_train_trg = X_trg[y_trg[:, 1] < train_src_days+train_trg_days]\n",
    "y_train_trg = y_trg[y_trg[:, 1] < train_src_days+train_trg_days, 0]\n",
    "y_train_trg = np.eye(len(classes))[y_train_trg]\n",
    "\n",
    "X_test_trg = X_data[y_data[:, 1] >= train_src_days+train_trg_days]\n",
    "y_test_trg = y_data[y_data[:, 1] >= train_src_days+train_trg_days, 0]\n",
    "y_test_trg = np.eye(len(classes))[y_test_trg]\n",
    "\n",
    "del X_src, y_src, X_trg, y_trg, X_data, y_data\n",
    "\n",
    "#mean center and normalize dataset\n",
    "X_train_src, src_mean = mean_center(X_train_src)\n",
    "X_train_src, src_min, src_ptp = normalize(X_train_src)\n",
    "\n",
    "X_test_src, _    = mean_center(X_test_src, src_mean)\n",
    "X_test_src, _, _ = normalize(X_test_src, src_min, src_ptp)\n",
    "\n",
    "if(X_train_trg.shape[0] != 0):\n",
    "  X_train_trg, trg_mean = mean_center(X_train_trg)\n",
    "  X_train_trg, trg_min, trg_ptp = normalize(X_train_trg)\n",
    "\n",
    "  X_test_trg, _    = mean_center(X_test_trg, trg_mean)\n",
    "  X_test_trg, _, _ = normalize(X_test_trg, trg_min, trg_ptp)  \n",
    "else:\n",
    "  X_test_trg, _    = mean_center(X_test_trg, src_mean)\n",
    "  X_test_trg, _, _ = normalize(X_test_trg, src_min, src_ptp)\n",
    "  \n",
    "X_train_src = X_train_src.astype(np.float32)\n",
    "y_train_src = y_train_src.astype(np.uint8)\n",
    "X_test_src  = X_test_src.astype(np.float32)\n",
    "y_test_src  = y_test_src.astype(np.uint8)\n",
    "X_train_trg = X_train_trg.astype(np.float32)\n",
    "y_train_trg = y_train_trg.astype(np.uint8)\n",
    "X_test_trg  = X_test_trg.astype(np.float32)\n",
    "y_test_trg  = y_test_trg.astype(np.uint8)\n",
    "print(\"Final shapes: \")\n",
    "print(X_train_src.shape, y_train_src.shape,  X_test_src.shape, y_test_src.shape, X_train_trg.shape, y_train_trg.shape, X_test_trg.shape, y_test_trg.shape)\n",
    "\n",
    "X_train_conf,   y_train_conf,   X_test_conf,   y_test_conf   = get_trg_data(os.path.join(dataset_path, 'target_conf_data.h5'),   classes, train_trg_env_days)\n",
    "X_train_server, y_train_server, X_test_server, y_test_server = get_trg_data(os.path.join(dataset_path, 'target_server_data.h5'), classes, train_trg_env_days)\n",
    "_             , _             , X_data_office, y_data_office = get_trg_data(os.path.join(dataset_path, 'target_office_data.h5'), classes, 0)\n",
    "\n",
    "print(X_train_conf.shape,   y_train_conf.shape,    X_test_conf.shape,   y_test_conf.shape)\n",
    "print(X_train_server.shape, y_train_server.shape,  X_test_server.shape, y_test_server.shape)\n",
    "print(X_data_office.shape,  y_data_office.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/kjakkala/mmwave/checkpoints/Baselines/ckpt/ckpt-80\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f8a5001e390>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "  def __init__(self, activation='relu'):\n",
    "    super().__init__(name='decoder')\n",
    "    self.up_stack = [\n",
    "      upsample(256, 4, \"batchnorm\", activation),\n",
    "      upsample(256, 4, \"batchnorm\", activation),\n",
    "      upsample(256, 4, \"batchnorm\", activation),\n",
    "    ]\n",
    "    \n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "    self.last_conv = tf.keras.layers.Conv2DTranspose(1, 4, \n",
    "                                                     strides=2,\n",
    "                                                     padding='same', \n",
    "                                                     kernel_initializer=initializer,\n",
    "                                                     activation='tanh')\n",
    "\n",
    "  def call(self, x, training=False):\n",
    "    for up in self.up_stack:\n",
    "      x = up(x, training=training)\n",
    "    x = self.last_conv(x)\n",
    "    return x\n",
    "  \n",
    "\"\"\"Instantiates the ResNet50 architecture with discriminator and GRL layer.\n",
    "\n",
    "Args:\n",
    "  num_classes: `int` number of classes for image classification.\n",
    "\n",
    "Returns:\n",
    "    A Keras model instance.\n",
    "\"\"\"\n",
    "class ReconstructionResNet50(ResNet50):\n",
    "  def __init__(self, num_classes, num_features, activation='relu'):\n",
    "    super().__init__(num_classes, num_features, activation)\n",
    "    self.decoder = Decoder(activation=self.activation)\n",
    "    \n",
    "  def call(self, img_input, training=False):\n",
    "    x = self.conv1(img_input)\n",
    "    x = self.bn1(x, training=training)\n",
    "    x = self.act1(x)\n",
    "    x = self.max_pool1(x)\n",
    "\n",
    "    for block in self.blocks:\n",
    "      x = block(x, training=training)\n",
    "\n",
    "    decoded = self.decoder(x, training=training)\n",
    "    x = self.avg_pool(x)\n",
    "    fc1 = self.fc1(x)\n",
    "    logits = self.logits(fc1)\n",
    "\n",
    "    return logits, fc1, decoded\n",
    "  \n",
    "@tf.function\n",
    "def predict(images):\n",
    "  logits, fc1, _ = model(images, training=False)\n",
    "  return tf.nn.softmax(logits), fc1\n",
    "\n",
    "model = ReconstructionResNet50(num_classes, num_features, activation_fn)\n",
    "ckpt  = tf.train.Checkpoint(model=model)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "print(ckpt_manager.checkpoints[-1])\n",
    "ckpt.restore(ckpt_manager.latest_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acc_encodings(data, labels):\n",
    "  acc = tf.keras.metrics.CategoricalAccuracy(name='acc')\n",
    "  data_list = []\n",
    "  for i in range(len(data)):\n",
    "    logits, encodings = predict(np.expand_dims(data[i], axis=0))\n",
    "    data_list.append(np.array(encodings))\n",
    "    acc(logits, labels[i])\n",
    "  return np.squeeze(data_list), float(acc.result())\n",
    "\n",
    "X_train_src, X_train_src_acc  = get_acc_encodings(X_train_src, y_train_src)\n",
    "X_test_src,  X_test_src_acc   = get_acc_encodings(X_test_src, y_test_src)\n",
    "X_train_trg, X_train_trg_acc  = get_acc_encodings(X_train_trg, y_train_trg)\n",
    "X_test_trg,  X_test_trg_acc   = get_acc_encodings(X_test_trg, y_test_trg)\n",
    "\n",
    "X_train_conf, X_train_conf_acc = get_acc_encodings(X_train_conf, y_train_conf)\n",
    "X_test_conf,  X_test_conf_acc  = get_acc_encodings(X_test_conf, y_test_conf)\n",
    "\n",
    "X_train_server, X_train_server_acc = get_acc_encodings(X_train_server, y_train_server)\n",
    "X_test_server,  X_test_server_acc  = get_acc_encodings(X_test_server, y_test_server)\n",
    "\n",
    "X_data_office, X_data_office_acc = get_acc_encodings(X_data_office, y_data_office)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracies and shapes: \n",
      "X_train_src:    1.0000 | (2308, 256)  (2308, 9) \n",
      "X_test_src:     0.9844 | (257, 256)   (257, 9)  \n",
      "X_train_trg:    0.0000 | (0,)         (0, 9)    \n",
      "X_test_trg:     0.9502 | (5982, 256)  (5982, 9) \n",
      "X_train_conf:   0.8944 | (900, 256)   (900, 9)  \n",
      "X_test_conf:    0.7556 | (450, 256)   (450, 9)  \n",
      "X_train_server: 0.8853 | (898, 256)   (898, 9)  \n",
      "X_test_server:  0.7188 | (448, 256)   (448, 9)  \n",
      "X_data_office:  0.8276 | (899, 256)   (899, 9)  \n"
     ]
    }
   ],
   "source": [
    "print(\"Final accuracies and shapes: \")\n",
    "print(\"X_train_src:    {:.4f} | {:<12} {:<10}\".format(X_train_src_acc, str(X_train_src.shape),    str(y_train_src.shape)))\n",
    "print(\"X_test_src:     {:.4f} | {:<12} {:<10}\".format(X_test_src_acc, str(X_test_src.shape),     str(y_test_src.shape)))\n",
    "print(\"X_train_trg:    {:.4f} | {:<12} {:<10}\".format(X_train_trg_acc, str(X_train_trg.shape),    str(y_train_trg.shape)))\n",
    "print(\"X_test_trg:     {:.4f} | {:<12} {:<10}\".format(X_test_trg_acc, str(X_test_trg.shape),     str(y_test_trg.shape)))\n",
    "print(\"X_train_conf:   {:.4f} | {:<12} {:<10}\".format(X_train_conf_acc, str(X_train_conf.shape),   str(y_train_conf.shape)))\n",
    "print(\"X_test_conf:    {:.4f} | {:<12} {:<10}\".format(X_test_conf_acc, str(X_test_conf.shape),    str(y_test_conf.shape)))\n",
    "print(\"X_train_server: {:.4f} | {:<12} {:<10}\".format(X_train_server_acc, str(X_train_server.shape), str(y_train_server.shape)))\n",
    "print(\"X_test_server:  {:.4f} | {:<12} {:<10}\".format(X_test_server_acc, str(X_test_server.shape),  str(y_test_server.shape)))\n",
    "print(\"X_data_office:  {:.4f} | {:<12} {:<10}\".format(X_data_office_acc, str(X_data_office.shape),  str(y_data_office.shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf = h5py.File(encodings_file, 'w')\n",
    "\n",
    "hf.create_dataset('X_train_src', data=X_train_src)\n",
    "hf.create_dataset('y_train_src', data=y_train_src)\n",
    "hf.create_dataset('X_test_src', data=X_test_src)\n",
    "hf.create_dataset('y_test_src', data=y_test_src)\n",
    "hf.create_dataset('X_train_trg', data=X_train_trg)\n",
    "hf.create_dataset('y_train_trg', data=y_train_trg)\n",
    "hf.create_dataset('X_test_trg', data=X_test_trg)\n",
    "hf.create_dataset('y_test_trg', data=y_test_trg)\n",
    "\n",
    "hf.create_dataset('X_train_conf', data=X_train_conf)\n",
    "hf.create_dataset('y_train_conf', data=y_train_conf)\n",
    "hf.create_dataset('X_test_conf', data=X_test_conf)\n",
    "hf.create_dataset('y_test_conf', data=y_test_conf)\n",
    "\n",
    "hf.create_dataset('X_train_server', data=X_train_server)\n",
    "hf.create_dataset('y_train_server', data=y_train_server)\n",
    "hf.create_dataset('X_test_server', data=X_test_server)\n",
    "hf.create_dataset('y_test_server', data=y_test_server)\n",
    "\n",
    "hf.create_dataset('X_data_office', data=X_data_office)\n",
    "hf.create_dataset('y_data_office', data=y_data_office)\n",
    "\n",
    "hf.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
