{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kjakkala/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/kjakkala/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/kjakkala/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/kjakkala/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/kjakkala/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/kjakkala/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/kjakkala/.local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/kjakkala/.local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/kjakkala/.local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/kjakkala/.local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/kjakkala/.local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/kjakkala/.local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes     = 14\n",
    "batch_size      = 16\n",
    "epochs          = 1000\n",
    "learning_rate   = 0.0001\n",
    "num_features    = 256\n",
    "gen_activation  = 'selu'\n",
    "notes           = \"resnet_pretraining\" \n",
    "\n",
    "log_data = \"num_classes-{}_batch_size-{}_lr-{}_num_features-{}_gen_act-{}_notes-{}\".format(num_classes,\n",
    "                                                                                           batch_size,\n",
    "                                                                                           learning_rate,\n",
    "                                                                                           num_features,\n",
    "                                                                                           gen_activation,\n",
    "                                                                                           notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1976, 128, 1024, 1) (1976,) \n",
      " ['amit_pandit', 'archit', 'bivor', 'champ', 'dan_han', 'dinesh', 'donglin', 'giang_dao', 'haoxin_wang', 'kalvik', 'kayan', 'minwoo_lee', 'prabhu', 'pu_wang', 'shamika', 'siqi_huang', 'sneha', 'srisanmathi', 'ting', 'wei'] 20\n",
      "(1387, 128, 1024, 1) (1387,) \n",
      " ['amit_pandit', 'archit', 'dan_han', 'dinesh', 'donglin', 'giang_dao', 'haoxin_wang', 'kayan', 'minwoo_lee', 'shamika', 'siqi_huang', 'sneha', 'srisanmathi', 'ting'] 14\n"
     ]
    }
   ],
   "source": [
    "#Read data\n",
    "hf = h5py.File('/home/kjakkala/mmwave/data/original_source_data.h5', 'r')\n",
    "X_data = np.expand_dims(hf.get('X_data'), axis=-1).astype(np.float32)\n",
    "y_data = np.array(hf.get('y_data')).astype(np.int32)[:, 0]\n",
    "classes = list(hf.get('classes'))\n",
    "classes = [n.decode(\"ascii\", \"ignore\") for n in classes]\n",
    "hf.close()\n",
    "print(X_data.shape, y_data.shape, \"\\n\", classes, len(classes))\n",
    "\n",
    "#remove data for people in new dataset\n",
    "del_labels = [\"bivor\", \"champ\", \"kalvik\", \"prabhu\", \"pu_wang\", \"wei\"]\n",
    "\n",
    "for iden in del_labels:\n",
    "  X_data = np.delete(X_data, np.where(y_data == classes.index(iden))[0], 0)\n",
    "  y_data = np.delete(y_data, np.where(y_data == classes.index(iden))[0], 0)\n",
    "  y_data[y_data >= classes.index(iden)] -= 1\n",
    "  del classes[classes.index(iden)]\n",
    "\n",
    "print(X_data.shape, y_data.shape, \"\\n\", classes, len(classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1248, 128, 1024, 1) (1248, 14) (139, 128, 1024, 1) (139, 14)\n"
     ]
    }
   ],
   "source": [
    "#split data into train and test\n",
    "y_data = (np.eye(len(classes))[y_data]).astype(np.int32)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data,\n",
    "                                                    y_data,\n",
    "                                                    stratify=y_data,\n",
    "                                                    test_size=0.10,\n",
    "                                                    random_state=42)\n",
    "\n",
    "print(X_train.shape, y_train.shape,  X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1248, 128, 1024, 1) (1248, 14) (139, 128, 1024, 1) (139, 14)\n"
     ]
    }
   ],
   "source": [
    "#standardise dataset\n",
    "src_mean = np.mean(X_train)\n",
    "X_train -= src_mean\n",
    "src_std  = np.std(X_train)\n",
    "X_train /= src_std\n",
    "X_test -= src_mean\n",
    "X_test /= src_std\n",
    "\n",
    "print(X_train.shape, y_train.shape,  X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get tf.data objects for each set\n",
    "src_train_set = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "src_train_set = src_train_set.shuffle(X_train.shape[0])\n",
    "src_train_set = src_train_set.batch(batch_size, drop_remainder=True)\n",
    "src_train_set = src_train_set.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "src_test_set = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "src_test_set = src_test_set.batch(batch_size, drop_remainder=False)\n",
    "src_test_set = src_test_set.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "L2_WEIGHT_DECAY = 1e-4\n",
    "BATCH_NORM_DECAY = 0.9\n",
    "BATCH_NORM_EPSILON = 1e-5\n",
    "\n",
    "class GaussianNoise(tf.keras.layers.Layer):\n",
    "  def __init__(self, std):\n",
    "    super(GaussianNoise, self).__init__()\n",
    "    self.std = std\n",
    "\n",
    "  def build(self, input_shapes):\n",
    "    pass\n",
    "\n",
    "  def call(self, inputs, training=False):\n",
    "    eps = tf.random.normal(shape=tf.shape(inputs), mean=0.0, stddev=self.std)\n",
    "    return tf.where(training, inputs + eps, inputs)\n",
    "\n",
    "class IdentityBlock(tf.keras.Model):\n",
    "  def __init__(self, kernel_size, filters, stage, block, activation='relu'):\n",
    "    self.activation = activation\n",
    "    \n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "\n",
    "    super().__init__(name='stage-' + str(stage) + '_block-' + block)\n",
    "\n",
    "    filters1, filters2, filters3 = filters\n",
    "    bn_axis = -1\n",
    "\n",
    "    self.conv2a = tf.keras.layers.Conv2D(filters1, (1, 1),\n",
    "                                         use_bias=False,\n",
    "                                         kernel_initializer='he_normal',\n",
    "                                         kernel_regularizer=tf.keras.regularizers.l2(L2_WEIGHT_DECAY),\n",
    "                                         name=conv_name_base + '2a')\n",
    "    self.bn2a = tf.keras.layers.BatchNormalization(axis=bn_axis,\n",
    "                                                   momentum=BATCH_NORM_DECAY,\n",
    "                                                   epsilon=BATCH_NORM_EPSILON,\n",
    "                                                   name=bn_name_base + '2a')\n",
    "\n",
    "    self.conv2b = tf.keras.layers.Conv2D(filters2, kernel_size,\n",
    "                                         padding='same',\n",
    "                                         use_bias=False,\n",
    "                                         kernel_initializer='he_normal',\n",
    "                                         kernel_regularizer=tf.keras.regularizers.l2(L2_WEIGHT_DECAY),\n",
    "                                         name=conv_name_base + '2b')\n",
    "    self.bn2b = tf.keras.layers.BatchNormalization(axis=bn_axis,\n",
    "                                                   momentum=BATCH_NORM_DECAY,\n",
    "                                                   epsilon=BATCH_NORM_EPSILON,\n",
    "                                                   name=bn_name_base + '2b')\n",
    "\n",
    "    self.conv2c = tf.keras.layers.Conv2D(filters3, (1, 1),\n",
    "                                         use_bias=False,\n",
    "                                         kernel_initializer='he_normal',\n",
    "                                         kernel_regularizer=tf.keras.regularizers.l2(L2_WEIGHT_DECAY),\n",
    "                                         name=conv_name_base + '2c')\n",
    "    self.bn2c = tf.keras.layers.BatchNormalization(axis=bn_axis,\n",
    "                                                   momentum=BATCH_NORM_DECAY,\n",
    "                                                   epsilon=BATCH_NORM_EPSILON,\n",
    "                                                   name=bn_name_base + '2c')\n",
    "\n",
    "  def call(self, input_tensor, training=False):\n",
    "    x = self.conv2a(input_tensor)\n",
    "    x = self.bn2a(x, training=training)\n",
    "    x = tf.keras.layers.Activation(self.activation)(x)\n",
    "\n",
    "    x = self.conv2b(x)\n",
    "    x = self.bn2b(x, training=training)\n",
    "    x = tf.keras.layers.Activation(self.activation)(x)\n",
    "\n",
    "    x = self.conv2c(x)\n",
    "    x = self.bn2c(x, training=training)\n",
    "\n",
    "    x = tf.keras.layers.add([x, input_tensor])\n",
    "    x = tf.keras.layers.Activation(self.activation)(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "\"\"\"A block that has a conv layer at shortcut.\n",
    "\n",
    "Note that from stage 3,\n",
    "the second conv layer at main path is with strides=(2, 2)\n",
    "And the shortcut should have strides=(2, 2) as well\n",
    "\n",
    "Args:\n",
    "  kernel_size: the kernel size of middle conv layer at main path\n",
    "  filters: list of integers, the filters of 3 conv layer at main path\n",
    "  stage: integer, current stage label, used for generating layer names\n",
    "  block: 'a','b'..., current block label, used for generating layer names\n",
    "  strides: Strides for the second conv layer in the block.\n",
    "\n",
    "Returns:\n",
    "  A Keras model instance for the block.\n",
    "\"\"\"\n",
    "class ConvBlock(tf.keras.Model):\n",
    "  def __init__(self, kernel_size, filters, stage, block, strides=(2, 2), activation='relu'):\n",
    "    self.activation = activation\n",
    "    \n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "\n",
    "    super().__init__(name='stage-' + str(stage) + '_block-' + block)\n",
    "\n",
    "    filters1, filters2, filters3 = filters\n",
    "    bn_axis = -1\n",
    "\n",
    "    self.conv2a = tf.keras.layers.Conv2D(filters1, (1, 1),\n",
    "                                         use_bias=False,\n",
    "                                         kernel_initializer='he_normal',\n",
    "                                         kernel_regularizer=tf.keras.regularizers.l2(L2_WEIGHT_DECAY),\n",
    "                                         name=conv_name_base + '2a')\n",
    "    self.bn2a = tf.keras.layers.BatchNormalization(axis=bn_axis,\n",
    "                                                   momentum=BATCH_NORM_DECAY,\n",
    "                                                   epsilon=BATCH_NORM_EPSILON,\n",
    "                                                   name=bn_name_base + '2a')\n",
    "\n",
    "    self.conv2b = tf.keras.layers.Conv2D(filters2, kernel_size,\n",
    "                                         strides=strides,\n",
    "                                         padding='same',\n",
    "                                         use_bias=False,\n",
    "                                         kernel_initializer='he_normal',\n",
    "                                         kernel_regularizer=tf.keras.regularizers.l2(L2_WEIGHT_DECAY),\n",
    "                                         name=conv_name_base + '2b')\n",
    "    self.bn2b = tf.keras.layers.BatchNormalization(axis=bn_axis,\n",
    "                                                   momentum=BATCH_NORM_DECAY,\n",
    "                                                   epsilon=BATCH_NORM_EPSILON,\n",
    "                                                   name=bn_name_base + '2b')\n",
    "\n",
    "    self.conv2c = tf.keras.layers.Conv2D(filters3, (1, 1),\n",
    "                                         use_bias=False,\n",
    "                                         kernel_initializer='he_normal',\n",
    "                                         kernel_regularizer=tf.keras.regularizers.l2(L2_WEIGHT_DECAY),\n",
    "                                         name=conv_name_base + '2c')\n",
    "    self.bn2c = tf.keras.layers.BatchNormalization(axis=bn_axis,\n",
    "                                                   momentum=BATCH_NORM_DECAY,\n",
    "                                                   epsilon=BATCH_NORM_EPSILON,\n",
    "                                                   name=bn_name_base + '2c')\n",
    "\n",
    "    self.conv2s = tf.keras.layers.Conv2D(filters3, (1, 1),\n",
    "                                         strides=strides,\n",
    "                                         use_bias=False,\n",
    "                                         kernel_initializer='he_normal',\n",
    "                                         kernel_regularizer=tf.keras.regularizers.l2(L2_WEIGHT_DECAY),\n",
    "                                         name=conv_name_base + '1')\n",
    "    self.bn2s = tf.keras.layers.BatchNormalization(axis=bn_axis,\n",
    "                                                   momentum=BATCH_NORM_DECAY,\n",
    "                                                   epsilon=BATCH_NORM_EPSILON,\n",
    "                                                   name=bn_name_base + '1')\n",
    "    self.gauss1   = GaussianNoise(1)\n",
    "\n",
    "  def call(self, input_tensor, training=False):\n",
    "    x = self.conv2a(input_tensor)\n",
    "    x = self.bn2a(x, training=training)\n",
    "    x = tf.keras.layers.Activation(self.activation)(x)\n",
    "\n",
    "    x = self.conv2b(x)\n",
    "    x = self.bn2b(x, training=training)\n",
    "    x = tf.keras.layers.Activation(self.activation)(x)\n",
    "\n",
    "    x = self.conv2c(x)\n",
    "    x = self.bn2c(x, training=training)\n",
    "\n",
    "    shortcut = self.conv2s(input_tensor)\n",
    "    shortcut = self.bn2s(shortcut, training=training)\n",
    "\n",
    "    x = tf.keras.layers.add([x, shortcut])\n",
    "    x = tf.keras.layers.Activation(self.activation)(x)\n",
    "    x = self.gauss1(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "\"\"\"Instantiates the ResNet50 architecture.\n",
    "\n",
    "Args:\n",
    "  num_classes: `int` number of classes for image classification.\n",
    "\n",
    "Returns:\n",
    "    A Keras model instance.\n",
    "\"\"\"\n",
    "class ResNet50(tf.keras.Model):\n",
    "  def __init__(self, num_classes, num_features, activation='relu'):\n",
    "    super().__init__(name='generator')\n",
    "    bn_axis = -1\n",
    "    self.activation = activation\n",
    "\n",
    "    self.conv1 = tf.keras.layers.Conv2D(32, (7, 7),\n",
    "                                        strides=(2, 2),\n",
    "                                        padding='valid',\n",
    "                                        use_bias=False,\n",
    "                                        kernel_initializer='he_normal',\n",
    "                                        kernel_regularizer=tf.keras.regularizers.l2(L2_WEIGHT_DECAY),\n",
    "                                        name='conv1')\n",
    "    self.bn1 = tf.keras.layers.BatchNormalization(axis=bn_axis,\n",
    "                                                  momentum=BATCH_NORM_DECAY,\n",
    "                                                  epsilon=BATCH_NORM_EPSILON,\n",
    "                                                  name='bn_conv1')\n",
    "    self.act1 = tf.keras.layers.Activation(self.activation, name=self.activation+'1')\n",
    "    self.max_pool1 = tf.keras.layers.MaxPooling2D((3, 3),\n",
    "                                                  strides=(2, 2),\n",
    "                                                  padding='same',\n",
    "                                                  name='max_pool1')\n",
    "\n",
    "    self.blocks = []\n",
    "    self.blocks.append(ConvBlock(3, [32, 32, 128], strides=(1, 1), stage=2, block='a', activation=self.activation))\n",
    "    self.blocks.append(IdentityBlock(3, [32, 32, 128], stage=2, block='b', activation=self.activation))\n",
    "\n",
    "    self.blocks.append(ConvBlock(3, [64, 64, 256], stage=3, block='a', activation=self.activation))\n",
    "    self.blocks.append(IdentityBlock(3, [64, 64, 256], stage=3, block='b', activation=self.activation))\n",
    "\n",
    "    self.blocks.append(ConvBlock(3, [64, 64, 256], stage=4, block='a', activation=self.activation))\n",
    "    self.blocks.append(IdentityBlock(3, [64, 64, 256], stage=4, block='b', activation=self.activation))\n",
    "\n",
    "    self.avg_pool = tf.keras.layers.GlobalAveragePooling2D(name='avg_pool')\n",
    "    self.fc1 = tf.keras.layers.Dense(num_features,\n",
    "                                     activation=self.activation,\n",
    "                                     kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.01),\n",
    "                                     kernel_regularizer=tf.keras.regularizers.l2(L2_WEIGHT_DECAY),\n",
    "                                     bias_regularizer=tf.keras.regularizers.l2(L2_WEIGHT_DECAY),\n",
    "                                     name='fc1')\n",
    "    self.logits = tf.keras.layers.Dense(num_classes,\n",
    "                                        activation=None,\n",
    "                                        kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.01),\n",
    "                                        kernel_regularizer=tf.keras.regularizers.l2(L2_WEIGHT_DECAY),\n",
    "                                        bias_regularizer=tf.keras.regularizers.l2(L2_WEIGHT_DECAY),\n",
    "                                        name='logits')\n",
    "\n",
    "  def call(self, img_input, training=False):\n",
    "    x = self.conv1(img_input)\n",
    "    x = self.bn1(x, training=training)\n",
    "    x = self.act1(x)\n",
    "    x = self.max_pool1(x)\n",
    "\n",
    "    for block in self.blocks:\n",
    "      x = block(x)\n",
    "\n",
    "    x = self.avg_pool(x)\n",
    "    fc1 = self.fc1(x)\n",
    "    logits = self.logits(fc1)\n",
    "    return logits, fc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cross_entropy_loss(labels, logits):\n",
    "  loss = tf.nn.softmax_cross_entropy_with_logits_v2(labels=labels, logits=logits)\n",
    "  return tf.reduce_mean(loss)\n",
    "\n",
    "def virtual_adversarial_images(images, logits, pert_norm_radius=3.5):  \n",
    "  with tf.GradientTape() as tape:\n",
    "    # Get normalised noise matrix\n",
    "    noise = tf.random.normal(shape=tf.shape(images))\n",
    "    noise = 1e-6 * tf.nn.l2_normalize(noise, axis=tf.range(1, len(noise.shape)))\n",
    "\n",
    "    # Add noise to image and get new logits\n",
    "    noise_logits, _ = generator(images + noise, \n",
    "                                tf.constant(False, dtype=tf.bool))\n",
    "\n",
    "    # Get loss from noisey logits\n",
    "    noise_loss = tf.nn.softmax_cross_entropy_with_logits_v2(labels=logits, logits=noise_logits)\n",
    "    noise_loss = tf.reduce_mean(noise_loss)\n",
    "\n",
    "  # Based on perturbed image loss, get direction of greatest error\n",
    "  adversarial_noise = tape.gradient(noise_loss, \n",
    "                                    [noise],\n",
    "                                    unconnected_gradients='zero')[0]\n",
    "\n",
    "  adversarial_noise = tf.nn.l2_normalize(adversarial_noise, \n",
    "                                         axis=tf.range(1, 4))\n",
    "\n",
    "  # return images with adversarial perturbation\n",
    "  return images + pert_norm_radius * adversarial_noise\n",
    "\n",
    "def mixup_preprocess(x, y, batch_size, alpha=1):\n",
    "    # random sample the lambda value from beta distribution.\n",
    "    weight     = np.random.beta(alpha, alpha, batch_size)\n",
    "    x_weight   = weight.reshape(batch_size, 1, 1, 1)\n",
    "    y_weight   = weight.reshape(batch_size, 1)\n",
    "    \n",
    "    # Perform the mixup.\n",
    "    indices = tf.random.shuffle(tf.range(batch_size))\n",
    "    mixup_images = (x * x_weight) + (tf.gather(x, indices) * (1 - x_weight))\n",
    "    mixup_labels = (y * y_weight) + (tf.gather(y, indices) * (1 - y_weight))    \n",
    "    \n",
    "    return mixup_images, tf.nn.softmax(mixup_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_total_loss         = tf.keras.metrics.Mean(name='train_total_loss')\n",
    "train_src_vat_loss       = tf.keras.metrics.Mean(name='train_src_vat_loss')\n",
    "train_src_mixup_loss     = tf.keras.metrics.Mean(name='train_src_mixup_loss')\n",
    "train_cross_entropy_loss = tf.keras.metrics.Mean(name='train_cross_entropy_loss')\n",
    "src_test_accuracy        = tf.keras.metrics.CategoricalAccuracy(name='src_test_accuracy')\n",
    "src_train_accuracy       = tf.keras.metrics.CategoricalAccuracy(name='src_train_accuracy')\n",
    "\n",
    "@tf.function\n",
    "def train_gen_step(src_images, src_labels):  \n",
    "  with tf.GradientTape() as gen_tape:\n",
    "    #Logits\n",
    "    src_logits, src_enc = generator(src_images, training=True)\n",
    "    \n",
    "    #VAT\n",
    "    src_adver_images    = virtual_adversarial_images(src_images, tf.nn.softmax(src_logits))\n",
    "    src_adver_logits, _ = generator(tf.stop_gradient(src_adver_images), training=True)\n",
    "    \n",
    "    #MixUp\n",
    "    src_mixup_images, src_mixup_labels = mixup_preprocess(src_images, src_logits, batch_size)\n",
    "    src_mixup_logits, _                = generator(tf.stop_gradient(src_mixup_images),\n",
    "                                                   training=True)\n",
    "    \n",
    "    cross_entropy_loss  = get_cross_entropy_loss(labels=src_labels, \n",
    "                                                 logits=src_logits)\n",
    "    src_vat_loss        = get_cross_entropy_loss(labels=tf.nn.softmax(tf.stop_gradient(src_logits)),\n",
    "                                                 logits=src_adver_logits)\n",
    "    src_mixup_loss      = get_cross_entropy_loss(labels=tf.stop_gradient(src_mixup_labels), \n",
    "                                                 logits=src_mixup_logits)\n",
    "\n",
    "    total_loss = cross_entropy_loss + \\\n",
    "                 1    * src_mixup_loss +\\\n",
    "                 1    * src_vat_loss\n",
    "    \n",
    "  gen_gradients = gen_tape.gradient(total_loss, generator.trainable_variables)\n",
    "  gen_optimizer.apply_gradients(zip(gen_gradients, generator.trainable_variables))\n",
    "\n",
    "  src_train_accuracy(src_labels, src_logits)\n",
    "  train_cross_entropy_loss(cross_entropy_loss)\n",
    "  train_src_mixup_loss(src_mixup_loss)\n",
    "  train_src_vat_loss(src_vat_loss)\n",
    "  train_total_loss(total_loss)\n",
    "\n",
    "@tf.function\n",
    "def test_source_step(source_images, source_labels):\n",
    "  source_logits, _ = generator(source_images, training=False)\n",
    "  src_test_accuracy(source_labels, source_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0829 17:18:47.398195 139946375395072 deprecation.py:323] From /home/kjakkala/.local/lib/python3.5/site-packages/tensorflow/python/keras/backend.py:4075: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, TotalL: 7.8736, CrossE: 2.6575, Src VAT: 2.6077, Src MixUp: 2.6085, Src Train Acc: 7.45, Src Test Acc: 10.07\n",
      "Epoch: 002, TotalL: 7.7825, CrossE: 2.5853, Src VAT: 2.5951, Src MixUp: 2.6021, Src Train Acc: 9.94, Src Test Acc: 11.51\n",
      "Epoch: 003, TotalL: 7.3809, CrossE: 2.3767, Src VAT: 2.4802, Src MixUp: 2.5239, Src Train Acc: 15.79, Src Test Acc: 17.27\n",
      "Epoch: 004, TotalL: 6.7750, CrossE: 2.1320, Src VAT: 2.2714, Src MixUp: 2.3716, Src Train Acc: 22.76, Src Test Acc: 25.90\n",
      "Epoch: 005, TotalL: 6.3062, CrossE: 1.9417, Src VAT: 2.1191, Src MixUp: 2.2454, Src Train Acc: 30.13, Src Test Acc: 25.90\n",
      "Epoch: 006, TotalL: 5.8537, CrossE: 1.7542, Src VAT: 1.9669, Src MixUp: 2.1326, Src Train Acc: 37.98, Src Test Acc: 33.09\n",
      "Epoch: 007, TotalL: 5.4373, CrossE: 1.5843, Src VAT: 1.8221, Src MixUp: 2.0309, Src Train Acc: 44.87, Src Test Acc: 35.97\n",
      "Epoch: 008, TotalL: 5.1099, CrossE: 1.4572, Src VAT: 1.7041, Src MixUp: 1.9486, Src Train Acc: 50.40, Src Test Acc: 44.60\n",
      "Epoch: 009, TotalL: 4.8215, CrossE: 1.3440, Src VAT: 1.6024, Src MixUp: 1.8751, Src Train Acc: 54.49, Src Test Acc: 56.83\n",
      "Epoch: 010, TotalL: 4.5778, CrossE: 1.2539, Src VAT: 1.5153, Src MixUp: 1.8086, Src Train Acc: 57.85, Src Test Acc: 55.40\n",
      "Epoch: 011, TotalL: 4.3827, CrossE: 1.1890, Src VAT: 1.4505, Src MixUp: 1.7432, Src Train Acc: 59.78, Src Test Acc: 56.83\n",
      "Epoch: 012, TotalL: 4.2187, CrossE: 1.1058, Src VAT: 1.3851, Src MixUp: 1.7278, Src Train Acc: 63.38, Src Test Acc: 58.99\n",
      "Epoch: 013, TotalL: 4.0229, CrossE: 1.0372, Src VAT: 1.3231, Src MixUp: 1.6626, Src Train Acc: 67.23, Src Test Acc: 62.59\n",
      "Epoch: 014, TotalL: 3.8337, CrossE: 0.9708, Src VAT: 1.2545, Src MixUp: 1.6084, Src Train Acc: 68.67, Src Test Acc: 66.91\n",
      "Epoch: 015, TotalL: 3.7034, CrossE: 0.9213, Src VAT: 1.2024, Src MixUp: 1.5797, Src Train Acc: 70.91, Src Test Acc: 72.66\n",
      "Epoch: 016, TotalL: 3.5105, CrossE: 0.8371, Src VAT: 1.1336, Src MixUp: 1.5398, Src Train Acc: 73.88, Src Test Acc: 68.35\n",
      "Epoch: 017, TotalL: 3.3838, CrossE: 0.7910, Src VAT: 1.0942, Src MixUp: 1.4987, Src Train Acc: 76.28, Src Test Acc: 71.94\n",
      "Epoch: 018, TotalL: 3.2283, CrossE: 0.7387, Src VAT: 1.0284, Src MixUp: 1.4612, Src Train Acc: 77.32, Src Test Acc: 73.38\n",
      "Epoch: 019, TotalL: 3.0673, CrossE: 0.6757, Src VAT: 0.9793, Src MixUp: 1.4122, Src Train Acc: 79.81, Src Test Acc: 79.14\n",
      "Epoch: 020, TotalL: 2.9659, CrossE: 0.6466, Src VAT: 0.9294, Src MixUp: 1.3899, Src Train Acc: 80.93, Src Test Acc: 84.17\n",
      "Epoch: 021, TotalL: 2.8693, CrossE: 0.6025, Src VAT: 0.8947, Src MixUp: 1.3721, Src Train Acc: 82.77, Src Test Acc: 86.33\n",
      "Epoch: 022, TotalL: 2.7976, CrossE: 0.5605, Src VAT: 0.8637, Src MixUp: 1.3734, Src Train Acc: 84.54, Src Test Acc: 81.29\n",
      "Epoch: 023, TotalL: 2.6637, CrossE: 0.5340, Src VAT: 0.8133, Src MixUp: 1.3164, Src Train Acc: 84.86, Src Test Acc: 84.89\n",
      "Epoch: 024, TotalL: 2.5873, CrossE: 0.5040, Src VAT: 0.7923, Src MixUp: 1.2910, Src Train Acc: 84.94, Src Test Acc: 82.73\n",
      "Epoch: 025, TotalL: 2.5364, CrossE: 0.4753, Src VAT: 0.7671, Src MixUp: 1.2940, Src Train Acc: 86.62, Src Test Acc: 88.49\n",
      "Epoch: 026, TotalL: 2.4783, CrossE: 0.4669, Src VAT: 0.7487, Src MixUp: 1.2627, Src Train Acc: 87.10, Src Test Acc: 84.89\n",
      "Epoch: 027, TotalL: 2.3508, CrossE: 0.4121, Src VAT: 0.7053, Src MixUp: 1.2333, Src Train Acc: 88.70, Src Test Acc: 89.21\n",
      "Epoch: 028, TotalL: 2.2626, CrossE: 0.4120, Src VAT: 0.6645, Src MixUp: 1.1860, Src Train Acc: 87.18, Src Test Acc: 84.17\n",
      "Epoch: 029, TotalL: 2.2347, CrossE: 0.3906, Src VAT: 0.6521, Src MixUp: 1.1921, Src Train Acc: 89.66, Src Test Acc: 84.89\n",
      "Epoch: 030, TotalL: 2.2145, CrossE: 0.3835, Src VAT: 0.6404, Src MixUp: 1.1907, Src Train Acc: 89.02, Src Test Acc: 88.49\n",
      "Epoch: 031, TotalL: 2.1063, CrossE: 0.3353, Src VAT: 0.6068, Src MixUp: 1.1642, Src Train Acc: 91.67, Src Test Acc: 88.49\n",
      "Epoch: 032, TotalL: 2.1170, CrossE: 0.3570, Src VAT: 0.6048, Src MixUp: 1.1552, Src Train Acc: 90.46, Src Test Acc: 89.93\n",
      "Epoch: 033, TotalL: 2.0285, CrossE: 0.3189, Src VAT: 0.5704, Src MixUp: 1.1392, Src Train Acc: 90.95, Src Test Acc: 84.17\n",
      "Epoch: 034, TotalL: 1.9637, CrossE: 0.2925, Src VAT: 0.5519, Src MixUp: 1.1192, Src Train Acc: 92.15, Src Test Acc: 89.93\n",
      "Epoch: 035, TotalL: 1.9182, CrossE: 0.2915, Src VAT: 0.5341, Src MixUp: 1.0927, Src Train Acc: 92.23, Src Test Acc: 90.65\n",
      "Epoch: 036, TotalL: 1.8690, CrossE: 0.2747, Src VAT: 0.5078, Src MixUp: 1.0865, Src Train Acc: 92.87, Src Test Acc: 89.21\n",
      "Epoch: 037, TotalL: 1.7861, CrossE: 0.2451, Src VAT: 0.4892, Src MixUp: 1.0519, Src Train Acc: 94.07, Src Test Acc: 89.93\n",
      "Epoch: 038, TotalL: 1.7972, CrossE: 0.2709, Src VAT: 0.4911, Src MixUp: 1.0352, Src Train Acc: 93.03, Src Test Acc: 92.81\n",
      "Epoch: 039, TotalL: 1.7397, CrossE: 0.2613, Src VAT: 0.4714, Src MixUp: 1.0070, Src Train Acc: 93.11, Src Test Acc: 87.77\n",
      "Epoch: 040, TotalL: 1.6758, CrossE: 0.2135, Src VAT: 0.4389, Src MixUp: 1.0234, Src Train Acc: 95.03, Src Test Acc: 92.09\n",
      "Epoch: 041, TotalL: 1.6531, CrossE: 0.2281, Src VAT: 0.4442, Src MixUp: 0.9807, Src Train Acc: 94.15, Src Test Acc: 92.81\n",
      "Epoch: 042, TotalL: 1.6016, CrossE: 0.2035, Src VAT: 0.4173, Src MixUp: 0.9808, Src Train Acc: 94.87, Src Test Acc: 87.77\n",
      "Epoch: 043, TotalL: 1.5956, CrossE: 0.2059, Src VAT: 0.4079, Src MixUp: 0.9819, Src Train Acc: 94.23, Src Test Acc: 92.09\n",
      "Epoch: 044, TotalL: 1.5941, CrossE: 0.2194, Src VAT: 0.4082, Src MixUp: 0.9665, Src Train Acc: 94.55, Src Test Acc: 93.53\n",
      "Epoch: 045, TotalL: 1.5959, CrossE: 0.2067, Src VAT: 0.4154, Src MixUp: 0.9738, Src Train Acc: 95.11, Src Test Acc: 93.53\n",
      "Epoch: 046, TotalL: 1.6148, CrossE: 0.2084, Src VAT: 0.4194, Src MixUp: 0.9870, Src Train Acc: 94.79, Src Test Acc: 92.81\n",
      "Epoch: 047, TotalL: 1.5205, CrossE: 0.1852, Src VAT: 0.3786, Src MixUp: 0.9568, Src Train Acc: 95.19, Src Test Acc: 91.37\n",
      "Epoch: 048, TotalL: 1.5653, CrossE: 0.1880, Src VAT: 0.3900, Src MixUp: 0.9873, Src Train Acc: 95.99, Src Test Acc: 89.93\n",
      "Epoch: 049, TotalL: 1.5181, CrossE: 0.1936, Src VAT: 0.3742, Src MixUp: 0.9503, Src Train Acc: 94.87, Src Test Acc: 90.65\n",
      "Epoch: 050, TotalL: 1.4927, CrossE: 0.1719, Src VAT: 0.3692, Src MixUp: 0.9515, Src Train Acc: 96.23, Src Test Acc: 89.21\n",
      "Epoch: 051, TotalL: 1.4586, CrossE: 0.1839, Src VAT: 0.3606, Src MixUp: 0.9142, Src Train Acc: 94.95, Src Test Acc: 95.68\n",
      "Epoch: 052, TotalL: 1.3926, CrossE: 0.1569, Src VAT: 0.3387, Src MixUp: 0.8970, Src Train Acc: 95.83, Src Test Acc: 94.24\n",
      "Epoch: 053, TotalL: 1.4163, CrossE: 0.1504, Src VAT: 0.3306, Src MixUp: 0.9353, Src Train Acc: 96.55, Src Test Acc: 92.81\n",
      "Epoch: 054, TotalL: 1.3133, CrossE: 0.1410, Src VAT: 0.3156, Src MixUp: 0.8567, Src Train Acc: 97.12, Src Test Acc: 92.81\n",
      "Epoch: 055, TotalL: 1.3720, CrossE: 0.1541, Src VAT: 0.3206, Src MixUp: 0.8973, Src Train Acc: 96.31, Src Test Acc: 92.09\n",
      "Epoch: 056, TotalL: 1.4352, CrossE: 0.1737, Src VAT: 0.3437, Src MixUp: 0.9179, Src Train Acc: 95.91, Src Test Acc: 92.09\n",
      "Epoch: 057, TotalL: 1.3281, CrossE: 0.1374, Src VAT: 0.3036, Src MixUp: 0.8872, Src Train Acc: 96.88, Src Test Acc: 92.09\n",
      "Epoch: 058, TotalL: 1.3483, CrossE: 0.1434, Src VAT: 0.3087, Src MixUp: 0.8962, Src Train Acc: 96.96, Src Test Acc: 94.24\n",
      "Epoch: 059, TotalL: 1.2491, CrossE: 0.1259, Src VAT: 0.2860, Src MixUp: 0.8372, Src Train Acc: 97.36, Src Test Acc: 90.65\n",
      "Epoch: 060, TotalL: 1.2470, CrossE: 0.1166, Src VAT: 0.2707, Src MixUp: 0.8597, Src Train Acc: 97.04, Src Test Acc: 93.53\n",
      "Epoch: 061, TotalL: 1.2342, CrossE: 0.1103, Src VAT: 0.2677, Src MixUp: 0.8562, Src Train Acc: 98.08, Src Test Acc: 92.81\n",
      "Epoch: 062, TotalL: 1.3143, CrossE: 0.1553, Src VAT: 0.3016, Src MixUp: 0.8574, Src Train Acc: 95.91, Src Test Acc: 91.37\n",
      "Epoch: 063, TotalL: 1.2195, CrossE: 0.1234, Src VAT: 0.2682, Src MixUp: 0.8279, Src Train Acc: 97.20, Src Test Acc: 94.24\n",
      "Epoch: 064, TotalL: 1.1898, CrossE: 0.1072, Src VAT: 0.2562, Src MixUp: 0.8264, Src Train Acc: 98.16, Src Test Acc: 88.49\n",
      "Epoch: 065, TotalL: 1.2197, CrossE: 0.1159, Src VAT: 0.2646, Src MixUp: 0.8392, Src Train Acc: 97.36, Src Test Acc: 94.24\n",
      "Epoch: 066, TotalL: 1.1107, CrossE: 0.0947, Src VAT: 0.2367, Src MixUp: 0.7793, Src Train Acc: 98.00, Src Test Acc: 94.24\n",
      "Epoch: 067, TotalL: 1.1655, CrossE: 0.1148, Src VAT: 0.2536, Src MixUp: 0.7971, Src Train Acc: 97.20, Src Test Acc: 94.96\n",
      "Epoch: 068, TotalL: 1.1643, CrossE: 0.1173, Src VAT: 0.2564, Src MixUp: 0.7906, Src Train Acc: 97.28, Src Test Acc: 93.53\n",
      "Epoch: 069, TotalL: 1.1345, CrossE: 0.0965, Src VAT: 0.2393, Src MixUp: 0.7987, Src Train Acc: 98.16, Src Test Acc: 93.53\n",
      "Epoch: 070, TotalL: 1.0539, CrossE: 0.0922, Src VAT: 0.2097, Src MixUp: 0.7520, Src Train Acc: 97.84, Src Test Acc: 90.65\n",
      "Epoch: 071, TotalL: 1.0917, CrossE: 0.0830, Src VAT: 0.2203, Src MixUp: 0.7884, Src Train Acc: 98.88, Src Test Acc: 88.49\n",
      "Epoch: 072, TotalL: 1.0801, CrossE: 0.0865, Src VAT: 0.2094, Src MixUp: 0.7842, Src Train Acc: 98.32, Src Test Acc: 94.96\n",
      "Epoch: 073, TotalL: 1.0735, CrossE: 0.0969, Src VAT: 0.2228, Src MixUp: 0.7538, Src Train Acc: 98.24, Src Test Acc: 91.37\n",
      "Epoch: 074, TotalL: 0.9897, CrossE: 0.0825, Src VAT: 0.2003, Src MixUp: 0.7069, Src Train Acc: 98.40, Src Test Acc: 94.96\n",
      "Epoch: 075, TotalL: 1.0746, CrossE: 0.1047, Src VAT: 0.2196, Src MixUp: 0.7502, Src Train Acc: 97.36, Src Test Acc: 92.81\n",
      "Epoch: 076, TotalL: 1.0255, CrossE: 0.0762, Src VAT: 0.2017, Src MixUp: 0.7476, Src Train Acc: 98.88, Src Test Acc: 95.68\n",
      "Epoch: 077, TotalL: 0.9875, CrossE: 0.0765, Src VAT: 0.1919, Src MixUp: 0.7192, Src Train Acc: 98.40, Src Test Acc: 93.53\n",
      "Epoch: 078, TotalL: 1.0552, CrossE: 0.0915, Src VAT: 0.2097, Src MixUp: 0.7541, Src Train Acc: 97.52, Src Test Acc: 91.37\n",
      "Epoch: 079, TotalL: 1.0601, CrossE: 0.0931, Src VAT: 0.2137, Src MixUp: 0.7533, Src Train Acc: 97.92, Src Test Acc: 94.24\n",
      "Epoch: 080, TotalL: 1.0263, CrossE: 0.0884, Src VAT: 0.1986, Src MixUp: 0.7393, Src Train Acc: 97.44, Src Test Acc: 96.40\n",
      "Epoch: 081, TotalL: 1.0689, CrossE: 0.1010, Src VAT: 0.2195, Src MixUp: 0.7484, Src Train Acc: 97.52, Src Test Acc: 93.53\n",
      "Epoch: 082, TotalL: 1.0021, CrossE: 0.0963, Src VAT: 0.1927, Src MixUp: 0.7131, Src Train Acc: 97.36, Src Test Acc: 91.37\n",
      "Epoch: 083, TotalL: 0.9572, CrossE: 0.0774, Src VAT: 0.1862, Src MixUp: 0.6936, Src Train Acc: 98.32, Src Test Acc: 94.96\n",
      "Epoch: 084, TotalL: 0.9777, CrossE: 0.0648, Src VAT: 0.1788, Src MixUp: 0.7340, Src Train Acc: 99.12, Src Test Acc: 95.68\n",
      "Epoch: 085, TotalL: 0.9613, CrossE: 0.0610, Src VAT: 0.1725, Src MixUp: 0.7277, Src Train Acc: 99.04, Src Test Acc: 91.37\n",
      "Epoch: 086, TotalL: 0.9686, CrossE: 0.0710, Src VAT: 0.1810, Src MixUp: 0.7165, Src Train Acc: 98.56, Src Test Acc: 92.81\n",
      "Epoch: 087, TotalL: 0.9352, CrossE: 0.0676, Src VAT: 0.1726, Src MixUp: 0.6949, Src Train Acc: 98.72, Src Test Acc: 82.73\n",
      "Epoch: 088, TotalL: 0.9326, CrossE: 0.0642, Src VAT: 0.1595, Src MixUp: 0.7089, Src Train Acc: 98.64, Src Test Acc: 93.53\n",
      "Epoch: 089, TotalL: 0.9455, CrossE: 0.0658, Src VAT: 0.1734, Src MixUp: 0.7063, Src Train Acc: 98.56, Src Test Acc: 94.24\n",
      "Epoch: 090, TotalL: 0.9545, CrossE: 0.0660, Src VAT: 0.1645, Src MixUp: 0.7240, Src Train Acc: 98.64, Src Test Acc: 88.49\n",
      "Epoch: 091, TotalL: 0.9801, CrossE: 0.0707, Src VAT: 0.1824, Src MixUp: 0.7271, Src Train Acc: 98.64, Src Test Acc: 94.24\n",
      "Epoch: 092, TotalL: 0.9263, CrossE: 0.0627, Src VAT: 0.1630, Src MixUp: 0.7006, Src Train Acc: 98.56, Src Test Acc: 94.96\n",
      "Epoch: 093, TotalL: 0.9953, CrossE: 0.0949, Src VAT: 0.1883, Src MixUp: 0.7121, Src Train Acc: 97.12, Src Test Acc: 94.96\n",
      "Epoch: 094, TotalL: 0.9071, CrossE: 0.0551, Src VAT: 0.1502, Src MixUp: 0.7018, Src Train Acc: 99.20, Src Test Acc: 91.37\n",
      "Epoch: 095, TotalL: 0.9772, CrossE: 0.0726, Src VAT: 0.1743, Src MixUp: 0.7303, Src Train Acc: 98.40, Src Test Acc: 97.84\n",
      "Epoch: 096, TotalL: 0.9107, CrossE: 0.0672, Src VAT: 0.1667, Src MixUp: 0.6768, Src Train Acc: 98.08, Src Test Acc: 95.68\n",
      "Epoch: 097, TotalL: 0.9026, CrossE: 0.0577, Src VAT: 0.1578, Src MixUp: 0.6871, Src Train Acc: 98.88, Src Test Acc: 96.40\n",
      "Epoch: 098, TotalL: 0.8701, CrossE: 0.0428, Src VAT: 0.1418, Src MixUp: 0.6855, Src Train Acc: 99.68, Src Test Acc: 92.81\n",
      "Epoch: 099, TotalL: 0.9218, CrossE: 0.0644, Src VAT: 0.1609, Src MixUp: 0.6965, Src Train Acc: 98.24, Src Test Acc: 89.93\n",
      "Epoch: 100, TotalL: 0.9286, CrossE: 0.0774, Src VAT: 0.1750, Src MixUp: 0.6763, Src Train Acc: 98.32, Src Test Acc: 91.37\n",
      "Epoch: 101, TotalL: 0.8700, CrossE: 0.0641, Src VAT: 0.1457, Src MixUp: 0.6601, Src Train Acc: 98.32, Src Test Acc: 96.40\n",
      "Epoch: 102, TotalL: 0.8943, CrossE: 0.0565, Src VAT: 0.1439, Src MixUp: 0.6938, Src Train Acc: 99.04, Src Test Acc: 95.68\n",
      "Epoch: 103, TotalL: 0.9142, CrossE: 0.0580, Src VAT: 0.1502, Src MixUp: 0.7061, Src Train Acc: 98.64, Src Test Acc: 97.12\n",
      "Epoch: 104, TotalL: 0.8661, CrossE: 0.0635, Src VAT: 0.1361, Src MixUp: 0.6665, Src Train Acc: 98.80, Src Test Acc: 94.24\n",
      "Epoch: 105, TotalL: 0.8413, CrossE: 0.0502, Src VAT: 0.1464, Src MixUp: 0.6447, Src Train Acc: 99.76, Src Test Acc: 93.53\n",
      "Epoch: 106, TotalL: 0.7798, CrossE: 0.0461, Src VAT: 0.1213, Src MixUp: 0.6124, Src Train Acc: 98.96, Src Test Acc: 95.68\n",
      "Epoch: 107, TotalL: 0.7655, CrossE: 0.0406, Src VAT: 0.1176, Src MixUp: 0.6073, Src Train Acc: 99.44, Src Test Acc: 94.96\n",
      "Epoch: 108, TotalL: 0.8700, CrossE: 0.0695, Src VAT: 0.1457, Src MixUp: 0.6548, Src Train Acc: 98.00, Src Test Acc: 94.24\n",
      "Epoch: 109, TotalL: 0.8575, CrossE: 0.0537, Src VAT: 0.1379, Src MixUp: 0.6658, Src Train Acc: 99.20, Src Test Acc: 97.12\n",
      "Epoch: 110, TotalL: 0.8298, CrossE: 0.0458, Src VAT: 0.1335, Src MixUp: 0.6504, Src Train Acc: 99.20, Src Test Acc: 96.40\n",
      "Epoch: 111, TotalL: 0.8165, CrossE: 0.0563, Src VAT: 0.1267, Src MixUp: 0.6335, Src Train Acc: 98.88, Src Test Acc: 95.68\n",
      "Epoch: 112, TotalL: 0.8041, CrossE: 0.0460, Src VAT: 0.1286, Src MixUp: 0.6295, Src Train Acc: 99.28, Src Test Acc: 97.12\n",
      "Epoch: 113, TotalL: 0.8412, CrossE: 0.0591, Src VAT: 0.1321, Src MixUp: 0.6500, Src Train Acc: 98.64, Src Test Acc: 96.40\n",
      "Epoch: 114, TotalL: 0.7614, CrossE: 0.0451, Src VAT: 0.1197, Src MixUp: 0.5966, Src Train Acc: 99.12, Src Test Acc: 92.09\n",
      "Epoch: 115, TotalL: 0.8335, CrossE: 0.0507, Src VAT: 0.1398, Src MixUp: 0.6431, Src Train Acc: 99.12, Src Test Acc: 96.40\n",
      "Epoch: 116, TotalL: 0.8214, CrossE: 0.0606, Src VAT: 0.1303, Src MixUp: 0.6306, Src Train Acc: 98.56, Src Test Acc: 95.68\n",
      "Epoch: 117, TotalL: 0.8527, CrossE: 0.0709, Src VAT: 0.1416, Src MixUp: 0.6401, Src Train Acc: 98.16, Src Test Acc: 94.24\n",
      "Epoch: 118, TotalL: 0.7272, CrossE: 0.0361, Src VAT: 0.1046, Src MixUp: 0.5864, Src Train Acc: 99.28, Src Test Acc: 94.24\n",
      "Epoch: 119, TotalL: 0.8151, CrossE: 0.0644, Src VAT: 0.1306, Src MixUp: 0.6202, Src Train Acc: 98.40, Src Test Acc: 92.09\n",
      "Epoch: 120, TotalL: 0.7485, CrossE: 0.0403, Src VAT: 0.1114, Src MixUp: 0.5968, Src Train Acc: 99.20, Src Test Acc: 95.68\n",
      "Epoch: 121, TotalL: 0.8333, CrossE: 0.0578, Src VAT: 0.1316, Src MixUp: 0.6439, Src Train Acc: 99.12, Src Test Acc: 89.93\n",
      "Epoch: 122, TotalL: 0.8120, CrossE: 0.0499, Src VAT: 0.1279, Src MixUp: 0.6343, Src Train Acc: 99.04, Src Test Acc: 93.53\n",
      "Epoch: 123, TotalL: 0.7779, CrossE: 0.0453, Src VAT: 0.1165, Src MixUp: 0.6160, Src Train Acc: 99.12, Src Test Acc: 95.68\n",
      "Epoch: 124, TotalL: 0.6936, CrossE: 0.0306, Src VAT: 0.0950, Src MixUp: 0.5681, Src Train Acc: 99.60, Src Test Acc: 90.65\n",
      "Epoch: 125, TotalL: 0.7259, CrossE: 0.0404, Src VAT: 0.1022, Src MixUp: 0.5833, Src Train Acc: 99.44, Src Test Acc: 95.68\n",
      "Epoch: 126, TotalL: 0.7133, CrossE: 0.0348, Src VAT: 0.0957, Src MixUp: 0.5828, Src Train Acc: 99.68, Src Test Acc: 95.68\n",
      "Epoch: 127, TotalL: 0.7875, CrossE: 0.0551, Src VAT: 0.1283, Src MixUp: 0.6041, Src Train Acc: 99.12, Src Test Acc: 94.96\n",
      "Epoch: 128, TotalL: 0.7970, CrossE: 0.0507, Src VAT: 0.1165, Src MixUp: 0.6299, Src Train Acc: 98.96, Src Test Acc: 94.96\n",
      "Epoch: 129, TotalL: 0.7420, CrossE: 0.0349, Src VAT: 0.0982, Src MixUp: 0.6089, Src Train Acc: 99.20, Src Test Acc: 96.40\n",
      "Epoch: 130, TotalL: 0.7769, CrossE: 0.0396, Src VAT: 0.1131, Src MixUp: 0.6242, Src Train Acc: 99.44, Src Test Acc: 94.96\n",
      "Epoch: 131, TotalL: 0.7198, CrossE: 0.0374, Src VAT: 0.1073, Src MixUp: 0.5751, Src Train Acc: 99.44, Src Test Acc: 97.12\n",
      "Epoch: 132, TotalL: 0.7257, CrossE: 0.0375, Src VAT: 0.1031, Src MixUp: 0.5852, Src Train Acc: 99.28, Src Test Acc: 94.96\n",
      "Epoch: 133, TotalL: 0.7199, CrossE: 0.0352, Src VAT: 0.0994, Src MixUp: 0.5853, Src Train Acc: 99.44, Src Test Acc: 94.24\n",
      "Epoch: 134, TotalL: 0.6619, CrossE: 0.0264, Src VAT: 0.0893, Src MixUp: 0.5462, Src Train Acc: 99.84, Src Test Acc: 92.81\n",
      "Epoch: 135, TotalL: 0.7559, CrossE: 0.0446, Src VAT: 0.1153, Src MixUp: 0.5960, Src Train Acc: 99.52, Src Test Acc: 93.53\n",
      "Epoch: 136, TotalL: 0.7363, CrossE: 0.0408, Src VAT: 0.1052, Src MixUp: 0.5902, Src Train Acc: 98.88, Src Test Acc: 94.24\n",
      "Epoch: 137, TotalL: 0.6670, CrossE: 0.0278, Src VAT: 0.0842, Src MixUp: 0.5550, Src Train Acc: 99.52, Src Test Acc: 94.96\n",
      "Epoch: 138, TotalL: 0.7667, CrossE: 0.0451, Src VAT: 0.1095, Src MixUp: 0.6121, Src Train Acc: 98.64, Src Test Acc: 96.40\n",
      "Epoch: 139, TotalL: 0.6826, CrossE: 0.0285, Src VAT: 0.0886, Src MixUp: 0.5655, Src Train Acc: 99.52, Src Test Acc: 94.96\n",
      "Epoch: 140, TotalL: 0.7917, CrossE: 0.0717, Src VAT: 0.1150, Src MixUp: 0.6049, Src Train Acc: 98.64, Src Test Acc: 95.68\n",
      "Epoch: 141, TotalL: 0.7126, CrossE: 0.0365, Src VAT: 0.1070, Src MixUp: 0.5691, Src Train Acc: 99.44, Src Test Acc: 94.24\n",
      "Epoch: 142, TotalL: 0.7288, CrossE: 0.0355, Src VAT: 0.1046, Src MixUp: 0.5888, Src Train Acc: 99.60, Src Test Acc: 94.24\n",
      "Epoch: 143, TotalL: 0.6680, CrossE: 0.0340, Src VAT: 0.0887, Src MixUp: 0.5453, Src Train Acc: 99.12, Src Test Acc: 90.65\n",
      "Epoch: 144, TotalL: 0.7358, CrossE: 0.0410, Src VAT: 0.1018, Src MixUp: 0.5930, Src Train Acc: 99.04, Src Test Acc: 95.68\n",
      "Epoch: 145, TotalL: 0.7611, CrossE: 0.0463, Src VAT: 0.1073, Src MixUp: 0.6075, Src Train Acc: 99.12, Src Test Acc: 96.40\n",
      "Epoch: 146, TotalL: 0.7673, CrossE: 0.0495, Src VAT: 0.1151, Src MixUp: 0.6027, Src Train Acc: 98.72, Src Test Acc: 93.53\n",
      "Epoch: 147, TotalL: 0.6459, CrossE: 0.0238, Src VAT: 0.0797, Src MixUp: 0.5424, Src Train Acc: 99.52, Src Test Acc: 93.53\n",
      "Epoch: 148, TotalL: 0.6410, CrossE: 0.0233, Src VAT: 0.0762, Src MixUp: 0.5414, Src Train Acc: 99.68, Src Test Acc: 94.96\n",
      "Epoch: 149, TotalL: 0.6811, CrossE: 0.0321, Src VAT: 0.0867, Src MixUp: 0.5624, Src Train Acc: 99.28, Src Test Acc: 92.09\n",
      "Epoch: 150, TotalL: 0.6177, CrossE: 0.0301, Src VAT: 0.0798, Src MixUp: 0.5078, Src Train Acc: 99.28, Src Test Acc: 94.96\n",
      "Epoch: 151, TotalL: 0.6891, CrossE: 0.0366, Src VAT: 0.0970, Src MixUp: 0.5554, Src Train Acc: 99.28, Src Test Acc: 93.53\n",
      "Epoch: 152, TotalL: 0.6832, CrossE: 0.0294, Src VAT: 0.0891, Src MixUp: 0.5647, Src Train Acc: 99.44, Src Test Acc: 95.68\n",
      "Epoch: 153, TotalL: 0.6992, CrossE: 0.0464, Src VAT: 0.0931, Src MixUp: 0.5597, Src Train Acc: 98.96, Src Test Acc: 97.12\n",
      "Epoch: 154, TotalL: 0.6595, CrossE: 0.0269, Src VAT: 0.0863, Src MixUp: 0.5464, Src Train Acc: 99.68, Src Test Acc: 96.40\n",
      "Epoch: 155, TotalL: 0.6649, CrossE: 0.0325, Src VAT: 0.0889, Src MixUp: 0.5435, Src Train Acc: 99.44, Src Test Acc: 94.96\n",
      "Epoch: 156, TotalL: 0.6443, CrossE: 0.0248, Src VAT: 0.0797, Src MixUp: 0.5398, Src Train Acc: 99.76, Src Test Acc: 95.68\n",
      "Epoch: 157, TotalL: 0.6529, CrossE: 0.0184, Src VAT: 0.0697, Src MixUp: 0.5648, Src Train Acc: 99.68, Src Test Acc: 95.68\n",
      "Epoch: 158, TotalL: 0.6484, CrossE: 0.0201, Src VAT: 0.0729, Src MixUp: 0.5553, Src Train Acc: 99.84, Src Test Acc: 94.96\n",
      "Epoch: 159, TotalL: 0.6206, CrossE: 0.0231, Src VAT: 0.0785, Src MixUp: 0.5191, Src Train Acc: 99.68, Src Test Acc: 97.84\n",
      "Epoch: 160, TotalL: 0.6190, CrossE: 0.0191, Src VAT: 0.0666, Src MixUp: 0.5333, Src Train Acc: 99.68, Src Test Acc: 94.96\n",
      "Epoch: 161, TotalL: 0.6708, CrossE: 0.0398, Src VAT: 0.0881, Src MixUp: 0.5429, Src Train Acc: 99.12, Src Test Acc: 93.53\n",
      "Epoch: 162, TotalL: 0.6958, CrossE: 0.0357, Src VAT: 0.0929, Src MixUp: 0.5672, Src Train Acc: 99.36, Src Test Acc: 92.81\n",
      "Epoch: 163, TotalL: 0.6795, CrossE: 0.0368, Src VAT: 0.0879, Src MixUp: 0.5548, Src Train Acc: 99.12, Src Test Acc: 94.24\n",
      "Epoch: 164, TotalL: 0.6742, CrossE: 0.0288, Src VAT: 0.0878, Src MixUp: 0.5577, Src Train Acc: 99.60, Src Test Acc: 93.53\n",
      "Epoch: 165, TotalL: 0.6230, CrossE: 0.0242, Src VAT: 0.0753, Src MixUp: 0.5235, Src Train Acc: 99.68, Src Test Acc: 97.12\n",
      "Epoch: 166, TotalL: 0.6733, CrossE: 0.0373, Src VAT: 0.0879, Src MixUp: 0.5481, Src Train Acc: 98.64, Src Test Acc: 94.24\n",
      "Epoch: 167, TotalL: 0.6625, CrossE: 0.0310, Src VAT: 0.0767, Src MixUp: 0.5548, Src Train Acc: 99.04, Src Test Acc: 94.96\n",
      "Epoch: 168, TotalL: 0.6506, CrossE: 0.0246, Src VAT: 0.0778, Src MixUp: 0.5482, Src Train Acc: 99.60, Src Test Acc: 95.68\n",
      "Epoch: 169, TotalL: 0.5867, CrossE: 0.0170, Src VAT: 0.0605, Src MixUp: 0.5092, Src Train Acc: 99.52, Src Test Acc: 94.24\n",
      "Epoch: 170, TotalL: 0.6209, CrossE: 0.0261, Src VAT: 0.0784, Src MixUp: 0.5164, Src Train Acc: 99.60, Src Test Acc: 94.24\n",
      "Epoch: 171, TotalL: 0.6233, CrossE: 0.0322, Src VAT: 0.0684, Src MixUp: 0.5226, Src Train Acc: 99.20, Src Test Acc: 94.96\n",
      "Epoch: 172, TotalL: 0.5934, CrossE: 0.0259, Src VAT: 0.0718, Src MixUp: 0.4957, Src Train Acc: 99.60, Src Test Acc: 94.96\n",
      "Epoch: 173, TotalL: 0.6082, CrossE: 0.0221, Src VAT: 0.0725, Src MixUp: 0.5137, Src Train Acc: 99.76, Src Test Acc: 94.24\n",
      "Epoch: 174, TotalL: 0.6096, CrossE: 0.0201, Src VAT: 0.0664, Src MixUp: 0.5231, Src Train Acc: 99.76, Src Test Acc: 94.96\n",
      "Epoch: 175, TotalL: 0.6399, CrossE: 0.0288, Src VAT: 0.0824, Src MixUp: 0.5286, Src Train Acc: 99.60, Src Test Acc: 91.37\n",
      "Epoch: 176, TotalL: 0.5919, CrossE: 0.0207, Src VAT: 0.0676, Src MixUp: 0.5036, Src Train Acc: 99.68, Src Test Acc: 92.81\n",
      "Epoch: 177, TotalL: 0.6358, CrossE: 0.0260, Src VAT: 0.0759, Src MixUp: 0.5339, Src Train Acc: 99.52, Src Test Acc: 95.68\n",
      "Epoch: 178, TotalL: 0.6513, CrossE: 0.0356, Src VAT: 0.0868, Src MixUp: 0.5290, Src Train Acc: 99.20, Src Test Acc: 92.09\n",
      "Epoch: 179, TotalL: 0.6109, CrossE: 0.0293, Src VAT: 0.0766, Src MixUp: 0.5050, Src Train Acc: 99.36, Src Test Acc: 91.37\n",
      "Epoch: 180, TotalL: 0.6878, CrossE: 0.0439, Src VAT: 0.0903, Src MixUp: 0.5537, Src Train Acc: 98.96, Src Test Acc: 95.68\n",
      "Epoch: 181, TotalL: 0.6080, CrossE: 0.0232, Src VAT: 0.0722, Src MixUp: 0.5126, Src Train Acc: 99.60, Src Test Acc: 93.53\n",
      "Epoch: 182, TotalL: 0.6096, CrossE: 0.0211, Src VAT: 0.0706, Src MixUp: 0.5178, Src Train Acc: 99.84, Src Test Acc: 97.12\n",
      "Epoch: 183, TotalL: 0.5433, CrossE: 0.0146, Src VAT: 0.0573, Src MixUp: 0.4714, Src Train Acc: 100.00, Src Test Acc: 96.40\n",
      "Epoch: 184, TotalL: 0.5232, CrossE: 0.0172, Src VAT: 0.0560, Src MixUp: 0.4499, Src Train Acc: 99.68, Src Test Acc: 95.68\n",
      "Epoch: 185, TotalL: 0.5646, CrossE: 0.0188, Src VAT: 0.0630, Src MixUp: 0.4828, Src Train Acc: 99.76, Src Test Acc: 95.68\n",
      "Epoch: 186, TotalL: 0.6285, CrossE: 0.0382, Src VAT: 0.0854, Src MixUp: 0.5048, Src Train Acc: 99.36, Src Test Acc: 94.24\n",
      "Epoch: 187, TotalL: 0.6496, CrossE: 0.0408, Src VAT: 0.0835, Src MixUp: 0.5254, Src Train Acc: 98.88, Src Test Acc: 96.40\n",
      "Epoch: 188, TotalL: 0.6858, CrossE: 0.0474, Src VAT: 0.0924, Src MixUp: 0.5460, Src Train Acc: 98.64, Src Test Acc: 92.09\n",
      "Epoch: 189, TotalL: 0.5942, CrossE: 0.0194, Src VAT: 0.0662, Src MixUp: 0.5087, Src Train Acc: 99.84, Src Test Acc: 92.81\n",
      "Epoch: 190, TotalL: 0.5788, CrossE: 0.0192, Src VAT: 0.0648, Src MixUp: 0.4948, Src Train Acc: 99.68, Src Test Acc: 94.24\n",
      "Epoch: 191, TotalL: 0.6480, CrossE: 0.0236, Src VAT: 0.0743, Src MixUp: 0.5501, Src Train Acc: 99.68, Src Test Acc: 94.96\n",
      "Epoch: 192, TotalL: 0.6082, CrossE: 0.0238, Src VAT: 0.0665, Src MixUp: 0.5180, Src Train Acc: 99.60, Src Test Acc: 94.24\n",
      "Epoch: 193, TotalL: 0.5560, CrossE: 0.0180, Src VAT: 0.0568, Src MixUp: 0.4812, Src Train Acc: 99.68, Src Test Acc: 94.24\n",
      "Epoch: 194, TotalL: 0.6042, CrossE: 0.0251, Src VAT: 0.0720, Src MixUp: 0.5071, Src Train Acc: 99.60, Src Test Acc: 95.68\n",
      "Epoch: 195, TotalL: 0.6349, CrossE: 0.0344, Src VAT: 0.0780, Src MixUp: 0.5225, Src Train Acc: 99.36, Src Test Acc: 94.96\n",
      "Epoch: 196, TotalL: 0.6314, CrossE: 0.0269, Src VAT: 0.0783, Src MixUp: 0.5263, Src Train Acc: 99.52, Src Test Acc: 94.96\n",
      "Epoch: 197, TotalL: 0.5634, CrossE: 0.0194, Src VAT: 0.0596, Src MixUp: 0.4844, Src Train Acc: 99.76, Src Test Acc: 96.40\n",
      "Epoch: 198, TotalL: 0.5803, CrossE: 0.0203, Src VAT: 0.0670, Src MixUp: 0.4930, Src Train Acc: 99.84, Src Test Acc: 95.68\n",
      "Epoch: 199, TotalL: 0.5920, CrossE: 0.0229, Src VAT: 0.0664, Src MixUp: 0.5027, Src Train Acc: 99.52, Src Test Acc: 94.24\n",
      "Epoch: 200, TotalL: 0.5278, CrossE: 0.0153, Src VAT: 0.0527, Src MixUp: 0.4597, Src Train Acc: 99.76, Src Test Acc: 93.53\n",
      "Epoch: 201, TotalL: 0.5522, CrossE: 0.0164, Src VAT: 0.0594, Src MixUp: 0.4765, Src Train Acc: 100.00, Src Test Acc: 96.40\n",
      "Epoch: 202, TotalL: 0.7329, CrossE: 0.0907, Src VAT: 0.1005, Src MixUp: 0.5416, Src Train Acc: 97.92, Src Test Acc: 94.24\n",
      "Epoch: 203, TotalL: 0.6107, CrossE: 0.0212, Src VAT: 0.0654, Src MixUp: 0.5241, Src Train Acc: 99.76, Src Test Acc: 97.84\n",
      "Epoch: 204, TotalL: 0.6088, CrossE: 0.0236, Src VAT: 0.0728, Src MixUp: 0.5125, Src Train Acc: 99.68, Src Test Acc: 94.24\n",
      "Epoch: 205, TotalL: 0.5624, CrossE: 0.0146, Src VAT: 0.0542, Src MixUp: 0.4935, Src Train Acc: 99.84, Src Test Acc: 94.96\n",
      "Epoch: 206, TotalL: 0.5584, CrossE: 0.0174, Src VAT: 0.0544, Src MixUp: 0.4866, Src Train Acc: 99.76, Src Test Acc: 96.40\n",
      "Epoch: 207, TotalL: 0.5637, CrossE: 0.0185, Src VAT: 0.0573, Src MixUp: 0.4879, Src Train Acc: 99.76, Src Test Acc: 95.68\n",
      "Epoch: 208, TotalL: 0.5558, CrossE: 0.0142, Src VAT: 0.0530, Src MixUp: 0.4885, Src Train Acc: 99.84, Src Test Acc: 97.84\n",
      "Epoch: 209, TotalL: 0.5600, CrossE: 0.0148, Src VAT: 0.0577, Src MixUp: 0.4875, Src Train Acc: 100.00, Src Test Acc: 92.81\n",
      "Epoch: 210, TotalL: 0.4953, CrossE: 0.0115, Src VAT: 0.0431, Src MixUp: 0.4407, Src Train Acc: 99.84, Src Test Acc: 97.84\n",
      "Epoch: 211, TotalL: 0.5611, CrossE: 0.0162, Src VAT: 0.0580, Src MixUp: 0.4869, Src Train Acc: 99.92, Src Test Acc: 92.81\n",
      "Epoch: 212, TotalL: 0.5609, CrossE: 0.0197, Src VAT: 0.0587, Src MixUp: 0.4825, Src Train Acc: 99.60, Src Test Acc: 94.24\n",
      "Epoch: 213, TotalL: 0.6064, CrossE: 0.0276, Src VAT: 0.0743, Src MixUp: 0.5045, Src Train Acc: 99.60, Src Test Acc: 96.40\n",
      "Epoch: 214, TotalL: 0.5479, CrossE: 0.0176, Src VAT: 0.0583, Src MixUp: 0.4720, Src Train Acc: 99.76, Src Test Acc: 92.81\n",
      "Epoch: 215, TotalL: 0.5573, CrossE: 0.0179, Src VAT: 0.0586, Src MixUp: 0.4809, Src Train Acc: 99.68, Src Test Acc: 94.96\n",
      "Epoch: 216, TotalL: 0.5851, CrossE: 0.0331, Src VAT: 0.0741, Src MixUp: 0.4778, Src Train Acc: 98.96, Src Test Acc: 94.24\n",
      "Epoch: 217, TotalL: 0.5666, CrossE: 0.0193, Src VAT: 0.0581, Src MixUp: 0.4892, Src Train Acc: 99.68, Src Test Acc: 95.68\n",
      "Epoch: 218, TotalL: 0.5301, CrossE: 0.0155, Src VAT: 0.0556, Src MixUp: 0.4590, Src Train Acc: 100.00, Src Test Acc: 92.09\n",
      "Epoch: 219, TotalL: 0.5413, CrossE: 0.0179, Src VAT: 0.0603, Src MixUp: 0.4632, Src Train Acc: 99.76, Src Test Acc: 94.96\n",
      "Epoch: 220, TotalL: 0.5542, CrossE: 0.0233, Src VAT: 0.0567, Src MixUp: 0.4741, Src Train Acc: 99.60, Src Test Acc: 95.68\n",
      "Epoch: 221, TotalL: 0.6081, CrossE: 0.0336, Src VAT: 0.0718, Src MixUp: 0.5027, Src Train Acc: 98.96, Src Test Acc: 93.53\n",
      "Epoch: 222, TotalL: 0.5512, CrossE: 0.0128, Src VAT: 0.0522, Src MixUp: 0.4862, Src Train Acc: 100.00, Src Test Acc: 94.96\n",
      "Epoch: 223, TotalL: 0.5682, CrossE: 0.0247, Src VAT: 0.0613, Src MixUp: 0.4822, Src Train Acc: 99.52, Src Test Acc: 94.96\n",
      "Epoch: 224, TotalL: 0.5316, CrossE: 0.0151, Src VAT: 0.0532, Src MixUp: 0.4633, Src Train Acc: 99.84, Src Test Acc: 91.37\n",
      "Epoch: 225, TotalL: 0.5845, CrossE: 0.0267, Src VAT: 0.0670, Src MixUp: 0.4908, Src Train Acc: 99.36, Src Test Acc: 94.96\n",
      "Epoch: 226, TotalL: 0.5818, CrossE: 0.0248, Src VAT: 0.0631, Src MixUp: 0.4940, Src Train Acc: 99.52, Src Test Acc: 93.53\n",
      "Epoch: 227, TotalL: 0.5584, CrossE: 0.0204, Src VAT: 0.0634, Src MixUp: 0.4745, Src Train Acc: 99.60, Src Test Acc: 95.68\n",
      "Epoch: 228, TotalL: 0.5164, CrossE: 0.0153, Src VAT: 0.0492, Src MixUp: 0.4518, Src Train Acc: 99.76, Src Test Acc: 92.81\n",
      "Epoch: 229, TotalL: 0.5378, CrossE: 0.0170, Src VAT: 0.0584, Src MixUp: 0.4624, Src Train Acc: 99.84, Src Test Acc: 94.24\n",
      "Epoch: 230, TotalL: 0.5559, CrossE: 0.0201, Src VAT: 0.0572, Src MixUp: 0.4786, Src Train Acc: 99.68, Src Test Acc: 95.68\n",
      "Epoch: 231, TotalL: 0.5602, CrossE: 0.0220, Src VAT: 0.0534, Src MixUp: 0.4849, Src Train Acc: 99.36, Src Test Acc: 96.40\n",
      "Epoch: 232, TotalL: 0.5703, CrossE: 0.0292, Src VAT: 0.0648, Src MixUp: 0.4764, Src Train Acc: 99.36, Src Test Acc: 97.12\n",
      "Epoch: 233, TotalL: 0.5114, CrossE: 0.0145, Src VAT: 0.0451, Src MixUp: 0.4519, Src Train Acc: 99.76, Src Test Acc: 92.81\n",
      "Epoch: 234, TotalL: 0.4898, CrossE: 0.0109, Src VAT: 0.0406, Src MixUp: 0.4383, Src Train Acc: 99.84, Src Test Acc: 94.24\n",
      "Epoch: 235, TotalL: 0.6774, CrossE: 0.0372, Src VAT: 0.0868, Src MixUp: 0.5534, Src Train Acc: 99.20, Src Test Acc: 94.24\n",
      "Epoch: 236, TotalL: 0.5707, CrossE: 0.0196, Src VAT: 0.0629, Src MixUp: 0.4882, Src Train Acc: 99.76, Src Test Acc: 95.68\n",
      "Epoch: 237, TotalL: 0.5472, CrossE: 0.0226, Src VAT: 0.0538, Src MixUp: 0.4708, Src Train Acc: 99.44, Src Test Acc: 95.68\n",
      "Epoch: 238, TotalL: 0.5190, CrossE: 0.0174, Src VAT: 0.0519, Src MixUp: 0.4497, Src Train Acc: 99.52, Src Test Acc: 94.96\n",
      "Epoch: 239, TotalL: 0.5274, CrossE: 0.0211, Src VAT: 0.0564, Src MixUp: 0.4499, Src Train Acc: 99.60, Src Test Acc: 95.68\n",
      "Epoch: 240, TotalL: 0.5293, CrossE: 0.0154, Src VAT: 0.0464, Src MixUp: 0.4674, Src Train Acc: 99.76, Src Test Acc: 96.40\n",
      "Epoch: 241, TotalL: 0.4733, CrossE: 0.0107, Src VAT: 0.0373, Src MixUp: 0.4252, Src Train Acc: 99.60, Src Test Acc: 94.24\n",
      "Epoch: 242, TotalL: 0.5044, CrossE: 0.0136, Src VAT: 0.0466, Src MixUp: 0.4443, Src Train Acc: 99.84, Src Test Acc: 95.68\n",
      "Epoch: 243, TotalL: 0.4880, CrossE: 0.0131, Src VAT: 0.0460, Src MixUp: 0.4290, Src Train Acc: 99.84, Src Test Acc: 95.68\n",
      "Epoch: 244, TotalL: 0.4991, CrossE: 0.0161, Src VAT: 0.0530, Src MixUp: 0.4299, Src Train Acc: 99.68, Src Test Acc: 97.12\n",
      "Epoch: 245, TotalL: 0.4986, CrossE: 0.0118, Src VAT: 0.0427, Src MixUp: 0.4442, Src Train Acc: 99.92, Src Test Acc: 95.68\n",
      "Epoch: 246, TotalL: 0.4887, CrossE: 0.0119, Src VAT: 0.0424, Src MixUp: 0.4344, Src Train Acc: 99.76, Src Test Acc: 97.12\n",
      "Epoch: 247, TotalL: 0.4924, CrossE: 0.0112, Src VAT: 0.0445, Src MixUp: 0.4366, Src Train Acc: 99.84, Src Test Acc: 97.12\n",
      "Epoch: 248, TotalL: 0.5282, CrossE: 0.0173, Src VAT: 0.0586, Src MixUp: 0.4523, Src Train Acc: 99.68, Src Test Acc: 88.49\n",
      "Epoch: 249, TotalL: 0.5320, CrossE: 0.0158, Src VAT: 0.0519, Src MixUp: 0.4643, Src Train Acc: 99.84, Src Test Acc: 94.96\n",
      "Epoch: 250, TotalL: 0.4893, CrossE: 0.0132, Src VAT: 0.0445, Src MixUp: 0.4317, Src Train Acc: 99.84, Src Test Acc: 94.96\n",
      "Epoch: 251, TotalL: 0.4724, CrossE: 0.0153, Src VAT: 0.0482, Src MixUp: 0.4090, Src Train Acc: 99.60, Src Test Acc: 95.68\n",
      "Epoch: 252, TotalL: 0.5129, CrossE: 0.0146, Src VAT: 0.0494, Src MixUp: 0.4489, Src Train Acc: 99.84, Src Test Acc: 94.24\n",
      "Epoch: 253, TotalL: 0.5313, CrossE: 0.0168, Src VAT: 0.0560, Src MixUp: 0.4585, Src Train Acc: 99.60, Src Test Acc: 97.12\n",
      "Epoch: 254, TotalL: 0.5343, CrossE: 0.0141, Src VAT: 0.0518, Src MixUp: 0.4684, Src Train Acc: 99.84, Src Test Acc: 97.12\n",
      "Epoch: 255, TotalL: 0.4712, CrossE: 0.0088, Src VAT: 0.0397, Src MixUp: 0.4227, Src Train Acc: 100.00, Src Test Acc: 96.40\n",
      "Epoch: 256, TotalL: 0.4978, CrossE: 0.0118, Src VAT: 0.0421, Src MixUp: 0.4438, Src Train Acc: 99.76, Src Test Acc: 97.12\n",
      "Epoch: 257, TotalL: 0.4942, CrossE: 0.0148, Src VAT: 0.0430, Src MixUp: 0.4364, Src Train Acc: 99.52, Src Test Acc: 95.68\n",
      "Epoch: 258, TotalL: 0.5202, CrossE: 0.0131, Src VAT: 0.0467, Src MixUp: 0.4605, Src Train Acc: 99.84, Src Test Acc: 95.68\n",
      "Epoch: 259, TotalL: 0.5098, CrossE: 0.0155, Src VAT: 0.0503, Src MixUp: 0.4440, Src Train Acc: 99.76, Src Test Acc: 97.12\n",
      "Epoch: 260, TotalL: 0.5502, CrossE: 0.0268, Src VAT: 0.0623, Src MixUp: 0.4612, Src Train Acc: 99.68, Src Test Acc: 97.12\n",
      "Epoch: 261, TotalL: 0.4889, CrossE: 0.0093, Src VAT: 0.0412, Src MixUp: 0.4383, Src Train Acc: 100.00, Src Test Acc: 97.12\n",
      "Epoch: 262, TotalL: 0.4812, CrossE: 0.0122, Src VAT: 0.0389, Src MixUp: 0.4300, Src Train Acc: 99.68, Src Test Acc: 98.56\n",
      "Epoch: 263, TotalL: 0.5030, CrossE: 0.0251, Src VAT: 0.0527, Src MixUp: 0.4251, Src Train Acc: 99.04, Src Test Acc: 95.68\n",
      "Epoch: 264, TotalL: 0.4717, CrossE: 0.0112, Src VAT: 0.0375, Src MixUp: 0.4230, Src Train Acc: 99.76, Src Test Acc: 97.12\n",
      "Epoch: 265, TotalL: 0.5324, CrossE: 0.0260, Src VAT: 0.0573, Src MixUp: 0.4492, Src Train Acc: 99.52, Src Test Acc: 94.96\n",
      "Epoch: 266, TotalL: 0.4638, CrossE: 0.0112, Src VAT: 0.0424, Src MixUp: 0.4101, Src Train Acc: 99.84, Src Test Acc: 90.65\n",
      "Epoch: 267, TotalL: 0.4637, CrossE: 0.0098, Src VAT: 0.0387, Src MixUp: 0.4153, Src Train Acc: 99.92, Src Test Acc: 92.81\n",
      "Epoch: 268, TotalL: 0.5333, CrossE: 0.0285, Src VAT: 0.0605, Src MixUp: 0.4444, Src Train Acc: 99.20, Src Test Acc: 96.40\n",
      "Epoch: 269, TotalL: 0.4467, CrossE: 0.0114, Src VAT: 0.0412, Src MixUp: 0.3941, Src Train Acc: 99.92, Src Test Acc: 94.96\n",
      "Epoch: 270, TotalL: 0.4868, CrossE: 0.0128, Src VAT: 0.0470, Src MixUp: 0.4270, Src Train Acc: 99.92, Src Test Acc: 94.96\n",
      "Epoch: 271, TotalL: 0.4774, CrossE: 0.0137, Src VAT: 0.0445, Src MixUp: 0.4191, Src Train Acc: 99.76, Src Test Acc: 94.96\n",
      "Epoch: 272, TotalL: 0.5156, CrossE: 0.0182, Src VAT: 0.0555, Src MixUp: 0.4419, Src Train Acc: 99.76, Src Test Acc: 97.12\n",
      "Epoch: 273, TotalL: 0.4997, CrossE: 0.0188, Src VAT: 0.0502, Src MixUp: 0.4307, Src Train Acc: 99.52, Src Test Acc: 94.96\n",
      "Epoch: 274, TotalL: 0.4712, CrossE: 0.0143, Src VAT: 0.0412, Src MixUp: 0.4158, Src Train Acc: 99.76, Src Test Acc: 95.68\n",
      "Epoch: 275, TotalL: 0.4835, CrossE: 0.0137, Src VAT: 0.0450, Src MixUp: 0.4248, Src Train Acc: 99.92, Src Test Acc: 92.09\n",
      "Epoch: 276, TotalL: 0.5353, CrossE: 0.0191, Src VAT: 0.0549, Src MixUp: 0.4614, Src Train Acc: 99.68, Src Test Acc: 97.84\n",
      "Epoch: 277, TotalL: 0.4653, CrossE: 0.0161, Src VAT: 0.0420, Src MixUp: 0.4072, Src Train Acc: 99.52, Src Test Acc: 92.81\n",
      "Epoch: 278, TotalL: 0.5816, CrossE: 0.0441, Src VAT: 0.0650, Src MixUp: 0.4725, Src Train Acc: 98.96, Src Test Acc: 94.96\n",
      "Epoch: 279, TotalL: 0.4993, CrossE: 0.0104, Src VAT: 0.0399, Src MixUp: 0.4490, Src Train Acc: 99.84, Src Test Acc: 93.53\n",
      "Epoch: 280, TotalL: 0.4709, CrossE: 0.0089, Src VAT: 0.0393, Src MixUp: 0.4227, Src Train Acc: 99.92, Src Test Acc: 94.96\n",
      "Epoch: 281, TotalL: 0.4440, CrossE: 0.0070, Src VAT: 0.0327, Src MixUp: 0.4043, Src Train Acc: 99.92, Src Test Acc: 97.12\n",
      "Epoch: 282, TotalL: 0.5096, CrossE: 0.0289, Src VAT: 0.0527, Src MixUp: 0.4279, Src Train Acc: 99.12, Src Test Acc: 95.68\n",
      "Epoch: 283, TotalL: 0.4732, CrossE: 0.0121, Src VAT: 0.0424, Src MixUp: 0.4187, Src Train Acc: 99.84, Src Test Acc: 94.96\n",
      "Epoch: 284, TotalL: 0.5130, CrossE: 0.0239, Src VAT: 0.0549, Src MixUp: 0.4343, Src Train Acc: 99.60, Src Test Acc: 94.96\n",
      "Epoch: 285, TotalL: 0.4786, CrossE: 0.0101, Src VAT: 0.0393, Src MixUp: 0.4291, Src Train Acc: 99.92, Src Test Acc: 97.12\n",
      "Epoch: 286, TotalL: 0.4433, CrossE: 0.0080, Src VAT: 0.0307, Src MixUp: 0.4047, Src Train Acc: 99.76, Src Test Acc: 97.84\n",
      "Epoch: 287, TotalL: 0.4708, CrossE: 0.0149, Src VAT: 0.0446, Src MixUp: 0.4113, Src Train Acc: 99.68, Src Test Acc: 97.84\n",
      "Epoch: 288, TotalL: 0.5205, CrossE: 0.0267, Src VAT: 0.0600, Src MixUp: 0.4338, Src Train Acc: 99.28, Src Test Acc: 96.40\n",
      "Epoch: 289, TotalL: 0.4872, CrossE: 0.0115, Src VAT: 0.0422, Src MixUp: 0.4336, Src Train Acc: 99.84, Src Test Acc: 97.12\n",
      "Epoch: 290, TotalL: 0.5202, CrossE: 0.0246, Src VAT: 0.0570, Src MixUp: 0.4386, Src Train Acc: 99.36, Src Test Acc: 97.84\n",
      "Epoch: 291, TotalL: 0.5061, CrossE: 0.0117, Src VAT: 0.0442, Src MixUp: 0.4502, Src Train Acc: 100.00, Src Test Acc: 97.12\n",
      "Epoch: 292, TotalL: 0.4303, CrossE: 0.0084, Src VAT: 0.0354, Src MixUp: 0.3865, Src Train Acc: 100.00, Src Test Acc: 94.24\n",
      "Epoch: 293, TotalL: 0.4853, CrossE: 0.0090, Src VAT: 0.0371, Src MixUp: 0.4391, Src Train Acc: 99.92, Src Test Acc: 95.68\n",
      "Epoch: 294, TotalL: 0.4367, CrossE: 0.0095, Src VAT: 0.0345, Src MixUp: 0.3926, Src Train Acc: 99.84, Src Test Acc: 91.37\n",
      "Epoch: 295, TotalL: 0.4335, CrossE: 0.0091, Src VAT: 0.0346, Src MixUp: 0.3898, Src Train Acc: 99.84, Src Test Acc: 94.96\n",
      "Epoch: 296, TotalL: 0.3933, CrossE: 0.0048, Src VAT: 0.0223, Src MixUp: 0.3663, Src Train Acc: 100.00, Src Test Acc: 95.68\n",
      "Epoch: 297, TotalL: 0.3978, CrossE: 0.0059, Src VAT: 0.0254, Src MixUp: 0.3665, Src Train Acc: 99.92, Src Test Acc: 97.12\n",
      "Epoch: 298, TotalL: 0.4859, CrossE: 0.0156, Src VAT: 0.0422, Src MixUp: 0.4280, Src Train Acc: 99.76, Src Test Acc: 97.84\n",
      "Epoch: 299, TotalL: 0.4536, CrossE: 0.0126, Src VAT: 0.0446, Src MixUp: 0.3965, Src Train Acc: 99.92, Src Test Acc: 97.12\n",
      "Epoch: 300, TotalL: 0.4431, CrossE: 0.0076, Src VAT: 0.0335, Src MixUp: 0.4020, Src Train Acc: 100.00, Src Test Acc: 97.84\n",
      "Epoch: 301, TotalL: 0.4572, CrossE: 0.0244, Src VAT: 0.0390, Src MixUp: 0.3938, Src Train Acc: 99.60, Src Test Acc: 96.40\n",
      "Epoch: 302, TotalL: 0.4735, CrossE: 0.0123, Src VAT: 0.0384, Src MixUp: 0.4228, Src Train Acc: 99.92, Src Test Acc: 96.40\n",
      "Epoch: 303, TotalL: 0.4513, CrossE: 0.0141, Src VAT: 0.0384, Src MixUp: 0.3988, Src Train Acc: 99.52, Src Test Acc: 95.68\n",
      "Epoch: 304, TotalL: 0.4294, CrossE: 0.0091, Src VAT: 0.0346, Src MixUp: 0.3857, Src Train Acc: 99.92, Src Test Acc: 97.84\n",
      "Epoch: 305, TotalL: 0.4036, CrossE: 0.0079, Src VAT: 0.0306, Src MixUp: 0.3651, Src Train Acc: 99.92, Src Test Acc: 94.24\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-22abffcd2287>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0msource_data\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msrc_train_set\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mtrain_gen_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m   print(train_template.format(epoch+1,\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    400\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    403\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1324\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    576\u001b[0m     \"\"\"\n\u001b[1;32m    577\u001b[0m     return self._call_flat(\n\u001b[0;32m--> 578\u001b[0;31m         (t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[0m\u001b[1;32m    579\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m    580\u001b[0m                            resource_variable_ops.ResourceVariable))))\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    658\u001b[0m     \u001b[0;31m# Only need to override the gradient in graph mode and when we have outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 660\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    661\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_register_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args)\u001b[0m\n\u001b[1;32m    432\u001b[0m             attrs=(\"executor_type\", executor_type,\n\u001b[1;32m    433\u001b[0m                    \"config_proto\", config),\n\u001b[0;32m--> 434\u001b[0;31m             ctx=ctx)\n\u001b[0m\u001b[1;32m    435\u001b[0m       \u001b[0;31m# Replace empty list with None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_template = 'Epoch: {:03d}, TotalL: {:.4f}, CrossE: {:.4f}, Src VAT: {:.4f}, Src MixUp: {:.4f}, Src Train Acc: {:.2f}, '\n",
    "test_template  = 'Src Test Acc: {:.2f}'\n",
    "\n",
    "generator      = ResNet50(num_classes, num_features, gen_activation)\n",
    "gen_optimizer  = tf.keras.optimizers.Adam(learning_rate = learning_rate, beta_1 = 0.5)\n",
    "\n",
    "#summary_writer = tf.contrib.summary.create_file_writer('../logs/{}'.format(log_data), flush_millis=10000)\n",
    "#summary_writer.set_as_default()\n",
    "#global_step = tf.train.get_or_create_global_step()\n",
    "\n",
    "def log_loss():\n",
    "  with tf.contrib.summary.always_record_summaries():\n",
    "    tf.contrib.summary.scalar(\"train_total_loss\", train_total_loss.result())\n",
    "    tf.contrib.summary.scalar(\"train_cross_entropy_loss\", train_cross_entropy_loss.result())\n",
    "    tf.contrib.summary.scalar(\"src_train_accuracy\", src_train_accuracy.result())\n",
    "    tf.contrib.summary.scalar(\"train_src_vat_loss\", train_src_vat_loss.result())\n",
    "    tf.contrib.summary.scalar(\"train_src_mixup_loss\", train_src_mixup_loss.result())\n",
    "    tf.contrib.summary.scalar(\"src_test_accuracy\", src_test_accuracy.result())\n",
    "    \n",
    "for epoch in range(epochs):\n",
    "  #global_step.assign_add(1)  \n",
    "    \n",
    "  for source_data in src_train_set:\n",
    "    train_gen_step(source_data[0], source_data[1])    \n",
    "\n",
    "  print(train_template.format(epoch+1,\n",
    "                              train_total_loss.result(),\n",
    "                              train_cross_entropy_loss.result(),\n",
    "                              train_src_vat_loss.result(),\n",
    "                              train_src_mixup_loss.result(),\n",
    "                              src_train_accuracy.result()*100), end=\"\")\n",
    "\n",
    "  for source_data in src_test_set:\n",
    "    test_source_step(source_data[0], source_data[1])\n",
    "    \n",
    "  print(test_template.format(src_test_accuracy.result()*100))\n",
    "\n",
    "  if (epoch % 50 == 0):\n",
    "    generator.save_weights('../checkpoints/resnet_pretrained_epoch-{}_test_accuracy-{:.2f}'.format(epoch, src_test_accuracy.result()*100))\n",
    "  \n",
    "  #log_loss()\n",
    "  \n",
    "  train_total_loss.reset_states()\n",
    "  train_cross_entropy_loss.reset_states()\n",
    "  src_train_accuracy.reset_states()\n",
    "  train_src_vat_loss.reset_states()\n",
    "  train_src_mixup_loss.reset_states()\n",
    "  src_test_accuracy.reset_states()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
