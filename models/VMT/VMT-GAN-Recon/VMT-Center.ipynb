{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_path = \"/home/kjakkala/mmwave\"\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='3'\n",
    "\n",
    "import sys\n",
    "sys.path.append(os.path.join(repo_path, 'models'))\n",
    "\n",
    "from utils import *\n",
    "from resnet import ResNet50\n",
    "from pix2pix import Upsample\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path    = os.path.join(repo_path, 'data')\n",
    "num_classes     = 9\n",
    "batch_size      = 8\n",
    "train_src_days  = 6\n",
    "train_trg_days  = 0\n",
    "train_trg_env_days = 2\n",
    "epochs          = 500\n",
    "init_lr         = 0.0001\n",
    "num_features    = 256\n",
    "activation_fn   = 'selu'\n",
    "alpha           = 0.05\n",
    "disc_hidden     = [1024, 512]\n",
    "notes           = \"resnet_server_conference_adapt_center_fdisc\".format(train_trg_env_days)\n",
    "log_data = \"classes-{}_bs-{}_train_src_days-{}_train_trg_days-{}_train_trgenv_days-{}_initlr-{}_num_feat-{}_act_fn-{}_alpha-{}_disc_hidden-{}_{}\".format(num_classes,\n",
    "                                                                                                                                                         batch_size,\n",
    "                                                                                                                                                         train_src_days,\n",
    "                                                                                                                                                         train_trg_days,\n",
    "                                                                                                                                                         train_trg_env_days,\n",
    "                                                                                                                                                         init_lr,\n",
    "                                                                                                                                                         num_features,\n",
    "                                                                                                                                                         activation_fn,\n",
    "                                                                                                                                                         alpha,\n",
    "                                                                                                                                                         disc_hidden,\n",
    "                                                                                                                                                         notes)\n",
    "log_dir         = os.path.join(repo_path, 'logs/new_logs/VMT/{}'.format(log_data))\n",
    "checkpoint_path = os.path.join(repo_path, 'checkpoints/{}'.format(log_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data, y_data, classes = get_h5dataset(os.path.join(dataset_path, 'source_data.h5'))\n",
    "X_data = resize_data(X_data)\n",
    "print(X_data.shape, y_data.shape, \"\\n\", classes)\n",
    "\n",
    "X_data, y_data = balance_dataset(X_data, y_data, \n",
    "                                 num_days=10, \n",
    "                                 num_classes=len(classes), \n",
    "                                 max_samples_per_class=95)\n",
    "print(X_data.shape, y_data.shape)\n",
    "\n",
    "#remove harika's data (incomplete data)\n",
    "X_data = np.delete(X_data, np.where(y_data[:, 0] == 1)[0], 0)\n",
    "y_data = np.delete(y_data, np.where(y_data[:, 0] == 1)[0], 0)\n",
    "\n",
    "#update labes to handle 9 classes instead of 10\n",
    "y_data[y_data[:, 0] >= 2, 0] -= 1\n",
    "del classes[1]\n",
    "print(X_data.shape, y_data.shape, \"\\n\", classes)\n",
    "\n",
    "#split days of data to train and test\n",
    "X_src = X_data[y_data[:, 1] < train_src_days]\n",
    "y_src = y_data[y_data[:, 1] < train_src_days, 0]\n",
    "y_src = np.eye(len(classes))[y_src]\n",
    "X_train_src, X_test_src, y_train_src, y_test_src = train_test_split(X_src,\n",
    "                                                                    y_src,\n",
    "                                                                    stratify=y_src,\n",
    "                                                                    test_size=0.10,\n",
    "                                                                    random_state=42)\n",
    "\n",
    "X_trg = X_data[y_data[:, 1] >= train_src_days]\n",
    "y_trg = y_data[y_data[:, 1] >= train_src_days]\n",
    "X_train_trg = X_trg[y_trg[:, 1] < train_src_days+train_trg_days]\n",
    "y_train_trg = y_trg[y_trg[:, 1] < train_src_days+train_trg_days, 0]\n",
    "y_train_trg = np.eye(len(classes))[y_train_trg]\n",
    "\n",
    "X_test_trg = X_data[y_data[:, 1] >= train_src_days+train_trg_days]\n",
    "y_test_trg = y_data[y_data[:, 1] >= train_src_days+train_trg_days, 0]\n",
    "y_test_trg = np.eye(len(classes))[y_test_trg]\n",
    "\n",
    "del X_src, y_src, X_trg, y_trg, X_data, y_data\n",
    "\n",
    "#mean center and normalize dataset\n",
    "X_train_src, src_mean = mean_center(X_train_src)\n",
    "X_train_src, src_min, src_ptp = normalize(X_train_src)\n",
    "\n",
    "X_test_src, _    = mean_center(X_test_src, src_mean)\n",
    "X_test_src, _, _ = normalize(X_test_src, src_min, src_ptp)\n",
    "\n",
    "if(X_train_trg.shape[0] != 0):\n",
    "  X_train_trg, trg_mean = mean_center(X_train_trg)\n",
    "  X_train_trg, trg_min, trg_ptp = normalize(X_train_trg)\n",
    "\n",
    "  X_test_trg, _    = mean_center(X_test_trg, trg_mean)\n",
    "  X_test_trg, _, _ = normalize(X_test_trg, trg_min, trg_ptp)  \n",
    "else:\n",
    "  X_test_trg, _    = mean_center(X_test_trg, src_mean)\n",
    "  X_test_trg, _, _ = normalize(X_test_trg, src_min, src_ptp)\n",
    "  \n",
    "X_train_src = X_train_src.astype(np.float32)\n",
    "y_train_src = y_train_src.astype(np.uint8)\n",
    "X_test_src  = X_test_src.astype(np.float32)\n",
    "y_test_src  = y_test_src.astype(np.uint8)\n",
    "X_train_trg = X_train_trg.astype(np.float32)\n",
    "y_train_trg = y_train_trg.astype(np.uint8)\n",
    "X_test_trg  = X_test_trg.astype(np.float32)\n",
    "y_test_trg  = y_test_trg.astype(np.uint8)\n",
    "print(\"Final shapes: \")\n",
    "print(\"Source:\", X_train_src.shape, y_train_src.shape,  X_test_src.shape, y_test_src.shape)\n",
    "print(\"Time:\", X_train_trg.shape, y_train_trg.shape, X_test_trg.shape, y_test_trg.shape)\n",
    "\n",
    "X_train_conf,   y_train_conf,   X_test_conf,   y_test_conf   = get_trg_data(os.path.join(dataset_path, 'target_conf_data.h5'),   classes, train_trg_env_days)\n",
    "X_train_server, y_train_server, X_test_server, y_test_server = get_trg_data(os.path.join(dataset_path, 'target_server_data.h5'), classes, train_trg_env_days)\n",
    "_             , _             , X_data_office, y_data_office = get_trg_data(os.path.join(dataset_path, 'target_office_data.h5'), classes, 0)\n",
    "\n",
    "print(\"Conf:\",   X_train_conf.shape,   y_train_conf.shape,    X_test_conf.shape,   y_test_conf.shape)\n",
    "print(\"Server\",  X_train_server.shape, y_train_server.shape,  X_test_server.shape, y_test_server.shape)\n",
    "print(\"Office:\", X_data_office.shape,  y_data_office.shape)\n",
    "\n",
    "#get tf.data objects for each set\n",
    "\n",
    "#Test\n",
    "conf_test_set = tf.data.Dataset.from_tensor_slices((X_test_conf, y_test_conf))\n",
    "conf_test_set = conf_test_set.batch(batch_size, drop_remainder=False)\n",
    "conf_test_set = conf_test_set.prefetch(batch_size)\n",
    "\n",
    "server_test_set = tf.data.Dataset.from_tensor_slices((X_test_server, y_test_server))\n",
    "server_test_set = server_test_set.batch(batch_size, drop_remainder=False)\n",
    "server_test_set = server_test_set.prefetch(batch_size)\n",
    "\n",
    "office_test_set = tf.data.Dataset.from_tensor_slices((X_data_office, y_data_office))\n",
    "office_test_set = office_test_set.batch(batch_size, drop_remainder=False)\n",
    "office_test_set = office_test_set.prefetch(batch_size)\n",
    "\n",
    "src_test_set = tf.data.Dataset.from_tensor_slices((X_test_src, y_test_src))\n",
    "src_test_set = src_test_set.batch(batch_size, drop_remainder=False)\n",
    "src_test_set = src_test_set.prefetch(batch_size)\n",
    "\n",
    "time_test_set = tf.data.Dataset.from_tensor_slices((X_test_trg, y_test_trg))\n",
    "time_test_set = time_test_set.batch(batch_size, drop_remainder=False)\n",
    "time_test_set = time_test_set.prefetch(batch_size)\n",
    "\n",
    "#Train\n",
    "src_train_set = tf.data.Dataset.from_tensor_slices((X_train_src, y_train_src))\n",
    "src_train_set = src_train_set.shuffle(X_train_src.shape[0])\n",
    "src_train_set = src_train_set.batch(batch_size, drop_remainder=True)\n",
    "src_train_set = src_train_set.prefetch(batch_size)\n",
    "\n",
    "server_train_set = tf.data.Dataset.from_tensor_slices((X_train_server, y_train_server))\n",
    "server_train_set = server_train_set.shuffle(X_train_server.shape[0])\n",
    "server_train_set = server_train_set.batch(batch_size, drop_remainder=True)\n",
    "server_train_set = server_train_set.prefetch(batch_size)\n",
    "server_train_set = server_train_set.repeat(-1)\n",
    "\n",
    "conf_train_set = tf.data.Dataset.from_tensor_slices((X_train_conf, y_train_conf))\n",
    "conf_train_set = conf_train_set.shuffle(X_train_conf.shape[0])\n",
    "conf_train_set = conf_train_set.batch(batch_size, drop_remainder=True)\n",
    "conf_train_set = conf_train_set.prefetch(batch_size)\n",
    "conf_train_set = conf_train_set.repeat(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(tf.keras.Model):\n",
    "  def __init__(self, num_hidden, num_classes, activation='relu'):\n",
    "    super().__init__(name='discriminator')\n",
    "    self.hidden_layers = []\n",
    "    for dim in num_hidden:\n",
    "      self.hidden_layers.append(tf.keras.layers.Dense(dim, activation=activation))\n",
    "    self.logits = tf.keras.layers.Dense(num_classes, activation=None)\n",
    "\n",
    "  def call(self, x):\n",
    "    for layer in self.hidden_layers:\n",
    "      x = layer(x)\n",
    "    x = self.logits(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "class Decoder(tf.keras.Model):\n",
    "  def __init__(self, activation='relu'):\n",
    "    super().__init__(name='decoder')\n",
    "    self.up_stack = [\n",
    "      Upsample(512, 4, \"batchnorm\", activation),\n",
    "      Upsample(256, 4, \"batchnorm\", activation),\n",
    "      Upsample(128, 4, \"batchnorm\", activation),\n",
    "      Upsample(64 , 4, \"batchnorm\", activation),\n",
    "    ]\n",
    "    \n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "    self.last_conv = tf.keras.layers.Conv2DTranspose(1, 4, \n",
    "                                                     strides=2,\n",
    "                                                     padding='same', \n",
    "                                                     kernel_initializer=initializer,\n",
    "                                                     activation='tanh')\n",
    "\n",
    "  def call(self, x, training=False):\n",
    "    for up in self.up_stack:\n",
    "      x = up(x, training)\n",
    "    x = self.last_conv(x)\n",
    "    return x\n",
    "  \n",
    "\"\"\"Instantiates the ResNet50 architecture with discriminator and GRL layer.\n",
    "\n",
    "Args:\n",
    "  num_classes: `int` number of classes for image classification.\n",
    "\n",
    "Returns:\n",
    "    A Keras model instance.\n",
    "\"\"\"\n",
    "class ReconstructionResNet50(ResNet50):\n",
    "  def __init__(self, num_classes, num_features, num_hidden, num_disc, activation='relu'):\n",
    "    super().__init__(num_classes, num_features, activation)\n",
    "    self.decoder = Decoder(activation=self.activation)\n",
    "    \n",
    "  def call(self, img_input, training=False):\n",
    "    x = self.conv1(img_input)\n",
    "    x = self.bn1(x, training=training)\n",
    "    x = self.act1(x)\n",
    "    x = self.max_pool1(x)\n",
    "\n",
    "    for block in self.blocks:\n",
    "      x = block(x, training=training)\n",
    "\n",
    "    decoded = self.decoder(x)\n",
    "    x = self.avg_pool(x)\n",
    "    fc1 = self.fc1(x)\n",
    "    logits = self.logits(fc1)\n",
    "\n",
    "    return logits, fc1, decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cross_entropy_loss(labels, logits):\n",
    "  loss = tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n",
    "  return tf.reduce_mean(loss)\n",
    "\n",
    "class CenterLoss():\n",
    "    def __init__(self, batch_size, num_classes, len_features, alpha):\n",
    "      self.centers = tf.Variable(tf.zeros([num_classes, len_features]),\n",
    "                                 dtype=tf.float32,\n",
    "                                 trainable=False)\n",
    "      self.alpha = alpha\n",
    "      self.num_classes = num_classes\n",
    "      self.batch_size = batch_size    \n",
    "      self.margin = tf.constant(100, dtype=\"float32\")\n",
    "      self.norm = lambda x: tf.reduce_sum(tf.square(x), 1)\n",
    "      self.EdgeWeights = tf.ones((self.num_classes,self.num_classes)) - \\\n",
    "                                  tf.eye(self.num_classes)\n",
    "\n",
    "    def get_center_loss(self, features, labels):\n",
    "      labels = tf.reshape(tf.argmax(labels, axis=-1), [-1])\n",
    "      centers0 = tf.math.unsorted_segment_mean(features, \n",
    "                                               labels, \n",
    "                                               self.num_classes)\n",
    "      center_pairwise_dist = tf.transpose(self.norm(tf.expand_dims(centers0, 2) - \\\n",
    "                                                    tf.transpose(centers0)))\n",
    "      self.inter_loss = tf.math.reduce_sum(tf.multiply(tf.maximum(0.0, self.margin - center_pairwise_dist), \n",
    "                                                       self.EdgeWeights))\n",
    "\n",
    "      unique_label, unique_idx, unique_count = tf.unique_with_counts(labels)\n",
    "      appear_times = tf.gather(unique_count, unique_idx)\n",
    "      appear_times = tf.reshape(appear_times, [-1, 1])\n",
    "      centers_batch = tf.gather(self.centers, labels)\n",
    "      diff = centers_batch - features\n",
    "      diff /= tf.cast((1 + appear_times), tf.float32)\n",
    "      diff *= self.alpha\n",
    "      self.centers_update_op = tf.compat.v1.scatter_sub(self.centers, \n",
    "                                                        labels, \n",
    "                                                        diff)\n",
    "\n",
    "      self.intra_loss   = tf.nn.l2_loss(features - centers_batch)\n",
    "      self.center_loss  = self.intra_loss + self.inter_loss\n",
    "      self.center_loss /= (self.num_classes*self.batch_size+self.num_classes*self.num_classes)\n",
    "      return self.center_loss\n",
    "      \n",
    "def virtual_adversarial_images(images, logits, pert_norm_radius=3.5):  \n",
    "  with tf.GradientTape() as tape:\n",
    "    # Get normalised noise matrix\n",
    "    noise = tf.random.normal(shape=tf.shape(images))\n",
    "    noise = 1e-6 * tf.nn.l2_normalize(noise, axis=tf.range(1, len(noise.shape)))\n",
    "\n",
    "    # Add noise to image and get new logits\n",
    "    noise_logits, _, _ = generator(images + noise, \n",
    "                                   tf.constant(False, dtype=tf.bool))\n",
    "\n",
    "    # Get loss from noisey logits\n",
    "    noise_loss = tf.nn.softmax_cross_entropy_with_logits(labels=logits, logits=noise_logits)\n",
    "    noise_loss = tf.reduce_mean(noise_loss)\n",
    "\n",
    "  # Based on perturbed image loss, get direction of greatest error\n",
    "  adversarial_noise = tape.gradient(noise_loss, \n",
    "                                    [noise],\n",
    "                                    unconnected_gradients='zero')[0]\n",
    "\n",
    "  adversarial_noise = tf.nn.l2_normalize(adversarial_noise, \n",
    "                                         axis=tf.range(1, 4))\n",
    "\n",
    "  # return images with adversarial perturbation\n",
    "  return images + pert_norm_radius * adversarial_noise\n",
    "\n",
    "def mixup_preprocess(x, y, batch_size, alpha=1):\n",
    "    # random sample the lambda value from beta distribution.\n",
    "    weight     = np.random.beta(alpha, alpha, batch_size)\n",
    "    x_weight   = weight.reshape(batch_size, 1, 1, 1)\n",
    "    y_weight   = weight.reshape(batch_size, 1)\n",
    "    \n",
    "    # Perform the mixup.\n",
    "    indices = tf.random.shuffle(tf.range(batch_size))\n",
    "    mixup_images = (x * x_weight) + (tf.gather(x, indices) * (1 - x_weight))\n",
    "    mixup_labels = (y * y_weight) + (tf.gather(y, indices) * (1 - y_weight))    \n",
    "    \n",
    "    return mixup_images, tf.nn.softmax(mixup_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_entropy_loss    = tf.keras.metrics.Mean(name='cond_entropy_loss')\n",
    "source_vat_loss      = tf.keras.metrics.Mean(name='source_vat_loss')\n",
    "target_vat_loss      = tf.keras.metrics.Mean(name='target_vat_loss')\n",
    "source_mixup_loss    = tf.keras.metrics.Mean(name='source_mixup_loss')\n",
    "target_mixup_loss    = tf.keras.metrics.Mean(name='target_mixup_loss')\n",
    "domain_loss          = tf.keras.metrics.Mean(name='domain_loss')\n",
    "confusion_loss       = tf.keras.metrics.Mean(name='confusion_loss')\n",
    "center_loss          = tf.keras.metrics.Mean(name='center_loss') \n",
    "cross_entropy_loss   = tf.keras.metrics.Mean(name='cross_entropy_loss')\n",
    "source_rec_loss      = tf.keras.metrics.Mean(name='source_rec_loss')\n",
    "target_rec_loss      = tf.keras.metrics.Mean(name='target_rec_loss')\n",
    "temporal_test_acc    = tf.keras.metrics.CategoricalAccuracy(name='temporal_test_acc')\n",
    "source_train_acc     = tf.keras.metrics.CategoricalAccuracy(name='source_train_acc')\n",
    "source_test_acc      = tf.keras.metrics.CategoricalAccuracy(name='source_test_acc')\n",
    "office_test_acc      = tf.keras.metrics.CategoricalAccuracy(name='office_test_acc')\n",
    "server_train_acc     = tf.keras.metrics.CategoricalAccuracy(name='server_train_acc')\n",
    "server_test_acc      = tf.keras.metrics.CategoricalAccuracy(name='server_test_acc')\n",
    "conference_train_acc = tf.keras.metrics.CategoricalAccuracy(name='conference_train_acc')\n",
    "conference_test_acc  = tf.keras.metrics.CategoricalAccuracy(name='conference_test_acc')\n",
    "\n",
    "@tf.function\n",
    "def test_step(images):\n",
    "  logits, _, _ =  generator(images, training=False)\n",
    "  return tf.nn.softmax(logits)\n",
    "\n",
    "@tf.function\n",
    "def train_gen_step(src_images, src_labels, ser_images, ser_labels, con_images, con_labels):\n",
    "  with tf.GradientTape() as gen_tape:\n",
    "    #Logits\n",
    "    src_logits, src_enc, src_dec = generator(src_images, training=True)\n",
    "    ser_logits, ser_enc, ser_dec = generator(ser_images, training=True)\n",
    "    con_logits, con_enc, con_dec = generator(con_images, training=True)\n",
    "    \n",
    "    #VAT\n",
    "    src_adver_images       = virtual_adversarial_images(src_images, tf.nn.softmax(src_logits))\n",
    "    src_adver_logits, _, _ = generator(tf.stop_gradient(src_adver_images), training=True)\n",
    "    ser_adver_images       = virtual_adversarial_images(ser_images, tf.nn.softmax(ser_logits))\n",
    "    ser_adver_logits, _, _ = generator(tf.stop_gradient(ser_adver_images), training=True)\n",
    "    con_adver_images       = virtual_adversarial_images(con_images, tf.nn.softmax(con_logits))\n",
    "    con_adver_logits, _, _ = generator(tf.stop_gradient(con_adver_images), training=True)\n",
    "    \n",
    "    #MixUp\n",
    "    src_mixup_images, src_mixup_labels = mixup_preprocess(src_images, src_logits, batch_size)\n",
    "    src_mixup_logits, _, _             = generator(tf.stop_gradient(src_mixup_images), training=True)\n",
    "    ser_mixup_images, ser_mixup_labels = mixup_preprocess(ser_images, ser_logits, batch_size)\n",
    "    ser_mixup_logits, _, _             = generator(tf.stop_gradient(ser_mixup_images), training=True)\n",
    "    con_mixup_images, con_mixup_labels = mixup_preprocess(con_images, con_logits, batch_size)\n",
    "    con_mixup_logits, _, _             = generator(tf.stop_gradient(con_mixup_images), training=True)\n",
    "    \n",
    "    #Disc\n",
    "    src_disc_logits = discriminator(src_enc)\n",
    "    ser_disc_logits = discriminator(ser_enc)\n",
    "    con_disc_logits = discriminator(con_enc)\n",
    "    \n",
    "    #Loss\n",
    "    src_rec_loss              = tf.reduce_mean(tf.abs(src_images - src_dec))\n",
    "    ser_rec_loss              = tf.reduce_mean(tf.abs(ser_images - ser_dec))\n",
    "    con_rec_loss              = tf.reduce_mean(tf.abs(con_images - con_dec))\n",
    "    trg_rec_loss              = tf.reduce_mean(tf.concat([ser_rec_loss, con_rec_loss]))\n",
    "\n",
    "    batch_cross_entropy_loss  = get_cross_entropy_loss(labels=src_labels,\n",
    "                                                       logits=src_logits)\n",
    "    batch_cond_entropy_loss   = get_cross_entropy_loss(labels=tf.nn.softmax(tf.concat([ser_logits, \n",
    "                                                                                       con_logits], 0)), \n",
    "                                                       logits=tf.concat([ser_logits, \n",
    "                                                                         con_logits], 0))\n",
    "    \n",
    "    src_vat_loss              = get_cross_entropy_loss(labels=tf.nn.softmax(tf.stop_gradient(src_logits)),\n",
    "                                                       logits=src_adver_logits)\n",
    "    trg_vat_loss              = get_cross_entropy_loss(labels=tf.nn.softmax(tf.stop_gradient(tf.concat([ser_logits, \n",
    "                                                                                                        con_logits], 0))),\n",
    "                                                       logits=tf.concat([ser_adver_logits, \n",
    "                                                                         con_adver_logits], 0))\n",
    "\n",
    "    src_mixup_loss            = get_cross_entropy_loss(labels=tf.stop_gradient(src_mixup_labels), \n",
    "                                                       logits=src_mixup_logits)\n",
    "    trg_mixup_loss            = get_cross_entropy_loss(labels=tf.stop_gradient(tf.concat([ser_mixup_labels, \n",
    "                                                                                          con_mixup_labels], 0)), \n",
    "                                                       logits=tf.concat([ser_mixup_logits, \n",
    "                                                                         con_mixup_logits], 0))\n",
    "\n",
    "\n",
    "    batch_domain_loss         = get_cross_entropy_loss(labels=tf.one_hot(tf.cast(tf.concat([tf.zeros(tf.shape(src_disc_logits)[0]),\n",
    "                                                                                            tf.ones(tf.shape(ser_disc_logits)[0]),\n",
    "                                                                                            tf.ones(tf.shape(con_disc_logits)[0])*2], 0), tf.uint8), 3),\n",
    "                                                       logits=tf.concat([src_disc_logits,\n",
    "                                                                         ser_disc_logits, \n",
    "                                                                         con_disc_logits], 0))\n",
    "\n",
    "    batch_center_loss         = center_loss_obj.get_center_loss(src_enc, src_labels)\n",
    "\n",
    "\n",
    "    total_loss = batch_cross_entropy_loss + \\\n",
    "                 8e-2 * batch_domain_loss + \\\n",
    "                 8e-2 * batch_cond_entropy_loss + \\\n",
    "                 1    * src_mixup_loss +\\\n",
    "                 8e-2 * trg_mixup_loss +\\\n",
    "                 8e-2 * trg_vat_loss + \\\n",
    "                 1    * src_vat_loss + \\\n",
    "                 1    * batch_center_loss + \\\n",
    "                 1    * src_rec_loss + \\\n",
    "                 8e-2 * trg_rec_loss\n",
    "    \n",
    "  gen_gradients = gen_tape.gradient(total_loss, generator.trainable_variables)\n",
    "  with tf.control_dependencies([center_loss_obj.centers_update_op]):\n",
    "    gen_optimizer.apply_gradients(zip(gen_gradients, generator.trainable_variables))\n",
    "\n",
    "  source_train_acc(src_labels, tf.nn.softmax(src_logits))\n",
    "  server_train_acc(ser_labels, tf.nn.softmax(ser_logits))\n",
    "  conference_train_acc(con_labels, tf.nn.softmax(con_logits))\n",
    "  cross_entropy_loss(batch_cross_entropy_loss)\n",
    "  cond_entropy_loss(batch_cond_entropy_loss)\n",
    "  source_vat_loss(src_vat_loss)\n",
    "  target_vat_loss(trg_vat_loss)\n",
    "  source_mixup_loss(src_mixup_loss)\n",
    "  target_mixup_loss(trg_mixup_loss)\n",
    "  domain_loss(batch_domain_loss)\n",
    "  center_loss(batch_center_loss)\n",
    "  source_rec_loss(src_rec_loss)\n",
    "  target_rec_loss(trg_rec_loss)\n",
    "\n",
    "@tf.function\n",
    "def train_disc_step(src_images, ser_images, con_images):  \n",
    "  with tf.GradientTape() as disc_tape:    \n",
    "    _, src_enc, _ = generator(src_images, training=True)\n",
    "    _, ser_enc, _ = generator(ser_images, training=True)  \n",
    "    _, con_enc, _ = generator(con_images, training=True)  \n",
    "    \n",
    "    src_disc_logits = discriminator(src_enc)\n",
    "    ser_disc_logits = discriminator(ser_enc)\n",
    "    con_disc_logits = discriminator(con_enc)\n",
    "    \n",
    "    batch_confusion_loss = get_cross_entropy_loss(labels=tf.one_hot(tf.cast(tf.concat([tf.ones(tf.cast(tf.shape(src_disc_logits)[0]/2, tf.int32)),\n",
    "                                                                                       tf.ones(tf.cast(tf.shape(src_disc_logits)[0]/2, tf.int32))*2,\n",
    "                                                                                       tf.zeros(tf.shape(ser_disc_logits)[0]),\n",
    "                                                                                       tf.zeros(tf.shape(con_disc_logits)[0])], 0), tf.uint8), 3),\n",
    "                                                  logits=tf.concat([src_disc_logits,\n",
    "                                                                    ser_disc_logits, \n",
    "                                                                    con_disc_logits], 0))\n",
    "  \n",
    "  disc_gradients = disc_tape.gradient(batch_confusion_loss, discriminator.trainable_variables)\n",
    "  disc_optimizer.apply_gradients(zip(disc_gradients, discriminator.trainable_variables))\n",
    "  \n",
    "  confusion_loss(batch_confusion_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate  = init_lr\n",
    "generator      = ReconstructionResNet50(num_classes, num_features, activation_fn)\n",
    "discriminator  = Discriminator(disc_hidden, 3, activation_fn)\n",
    "\n",
    "gen_optimizer  = tf.keras.optimizers.Adam(learning_rate = learning_rate, beta_1 = 0.5)\n",
    "disc_optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate, beta_1 = 0.5)\n",
    "\n",
    "center_loss_obj= CenterLoss(batch_size, num_classes, num_features, alpha)\n",
    "\n",
    "summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "ckpt = tf.train.Checkpoint(generator=generator,\n",
    "                           discriminator=discriminator,\n",
    "                           gen_optimizer=gen_optimizer,\n",
    "                           disc_optimizer=disc_optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "  for source_data, server_data, conf_data in zip(src_train_set, server_train_set, conf_train_set):\n",
    "    train_gen_step(source_data[0], source_data[1], server_data[0], server_data[1], conf_data[0], conf_data[1])\n",
    "    train_disc_step(source_data[0], server_data[0], conf_data[0])\n",
    "\n",
    "  for data in time_test_set:\n",
    "    temporal_test_acc(test_step(data[0]), data[1])\n",
    "\n",
    "  for data in src_test_set:\n",
    "    source_test_acc(test_step(data[0]), data[1])\n",
    "\n",
    "  for data in office_test_set:\n",
    "    office_test_acc(test_step(data[0]), data[1])\n",
    "\n",
    "  for data in server_test_set:\n",
    "    server_test_acc(test_step(data[0]), data[1])\n",
    "\n",
    "  for data in conf_test_set:\n",
    "    conference_test_acc(test_step(data[0]), data[1])\n",
    "    \n",
    "  with summary_writer.as_default():\n",
    "    tf.summary.scalar(\"cross_entropy_loss\", cross_entropy_loss.result(), step=epoch)\n",
    "    tf.summary.scalar(\"temporal_test_acc\", temporal_test_acc.result(), step=epoch)\n",
    "    tf.summary.scalar(\"source_train_acc\", source_train_acc.result(), step=epoch)\n",
    "    tf.summary.scalar(\"source_test_acc\", source_test_acc.result(), step=epoch)\n",
    "    tf.summary.scalar(\"office_test_acc\", office_test_acc.result(), step=epoch)\n",
    "    tf.summary.scalar(\"server_train_acc\", server_train_acc.result(), step=epoch)\n",
    "    tf.summary.scalar(\"server_test_acc\", server_test_acc.result(), step=epoch)\n",
    "    tf.summary.scalar(\"conference_train_acc\", conference_train_acc.result(), step=epoch)\n",
    "    tf.summary.scalar(\"conference_test_acc\", conference_test_acc.result(), step=epoch)\n",
    "    tf.summary.scalar(\"cond_entropy_loss\", cond_entropy_loss.result(), step=epoch)\n",
    "    tf.summary.scalar(\"source_vat_loss\", source_vat_loss.result(), step=epoch)\n",
    "    tf.summary.scalar(\"target_vat_loss\", target_vat_loss.result(), step=epoch)\n",
    "    tf.summary.scalar(\"source_mixup_loss\", source_mixup_loss.result(), step=epoch)\n",
    "    tf.summary.scalar(\"target_mixup_loss\", target_mixup_loss.result(), step=epoch)\n",
    "    tf.summary.scalar(\"domain_loss\", domain_loss.result(), step=epoch)\n",
    "    tf.summary.scalar(\"center_loss\", center_loss.result(), step=epoch)\n",
    "    tf.summary.scalar(\"confusion_loss\", confusion_loss.result(), step=epoch)\n",
    "    tf.summary.scalar(\"source_rec_loss\", source_rec_loss.result(), step=epoch)\n",
    "    tf.summary.scalar(\"target_rec_loss\", target_rec_loss.result(), step=epoch)\n",
    "  \n",
    "  if (epoch + 1) % 25 == 0:\n",
    "    ckpt_save_path = ckpt_manager.save()\n",
    "    print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n",
    "                                                         ckpt_save_path))\n",
    "    \n",
    "  cross_entropy_loss.reset_states()\n",
    "  temporal_test_acc.reset_states()\n",
    "  source_train_acc.reset_states()\n",
    "  source_test_acc.reset_states()\n",
    "  office_test_acc.reset_states()\n",
    "  server_train_acc.reset_states()\n",
    "  server_test_acc.reset_states()\n",
    "  conference_train_acc.reset_states()\n",
    "  conference_test_acc.reset_states()\n",
    "  cond_entropy_loss.reset_states()\n",
    "  source_vat_loss.reset_states()\n",
    "  target_vat_loss.reset_states()\n",
    "  source_mixup_loss.reset_states()\n",
    "  target_mixup_loss.reset_states()\n",
    "  domain_loss.reset_states()\n",
    "  center_loss.reset_states()\n",
    "  confusion_loss.reset_states()\n",
    "  source_rec_loss.reset_states()\n",
    "  target_rec_loss.reset_states()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
