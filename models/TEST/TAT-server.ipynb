{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "repo_path = \"/home/kjakkala/mmwave\"\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "\n",
    "import sys\n",
    "sys.path.append(os.path.join(repo_path, 'models'))\n",
    "\n",
    "import h5py\n",
    "from utils import *\n",
    "from tqdm import tqdm\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path    = os.path.join(repo_path, 'data')\n",
    "num_classes     = 9\n",
    "batch_size      = 64\n",
    "train_src_days  = 3\n",
    "train_trg_days  = 0\n",
    "train_trg_env_days = 2\n",
    "epochs          = 250\n",
    "init_lr         = 0.0001\n",
    "num_features    = 256\n",
    "alpha           = 0.05\n",
    "clas_hidden     = [64]\n",
    "activation_fn   = 'selu'\n",
    "m               = 0.4\n",
    "s               = 10\n",
    "notes           = \"src-am-s-{}_m-{}-clas-{}\".format(s, m, clas_hidden)\n",
    "log_data = \"classes-{}_bs-{}_train_src_days-{}_train_trg_days-{}_train_trgenv_days-{}_alpha-{}_initlr-{}_num_feat-{}_act_fn-{}_{}\".format(num_classes,\n",
    "                                                                                                                                 batch_size,\n",
    "                                                                                                                                 train_src_days,\n",
    "                                                                                                                                 train_trg_days,\n",
    "                                                                                                                                 train_trg_env_days,\n",
    "                                                                                                                                 alpha,\n",
    "                                                                                                                                 init_lr,\n",
    "                                                                                                                                 num_features,\n",
    "                                                                                                                                 activation_fn,\n",
    "                                                                                                                                 notes)\n",
    "log_dir         = os.path.join(repo_path, 'logs/TEST/{}'.format(log_data))\n",
    "encodings_file  = os.path.join(repo_path, 'data/encodings.h5')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_train_src:    1.0000 | (2308, 256)  (2308, 9) \n",
    "X_test_src:     1.0000 | (257, 256)   (257, 9)  \n",
    "X_train_trg:    0.0000 | (0,)         (0, 9)    \n",
    "X_test_trg:     0.9886 | (5982, 256)  (5982, 9) \n",
    "X_train_conf:   0.0000 | (0,)         (0,)      \n",
    "X_test_conf:    0.8852 | (1350, 256)  (1350, 9) \n",
    "X_train_server: 0.9376 | (898, 256)   (898, 9)  \n",
    "X_test_server:  0.8705 | (448, 256)   (448, 9)  \n",
    "X_data_office:  0.8788 | (899, 256)   (899, 9)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final shapes: \n",
      "(2308, 256) (2308, 9) (257, 256) (257, 9) (0,) (0, 9) (5982, 256) (5982, 9)\n",
      "(0,) (0,) (1350, 256) (1350, 9)\n",
      "(898, 256) (898, 9) (448, 256) (448, 9)\n",
      "(899, 256) (899, 9)\n"
     ]
    }
   ],
   "source": [
    "hf = h5py.File(encodings_file, 'r')\n",
    "\n",
    "X_train_src = np.array(hf.get('X_train_src'))\n",
    "y_train_src = np.array(hf.get('y_train_src'))\n",
    "X_test_src  = np.array(hf.get('X_test_src'))\n",
    "y_test_src  = np.array(hf.get('y_test_src'))\n",
    "X_train_trg = np.array(hf.get('X_train_trg'))\n",
    "y_train_trg = np.array(hf.get('y_train_trg'))\n",
    "X_test_trg  = np.array(hf.get('X_test_trg'))\n",
    "y_test_trg  = np.array(hf.get('y_test_trg'))\n",
    "\n",
    "X_train_conf = np.array(hf.get('X_train_conf'))\n",
    "y_train_conf = np.array(hf.get('y_train_conf'))\n",
    "X_test_conf  = np.array(hf.get('X_test_conf'))\n",
    "y_test_conf  = np.array(hf.get('y_test_conf'))\n",
    "\n",
    "X_train_server = np.array(hf.get('X_train_server'))\n",
    "y_train_server = np.array(hf.get('y_train_server'))\n",
    "X_test_server  = np.array(hf.get('X_test_server'))\n",
    "y_test_server  = np.array(hf.get('y_test_server'))\n",
    "\n",
    "X_data_office  = np.array(hf.get('X_data_office'))\n",
    "y_data_office  = np.array(hf.get('y_data_office'))\n",
    "\n",
    "hf.close()\n",
    "\n",
    "print(\"Final shapes: \")\n",
    "print(X_train_src.shape, y_train_src.shape,  X_test_src.shape, y_test_src.shape, X_train_trg.shape, y_train_trg.shape, X_test_trg.shape, y_test_trg.shape)\n",
    "print(X_train_conf.shape,   y_train_conf.shape,    X_test_conf.shape,   y_test_conf.shape)\n",
    "print(X_train_server.shape, y_train_server.shape,  X_test_server.shape, y_test_server.shape)\n",
    "print(X_data_office.shape,  y_data_office.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_labels(target_data, target_labels):\n",
    "    centers = []\n",
    "    for i in range(num_classes):\n",
    "        centers.append(X_train_src[np.where(np.argmax(y_train_src, axis=1)==i)].mean(axis=0))\n",
    "    centers = np.array(centers) \n",
    "\n",
    "    kmeans = KMeans(n_clusters=9, init=centers, n_init=1).fit(target_data)\n",
    "    print((np.argmax(target_labels, axis=1)==kmeans.labels_).mean())\n",
    "    return tf.one_hot(kmeans.labels_, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9710467706013363\n"
     ]
    }
   ],
   "source": [
    "#get tf.data objects for each set\n",
    "\n",
    "#Test\n",
    "conf_test_set = tf.data.Dataset.from_tensor_slices((X_test_conf, y_test_conf))\n",
    "conf_test_set = conf_test_set.batch(batch_size, drop_remainder=False)\n",
    "conf_test_set = conf_test_set.prefetch(batch_size)\n",
    "\n",
    "server_test_set = tf.data.Dataset.from_tensor_slices((X_test_server, y_test_server))\n",
    "server_test_set = server_test_set.batch(batch_size, drop_remainder=False)\n",
    "server_test_set = server_test_set.prefetch(batch_size)\n",
    "\n",
    "office_test_set = tf.data.Dataset.from_tensor_slices((X_data_office, y_data_office))\n",
    "office_test_set = office_test_set.batch(batch_size, drop_remainder=False)\n",
    "office_test_set = office_test_set.prefetch(batch_size)\n",
    "\n",
    "src_test_set = tf.data.Dataset.from_tensor_slices((X_test_src, y_test_src))\n",
    "src_test_set = src_test_set.batch(batch_size, drop_remainder=False)\n",
    "src_test_set = src_test_set.prefetch(batch_size)\n",
    "\n",
    "time_test_set = tf.data.Dataset.from_tensor_slices((X_test_trg, y_test_trg))\n",
    "time_test_set = time_test_set.batch(batch_size, drop_remainder=False)\n",
    "time_test_set = time_test_set.prefetch(batch_size)\n",
    "\n",
    "#Train\n",
    "src_train_set = tf.data.Dataset.from_tensor_slices((X_train_src, y_train_src))\n",
    "src_train_set = src_train_set.shuffle(X_train_src.shape[0])\n",
    "src_train_set = src_train_set.batch(batch_size, drop_remainder=True)\n",
    "src_train_set = src_train_set.prefetch(batch_size)\n",
    "\n",
    "server_train_set = tf.data.Dataset.from_tensor_slices((X_train_server, y_train_server, get_target_labels(X_train_server, y_train_server)))\n",
    "server_train_set = server_train_set.shuffle(X_train_server.shape[0])\n",
    "server_train_set = server_train_set.batch(batch_size, drop_remainder=True)\n",
    "server_train_set = server_train_set.prefetch(batch_size)\n",
    "server_train_set = server_train_set.repeat(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConstrictiveRegularizer(tf.keras.regularizers.Regularizer):\n",
    "  def __init__(self, scale):\n",
    "    super().__init__()\n",
    "    self.scale = scale\n",
    "\n",
    "  def __call__(self, x):\n",
    "    l2_norm = tf.reduce_sum(tf.square(x), axis=0)\n",
    "    regularization = tf.reduce_mean(l2_norm-tf.reduce_mean(l2_norm))/4.0\n",
    "    return self.scale * regularization\n",
    "\n",
    "class AMDense(tf.keras.layers.Layer):\n",
    "  def __init__(self,\n",
    "               units,\n",
    "               kernel_initializer='glorot_uniform',\n",
    "               kernel_regularizer=None,\n",
    "               **kwargs):\n",
    "\n",
    "    super().__init__(**kwargs)\n",
    "    self.units = units\n",
    "    self.kernel_initializer = kernel_initializer\n",
    "    self.kernel_regularizer = kernel_regularizer\n",
    "\n",
    "  def build(self, input_shape):\n",
    "    self.kernel = self.add_weight(\"kernel\",\n",
    "                                  shape=[int(input_shape[-1]), self.units],\n",
    "                                  initializer=self.kernel_initializer,\n",
    "                                  regularizer=self.kernel_regularizer,\n",
    "                                  trainable=True)\n",
    "\n",
    "  def call(self, inputs):\n",
    "    return tf.matmul(tf.nn.l2_normalize(inputs, -1),\n",
    "                     tf.nn.l2_normalize(self.kernel, 0))\n",
    "\n",
    "  \n",
    "class Classifier(tf.keras.Model):\n",
    "  def __init__(self, num_hidden, num_classes, activation='relu', ca_decay=1e-3):\n",
    "    super().__init__(name='classifier')\n",
    "    self.hidden_layers = []\n",
    "    for dim in num_hidden[:-1]:\n",
    "      self.hidden_layers.append(tf.keras.layers.Dense(dim, activation=activation))\n",
    "    self.hidden_layers.append(tf.keras.layers.Dense(num_hidden[-1], \n",
    "                                                    activation=activation, \n",
    "                                                    activity_regularizer=ConstrictiveRegularizer(ca_decay)))\n",
    "    self.logits = AMDense(num_classes,\n",
    "                          kernel_regularizer=ConstrictiveRegularizer(ca_decay),\n",
    "                          name='logits')\n",
    "    \n",
    "  def call(self, x):\n",
    "    for layer in self.hidden_layers:\n",
    "      x = layer(x)\n",
    "    embed = x\n",
    "    x = self.logits(x)\n",
    "    return x, embed\n",
    "  \n",
    "def get_cross_entropy_loss(labels, logits):\n",
    "  loss = tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n",
    "  return tf.reduce_mean(loss)\n",
    "\n",
    "def AM_logits(labels, logits, m, s):\n",
    "  cos_theta = tf.clip_by_value(logits, -1,1)\n",
    "  phi = cos_theta - m\n",
    "  adjust_theta = s * tf.where(tf.equal(labels,1), phi, cos_theta)\n",
    "  return adjust_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy_loss   = tf.keras.metrics.Mean(name='cross_entropy_loss')\n",
    "temporal_test_acc    = tf.keras.metrics.CategoricalAccuracy(name='temporal_test_acc')\n",
    "source_train_acc     = tf.keras.metrics.CategoricalAccuracy(name='source_train_acc')\n",
    "source_test_acc      = tf.keras.metrics.CategoricalAccuracy(name='source_test_acc')\n",
    "office_test_acc      = tf.keras.metrics.CategoricalAccuracy(name='office_test_acc')\n",
    "server_test_acc      = tf.keras.metrics.CategoricalAccuracy(name='server_test_acc')\n",
    "server_train_acc     = tf.keras.metrics.CategoricalAccuracy(name='server_train_acc')\n",
    "conference_test_acc  = tf.keras.metrics.CategoricalAccuracy(name='conference_test_acc')\n",
    "\n",
    "@tf.function\n",
    "def test_step(images):\n",
    "  logits, _ = classifier(images, training=False)\n",
    "  return tf.nn.softmax(logits)\n",
    "\n",
    "@tf.function\n",
    "def train_clas_step(src_enc, src_labels, ser_enc, ser_labels, ser_labels_soft, m, s):\n",
    "  with tf.GradientTape() as tape:\n",
    "    #Logits\n",
    "    src_logits, _ = classifier(src_enc, training=True)\n",
    "    src_logits_am = AM_logits(labels=src_labels, logits=src_logits, m=m, s=s)\n",
    "    ser_logits, _ = classifier(ser_enc, training=True)\n",
    "    #ser_logits_am = AM_logits(labels=ser_labels_soft, logits=ser_logits, m=m, s=s)\n",
    "\n",
    "    #Loss\n",
    "    batch_cross_entropy_loss  = get_cross_entropy_loss(labels=ser_labels_soft,\n",
    "                                                       logits=ser_logits) + \\\n",
    "                                get_cross_entropy_loss(labels=src_labels,\n",
    "                                                       logits=src_logits_am)\n",
    "          \n",
    "    total_loss = batch_cross_entropy_loss\n",
    "    \n",
    "  clas_gradients = tape.gradient(total_loss, classifier.trainable_variables)\n",
    "  clas_optimizer.apply_gradients(zip(clas_gradients, classifier.trainable_variables))\n",
    "\n",
    "  source_train_acc(src_labels, tf.nn.softmax(src_logits))\n",
    "  server_train_acc(ser_labels, tf.nn.softmax(ser_logits))\n",
    "  cross_entropy_loss(batch_cross_entropy_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate  = tf.keras.optimizers.schedules.PolynomialDecay(init_lr,\n",
    "                                                               decay_steps=5000,\n",
    "                                                               end_learning_rate=init_lr*1e-2,\n",
    "                                                               cycle=True)\n",
    "classifier      = Classifier(clas_hidden, num_classes, activation_fn)\n",
    "clas_optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate, beta_1 = 0.5)\n",
    "\n",
    "summary_writer = tf.summary.create_file_writer(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [01:02<00:00,  4.02it/s]\n"
     ]
    }
   ],
   "source": [
    "m_anneal = tf.Variable(0, dtype=\"float32\")\n",
    "for epoch in tqdm(range(epochs)):  \n",
    "  m_anneal.assign(tf.minimum(m*((epoch)/1000.0), m))\n",
    "  for source_data, server_data in zip(src_train_set, server_train_set):\n",
    "    train_clas_step(source_data[0], source_data[1], server_data[0], server_data[1], server_data[2], s, m_anneal)\n",
    "\n",
    "  for data in time_test_set:\n",
    "    temporal_test_acc(test_step(data[0]), data[1])\n",
    "\n",
    "  for data in src_test_set:\n",
    "    source_test_acc(test_step(data[0]), data[1])\n",
    "\n",
    "  for data in office_test_set:\n",
    "    office_test_acc(test_step(data[0]), data[1])\n",
    "\n",
    "  for data in server_test_set:\n",
    "    server_test_acc(test_step(data[0]), data[1])\n",
    "\n",
    "  for data in conf_test_set:\n",
    "    conference_test_acc(test_step(data[0]), data[1])\n",
    "\n",
    "  with summary_writer.as_default():\n",
    "    tf.summary.scalar(\"cross_entropy_loss\", cross_entropy_loss.result(), step=epoch)\n",
    "    tf.summary.scalar(\"temporal_test_acc\", temporal_test_acc.result(), step=epoch)\n",
    "    tf.summary.scalar(\"source_train_acc\", source_train_acc.result(), step=epoch)\n",
    "    tf.summary.scalar(\"source_test_acc\", source_test_acc.result(), step=epoch)\n",
    "    tf.summary.scalar(\"office_test_acc\", office_test_acc.result(), step=epoch)\n",
    "    tf.summary.scalar(\"server_test_acc\", server_test_acc.result(), step=epoch)\n",
    "    tf.summary.scalar(\"server_train_acc\", server_train_acc.result(), step=epoch)\n",
    "    tf.summary.scalar(\"conference_test_acc\", conference_test_acc.result(), step=epoch)\n",
    "\n",
    "  cross_entropy_loss.reset_states()\n",
    "  temporal_test_acc.reset_states()\n",
    "  source_train_acc.reset_states()\n",
    "  source_test_acc.reset_states()\n",
    "  office_test_acc.reset_states()\n",
    "  server_test_acc.reset_states()\n",
    "  server_train_acc.reset_states()\n",
    "  conference_test_acc.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def predict(images):\n",
    "  logits, fc1 = classifier(images, training=False)\n",
    "  return tf.nn.softmax(logits), fc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encodings(data):\n",
    "  data_list = []\n",
    "  for i in range(len(data)):\n",
    "    _, encodings = predict(np.expand_dims(data[i], axis=0))\n",
    "    data_list.append(np.array(encodings))\n",
    "  return np.squeeze(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_labels(source_data, source_labels, target_data, target_labels):\n",
    "    centers = []\n",
    "    for i in range(num_classes):\n",
    "        centers.append(source_data[np.where(np.argmax(source_labels, axis=1)==i)].mean(axis=0))\n",
    "    centers = np.array(centers) \n",
    "\n",
    "    kmeans = KMeans(n_clusters=9, init=centers, n_init=1).fit(target_data)\n",
    "    print((np.argmax(target_labels, axis=1)==kmeans.labels_).mean())\n",
    "    print(kmeans.labels_)\n",
    "    return tf.one_hot(kmeans.labels_, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.967706013363029\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 5 1 1 5 1 1 1 1 1 1 1 5\n",
      " 1 1 1 1 1 1 1 1 5 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 4 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 2 2 2 2 2 2 2 2 2 2 2 2 4 2 2\n",
      " 2 3 4 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 4 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 4 4 4 4 1 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 1 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 1 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4 5 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 5 7 5 5 5 5 5 5 5\n",
      " 5 5 5 5 5 5 5 5 5 5 5 5 7 5 5 5 5 5 5 5 5 5 5 1 5 1 5 5 7 5 5 5 5 5 5 5 5\n",
      " 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 1 5 5 5 5 5 5\n",
      " 5 5 5 5 5 5 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 7 7 7 7 7\n",
      " 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7\n",
      " 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7\n",
      " 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 8 8 8 8 8 8 8 8 8 8 8 8 8 4 8 8\n",
      " 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 4\n",
      " 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 4 8 4 8 8 8 8 8 8 8 8 8 8 8 8 8 8 4 8 8\n",
      " 8 8 8 8 8 8 1 4 8 8]\n"
     ]
    }
   ],
   "source": [
    "new_embed_ser = get_encodings(X_train_server)\n",
    "new_embed_src = get_encodings(X_train_src)\n",
    "_ = get_target_labels(new_embed_src, y_train_src, new_embed_ser, y_train_server)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
